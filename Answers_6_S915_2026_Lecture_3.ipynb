{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oPZNCPGWP7lF"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Concept Questions"
      ],
      "metadata": {
        "id": "oPZNCPGWP7lF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is the purpose of masking during self-attention?\n",
        "2. What matrix do we mask, and what matrix do we use to perform masking?\n",
        "3. What is the purpose of using dropout in a self-attention module?\n",
        "4. What is the purpose of multi-head attention?"
      ],
      "metadata": {
        "id": "LbBgMvACP93-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Masking can ensure that tokens don't attend to *future* tokens, which is useful because LLMs want to predict the next singular token based on *past* tokens.\n",
        "2. We want to mask $QK^T$ in order for the weights of future tokens to be 0 during the weighted sum of values process. We use a lower triangular matrix of 1s to mask $QK^T$.\n",
        "3. Dropout helps the model generalize better and reduces overfitting by having the model train on slightly malformed data. Then, when we actually use/test the model, we don't use dropout.\n",
        "4. We use multiple heads because each head can learn a different pattern from the text. Multi-head attention improves performance/accuracy in general."
      ],
      "metadata": {
        "id": "9d_5UP-eeELJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Review"
      ],
      "metadata": {
        "id": "9eu7zlxiP5Gx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XZUQimpokTL_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-attention module from last lecture:"
      ],
      "metadata": {
        "id": "rrGKKmEjZAe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionV2(nn.Module):\n",
        "    def __init__(self, d_in, d_out):\n",
        "        super().__init__()\n",
        "        self.d_in = d_in\n",
        "        self.d_out = d_out\n",
        "        # W_query is a linear function that maps a d_in dimensional vector\n",
        "        # to a d_out dimensional vector\n",
        "        # Mathematically, it is the same as a d_in by d_out matrix\n",
        "        # Same for the key and value\n",
        "        self.W_query = nn.Linear(d_in, d_out)\n",
        "        self.W_key = nn.Linear(d_in, d_out)\n",
        "        self.W_value = nn.Linear(d_in, d_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is B x N x d_in\n",
        "        # Q, K, and V are B x N x d_out\n",
        "        Q = self.W_query(x)\n",
        "        K = self.W_key(x)\n",
        "        V = self.W_value(x)\n",
        "\n",
        "        QKT = Q @ K.transpose(1, 2) # dim 1 is N, and dim 2 is d_out\n",
        "        # QKT is B x N x N\n",
        "        A = torch.softmax(QKT / (self.d_out ** 0.5), dim=-1)\n",
        "\n",
        "        # A is B x N x N\n",
        "        # V is B x N x d_out\n",
        "        # A @ V is B x N x d_out\n",
        "        context_vector = A @ V\n",
        "        return context_vector"
      ],
      "metadata": {
        "id": "praoZVNx33aN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the self-attention module:"
      ],
      "metadata": {
        "id": "vW5hsrv5c0Z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_batch = torch.randn(40, 50, 768)\n",
        "self_attention = SelfAttentionV2(768, 1024) # calls __init__\n",
        "context_batch = self_attention(X_batch) # calls the forward function\n",
        "print(context_batch.shape)"
      ],
      "metadata": {
        "id": "Z0emwQlN54iG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2c13970-72ea-43e0-aece-4c01de1f37d0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([40, 50, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Causal Attention"
      ],
      "metadata": {
        "id": "KkaFHf-wc30W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use `torch.tril` to create a lower triangular matrix used for masking. The function `torch.tril` zeros out the upper right portion of a matrix (we will only be using it for square matrices)."
      ],
      "metadata": {
        "id": "SfDkZh5ic7Kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([\n",
        "    [3, 4, 5],\n",
        "    [1, 7, 3],\n",
        "    [2, 4, 5]\n",
        "])\n",
        "b = torch.tril(a)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAUw6RYJdPr-",
        "outputId": "d6d1b1dc-f32b-477f-bbb3-a297149ebcf9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3, 0, 0],\n",
            "        [1, 7, 0],\n",
            "        [2, 4, 5]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can create a *mask* by creating a lower triangular matrix filled with ones."
      ],
      "metadata": {
        "id": "_ZaYaSGdeJga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.ones(3, 3)\n",
        "b = torch.tril(a)\n",
        "print(b)\n",
        "\n",
        "# Using the mask\n",
        "c = torch.tensor([\n",
        "    [3, 4, 5],\n",
        "    [1, 7, 3],\n",
        "    [2, 4, 5]\n",
        "])\n",
        "d = b * c\n",
        "print(d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30_GPe7AeQhF",
        "outputId": "496986ed-e8b6-4b40-c155-978b5500933a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0.],\n",
            "        [1., 1., 0.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[3., 0., 0.],\n",
            "        [1., 7., 0.],\n",
            "        [2., 4., 5.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 1:** Suppose the following matrix is $QK^T$. Apply the masking process."
      ],
      "metadata": {
        "id": "evoRWuMreVEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 3\n",
        "d_out = 100\n",
        "QKT = torch.tensor([\n",
        "    [0.1, 0.2, 0.3],\n",
        "    [0.4, 0.5, 0.6],\n",
        "    [0.7, 0.8, 0.9]\n",
        "])\n",
        "softmax_QKT = torch.softmax(QKT / (d_out ** 0.5), dim=-1)\n",
        "mask = torch.tril(torch.ones(n, n))\n",
        "masked_softmax_QKT = softmax_QKT * mask\n",
        "\n",
        "# Normalizing so that the sum of each row is still 1\n",
        "masked_attention = (masked_softmax_QKT / masked_softmax_QKT.sum(dim=-1, keepdim=True))\n",
        "print(masked_attention)"
      ],
      "metadata": {
        "id": "ZkUmrcZQgJJS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "939ed4a5-173f-418d-fc72-67656022184f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.4975, 0.5025, 0.0000],\n",
            "        [0.3300, 0.3333, 0.3367]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typically, masking is actually applied *before* softmax by setting the values we want to mask out to $-\\infty$."
      ],
      "metadata": {
        "id": "T9_m3CbngseA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([\n",
        "    [3, 4, 5],\n",
        "    [1, 7, 3],\n",
        "    [2, 4, 5]\n",
        "]).float()\n",
        "example_mask = torch.tril(torch.ones(3, 3))\n",
        "print(example_mask)\n",
        "b = a.masked_fill(example_mask == 0, float('-inf'))\n",
        "print(b)\n",
        "# We would then apply softmax afterwards so that the upper triangular portion becomes 0."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CFPdCN_g6Ri",
        "outputId": "c302b140-6e91-4841-81f3-750349106d4a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0.],\n",
            "        [1., 1., 0.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[3., -inf, -inf],\n",
            "        [1., 7., -inf],\n",
            "        [2., 4., 5.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(example_mask == 0)\n",
        "# In the masked_fill line, everywhere that example_mask == 0 is true,\n",
        "# it fills a -inf value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zT4Sx88TCsRX",
        "outputId": "2244999f-e1c5-4013-e557-bf7d9c3e1793"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[False,  True,  True],\n",
            "        [False, False,  True],\n",
            "        [False, False, False]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2:** Apply the masking process before softmax."
      ],
      "metadata": {
        "id": "IiyEYk0xhg5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 3\n",
        "d_out = 100\n",
        "QKT = torch.tensor([\n",
        "    [0.1, 0.2, 0.3],\n",
        "    [0.4, 0.5, 0.6],\n",
        "    [0.7, 0.8, 0.9]\n",
        "])\n",
        "\n",
        "# Your code here\n",
        "mask = torch.tril(torch.ones(n, n))\n",
        "masked_QKT = QKT.masked_fill(mask == 0, float('-inf'))\n",
        "softmax_QKT = torch.softmax(masked_QKT / (d_out ** 0.5), dim=-1)\n",
        "print(softmax_QKT)"
      ],
      "metadata": {
        "id": "dgrt2SjZhgMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f1d85d0-aeb7-4f0b-84d6-6bd908b3a528"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.4975, 0.5025, 0.0000],\n",
            "        [0.3300, 0.3333, 0.3367]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 3:** Fill in missing parts of a causal attention module:"
      ],
      "metadata": {
        "id": "ANC4Rq-BhyX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout=0.0, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.d_in = d_in\n",
        "        self.d_out = d_out\n",
        "        # bias in a linear layer means instead of just multiplying by a matrix,\n",
        "        # we multiply by a matrix and then add a vector afterwards\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.context_length = context_length\n",
        "        causal_mask = torch.tril(torch.ones(context_length, context_length))\n",
        "\n",
        "        self.register_buffer(\"mask\", causal_mask)\n",
        "        # Buffer is useful for later so we don't have to worry about\n",
        "        # using the correct device\n",
        "        # Access mask in the forward function using self.mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, D = x.shape\n",
        "        # D should be equal to d_in\n",
        "        # N should be equal to context_length\n",
        "        Q = self.W_query(x)     # (B, N, d_out)\n",
        "        K = self.W_key(x)       # (B, N, d_out)\n",
        "        V = self.W_value(x)     # (B, N, d_out)\n",
        "\n",
        "        attention_scores = Q @ K.transpose(1, 2) # QKT\n",
        "        attention_scores = attention_scores.masked_fill(self.mask == 0, float('-inf')) # Apply masking\n",
        "        attention_probs = torch.softmax(attention_scores / (d_out ** 0.5), dim=-1) # Apply softmax\n",
        "\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        context_vector = attention_probs @ V\n",
        "        return context_vector\n"
      ],
      "metadata": {
        "id": "LvaJRDUgiRfz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 4:** Create a tensor representing a batch of data with batch size 20, context length 100, and embedding size 512. Pass the tensor through a causal attention model with `d_out=768`."
      ],
      "metadata": {
        "id": "mQvROFdJit__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "x_batch = torch.randn(20, 100, 512)\n",
        "causal_att_model = CausalAttention(x_batch.shape[2], 768, x_batch.shape[1])\n",
        "contextual_embeddings = causal_att_model(x_batch)\n",
        "print(contextual_embeddings.shape)"
      ],
      "metadata": {
        "id": "UtIqIishitea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e75434c-850a-4de9-a0b5-c502a0e00af1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 100, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-head Attention"
      ],
      "metadata": {
        "id": "0Fn1H0H0kaYv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can implement multi-head attention by simply putting together multiple instances of single-head causal attention:"
      ],
      "metadata": {
        "id": "_UKXMl_PkkV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, num_heads, context_length, dropout=0.0, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.d_in = d_in\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = d_out // num_heads\n",
        "        self.heads = nn.ModuleList([\n",
        "            CausalAttention(d_in=d_in, d_out=self.d_head,\n",
        "                            context_length=context_length, dropout=dropout, qkv_bias=qkv_bias)\n",
        "                            for _ in range(num_heads)])\n",
        "        # ModuleList is needed instead of python list since the model will not register\n",
        "        # the parameters of modules in a regular python list, which will be important\n",
        "        # when training later on\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is B x N x d_in\n",
        "        # head(x) is B x n x d_head\n",
        "        # Concatenation is B x n x (d_head * num_heads)\n",
        "        # which is the same as B x n x d_out\n",
        "        return torch.cat([head(x) for head in self.heads], dim=-1)"
      ],
      "metadata": {
        "id": "4LTd9V1mkruW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 5:** Create a tensor representing a batch of data with batch size 20, context length 100, and embedding size 512. Pass the tensor through a multi-head attention model with `d_out=768` and `num_heads=3`."
      ],
      "metadata": {
        "id": "j-3iYEiGlVcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "x_batch = torch.randn(20, 100, 512)\n",
        "multihead_model = MultiHeadAttention(x_batch.shape[2], 768, 3, x_batch.shape[1])\n",
        "contextual_embeddings = multihead_model(x_batch)\n",
        "print(contextual_embeddings.shape)"
      ],
      "metadata": {
        "id": "UsrNb4jtlgRV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75d8ff58-b5bc-462e-e027-69e0a9696cc1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 100, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 6:** Fill in missing parts of the following multi-head attention class:"
      ],
      "metadata": {
        "id": "oD8O4ZVDlhVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionOneModule(nn.Module):\n",
        "    def __init__(self, d_in, d_out, num_heads, context_length, dropout=0.0, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
        "        self.d_in = d_in\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = d_out // num_heads # Dimension of each head\n",
        "        self.context_length = context_length\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        causal_mask = torch.tril(torch.ones(context_length, context_length))\n",
        "        self.projection = nn.Linear(d_out, d_out) # Optional linear layer at the end\n",
        "\n",
        "        self.register_buffer(\"mask\", causal_mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, D = x.shape   # D is d_in, N  is context_length\n",
        "        Q = self.W_query(x) # B x N x d_out\n",
        "        K = self.W_key(x) # B x N x d_out\n",
        "        V = self.W_value(x) # B x N x d_out\n",
        "\n",
        "        # After Q.view: B x N x d_out -> B x N x num_heads x d_head\n",
        "        # After .transpose(1, 2): B x num_heads x N x d_head\n",
        "        Q = Q.view(B, N, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        K = K.view(B, N, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        V = V.view(B, N, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        # Q, K, V have size B x num_heads x N x d_head\n",
        "\n",
        "        QKT = Q @ K.transpose(2, 3) # B x num_heads x N x N\n",
        "        masked_QKT = QKT.masked_fill(self.mask == 0, float('-inf')) # Apply mask\n",
        "        attention_probs = torch.softmax(masked_QKT / (self.d_head ** 0.5), dim=-1) # Apply softmax\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # attention_probs is B x num_heads x N x N\n",
        "        # V is B x num_heads x N x d_head\n",
        "\n",
        "        context_vector = attention_probs @ V # B x num_heads x N x d_head\n",
        "        context_vector = context_vector.transpose(1, 2).contiguous().view(B, N, self.d_out)\n",
        "        # context_vector.transpose(1, 2): B x N x num_heads x d_head\n",
        "        # After .view: B x N x d_out\n",
        "        return self.projection(context_vector)"
      ],
      "metadata": {
        "id": "ivK-yTShlwut"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 7:** Create a tensor representing a batch of data with batch size 40, context length 80, and embedding size 768. Pass the tensor through a MultiHeadAttentionOneModule model with d_out=1536 and num_heads=3."
      ],
      "metadata": {
        "id": "xsFHDjy4oKpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "x = torch.randn(40, 80, 768)\n",
        "multihead_model = MultiHeadAttentionOneModule(x.shape[2], 1536, 3, 80)\n",
        "contextual_embeddings = multihead_model(x)\n",
        "print(contextual_embeddings.shape)"
      ],
      "metadata": {
        "id": "4Nwc98u7oYPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a922d1b0-3485-44c1-b7e7-1990ba02b5df"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([40, 80, 1536])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optional:** Update BadLM to use multi-head attention."
      ],
      "metadata": {
        "id": "2LDYDUUDoZvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: B x N\n",
        "# After embedding: B x N x emb_dim\n",
        "# After self-attention: B x N x att_dim\n",
        "# What we want: B x N x vocab_size\n",
        "# This is an actually complete language model,\n",
        "# except it is way too small and doesn't have many of the ideas\n",
        "# that makes GPT actually work\n",
        "# Later on, we will actually build a functional GPT\n",
        "class BadLM_V2(nn.Module):\n",
        "  def __init__(self, context_length, vocab_size, emb_dim, att_dim, num_heads):\n",
        "    super().__init__()\n",
        "    self.context_length = context_length\n",
        "    self.emb_dim = emb_dim\n",
        "    self.att_dim = att_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.vocab_size = vocab_size\n",
        "    self.token_embs = nn.Embedding(vocab_size, emb_dim)\n",
        "    self.pos_embs = nn.Embedding(vocab_size, emb_dim)\n",
        "    self.att = MultiHeadAttentionOneModule(emb_dim, att_dim, num_heads, context_length)\n",
        "    self.prediction_layer = nn.Linear(att_dim, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x is B x N\n",
        "    embedding = self.token_embs(x) + self.pos_embs(torch.arange(self.context_length))\n",
        "    context_embedding = self.att(embedding)\n",
        "    prediction = self.prediction_layer(context_embedding)\n",
        "    return prediction\n"
      ],
      "metadata": {
        "id": "s2efwAudopE4"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Dataset class\n",
        "class MyData(Dataset):\n",
        "    # Init function, called when the dataset is created\n",
        "    # dataset = MyData(text, tokenizer, context_length=4, stride=1)\n",
        "    def __init__(self, text, tokenizer, context_length, stride=1):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "        token_ids = tokenizer.encode(text)\n",
        "        for i in range(0, len(token_ids) - context_length, stride):\n",
        "            self.input_ids.append(torch.tensor(token_ids[i : i + context_length]))\n",
        "            self.target_ids.append(torch.tensor(token_ids[i + 1 : i + context_length + 1]))\n",
        "\n",
        "    # Length function\n",
        "    # len(dataset)\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    # Get item function\n",
        "    # dataset[idx]\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "# Dataloader\n",
        "def my_batch(text, batch_size, context_length, stride, shuffle=True, drop_last=True, num_workers=0):\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create the dataset object\n",
        "    dataset = MyData(text, tokenizer, context_length, stride)\n",
        "\n",
        "    # Use the DataLoader library to create a dataloader that batches the data\n",
        "    dataloader = DataLoader(dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=shuffle,\n",
        "                            drop_last=drop_last,\n",
        "                            num_workers=num_workers)\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "xJDr7SzaaLEo"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a very useless sentence that talks about itself.\"\n",
        "batch_size = 2\n",
        "context_length = 4\n",
        "loader = my_batch(text, batch_size, context_length, 1)\n",
        "vocab_size = 50257\n",
        "model = BadLM_V2(context_length, vocab_size, 768, 768, 3)\n",
        "predictions = []\n",
        "for input, target in loader:\n",
        "  output = model(input)\n",
        "  tokens = torch.argmax(output, dim=-1)\n",
        "  predictions.append(tokens)\n",
        "\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cnQlPa4Z5g8",
        "outputId": "ae93f532-4cc9-48ec-80ba-567afc4124f6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[15005, 20879, 20686, 44703],\n",
            "        [46038, 41755,  1401, 13061]]), tensor([[22448, 23515, 23563, 18619],\n",
            "        [ 5891, 45383, 49042, 18885]]), tensor([[ 5891,  9232, 48254, 16371],\n",
            "        [29260, 20879, 18076, 35327]])]\n"
          ]
        }
      ]
    }
  ]
}