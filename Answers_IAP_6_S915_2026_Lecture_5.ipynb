{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mDDgcOYDH8FE",
        "kzXYlYkeM-d-",
        "1Hp25EnzNEIc"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Concept Questions"
      ],
      "metadata": {
        "id": "4uYDSBapIAxk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is the purpose of a loss function?\n",
        "2. True or False:\n",
        "    \n",
        "    a. A higher loss function means better performance.\n",
        "\n",
        "    b. The model only updates parameters once each epoch.\n",
        "\n",
        "    c. The model parameters update based on the gradient of the loss function.\n",
        "\n",
        "    d. Cross entropy loss is often used for classification.\n",
        "\n",
        "3. Why do we use cross entropy loss when training GPT?\n",
        "\n",
        "4. What are the inputs to cross entropy loss?"
      ],
      "metadata": {
        "id": "kn57M48FYyFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The loss function is used to optimize the model, as update the model parameters based on the gradient of the loss function with respect to the model parameters. (New parameters = old parameters - learning rate * gradient)\n",
        "\n",
        "2.\n",
        "\n",
        "  a. False. A lower loss function means better performance.\n",
        "\n",
        "  b. False. The model updates after each batch.\n",
        "\n",
        "  c. True.\n",
        "\n",
        "  d. True.\n",
        "\n",
        "3. GPT is actually performing classifcation. When GPT-2 generates a new token, it chooses one out of the 50257 tokens in the vocabulary dictionary.\n",
        "\n",
        "4. Cross entropy loss takes as input the model's output logits and the label (correct answer)."
      ],
      "metadata": {
        "id": "dkQXEmKUa1Vi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training GPT-2"
      ],
      "metadata": {
        "id": "mDDgcOYDH8FE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminaries"
      ],
      "metadata": {
        "id": "kzXYlYkeM-d-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall our GPT-2 model:"
      ],
      "metadata": {
        "id": "wQ4J1MyGHmSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.d_in = config[\"n_embd\"]\n",
        "        self.d_out = config[\"n_embd\"]\n",
        "        self.num_heads = config[\"n_heads\"]\n",
        "        self.d_head = self.d_out // self.num_heads # Dimension of each head\n",
        "        self.context_length = config[\"context_length\"]\n",
        "        self.W_query = nn.Linear(self.d_in, self.d_out, bias=config[\"qkv_bias\"])\n",
        "        self.W_key = nn.Linear(self.d_in, self.d_out, bias=config[\"qkv_bias\"])\n",
        "        self.W_value = nn.Linear(self.d_in, self.d_out, bias=config[\"qkv_bias\"])\n",
        "        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
        "        causal_mask = torch.tril(torch.ones(self.context_length, self.context_length))\n",
        "        self.projection = nn.Linear(self.d_out, self.d_out)\n",
        "\n",
        "        self.register_buffer(\"mask\", causal_mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, D = x.shape\n",
        "        Q = self.W_query(x)\n",
        "        K = self.W_key(x)\n",
        "        V = self.W_value(x)\n",
        "\n",
        "        Q = Q.view(B, N, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        K = K.view(B, N, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        V = V.view(B, N, self.num_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        QKT = Q @ K.transpose(2, 3)\n",
        "        masked_QKT = QKT.masked_fill(self.mask[:N, :N] == 0, float('-inf'))\n",
        "        # [:N, :N] is because N could be less than context length\n",
        "        # due to lack of words in the data\n",
        "        attention_probs = torch.softmax(masked_QKT / (self.d_head ** 0.5), dim=-1)\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_vector = attention_probs @ V\n",
        "        context_vector = context_vector.transpose(1, 2).contiguous().view(B, N, self.d_out)\n",
        "        return self.projection(context_vector)\n",
        "\n",
        "class FeedForward(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(nn.Linear(config[\"n_embd\"], 4 * config[\"n_embd\"]),\n",
        "                                    nn.GELU(),\n",
        "                                    nn.Linear(4 * config[\"n_embd\"], config[\"n_embd\"]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class LayerNorm(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(config[\"n_embd\"]))\n",
        "        self.beta = nn.Parameter(torch.zeros(config[\"n_embd\"]))\n",
        "        self.eps = 1e-5\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        x = (x - mean) / (std + self.eps) # Normalize\n",
        "        x = self.gamma * x + self.beta # Apply linear function\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = LayerNorm(config)\n",
        "        self.attn = MultiHeadAttention(config)\n",
        "        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
        "        self.ff = FeedForward(config)\n",
        "        self.ln2 = LayerNorm(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x -> Layer norm 1 -> attention -> dropout -> residual connection\n",
        "        saved_x = x\n",
        "        x = self.ln1(x)\n",
        "        x = self.attn(x)\n",
        "        x = self.dropout(x)\n",
        "        x = saved_x + x # residual connection\n",
        "\n",
        "        # x -> Layer norm 2 -> feed forward -> dropout -> residual connection\n",
        "        saved_x = x\n",
        "        x = self.ln2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.dropout(x)\n",
        "        x = saved_x + x # residual connection\n",
        "\n",
        "        # You can do the above with two lines:\n",
        "        # x = x + self.dropout(self.attn(self.ln1(x)))\n",
        "        # x = x + self.dropout(self.ff(self.ln2(x)))\n",
        "        return x\n",
        "\n",
        "class Simple_GPT(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_embedding = nn.Embedding(config[\"vocab_size\"], config[\"n_embd\"])\n",
        "        self.position_embedding = nn.Embedding(config[\"context_length\"], config[\"n_embd\"])\n",
        "        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
        "        self.blocks = nn.Sequential(*[TransformerBlock(config)\n",
        "                                    for _ in range(config[\"n_layers\"])]) # Transformer blocks\n",
        "        # f(*[2, 3, 5, 7]) means f(2, 3, 5, 7)\n",
        "        self.ln_f = LayerNorm(config) # Final layer norm\n",
        "        self.prediction_layer = nn.Linear(config[\"n_embd\"], config[\"vocab_size\"])\n",
        "        # Linear mapping to vocab size\n",
        "\n",
        "        # Register buffer torch.arange(N) to prevent issues with device\n",
        "        self.register_buffer(\"pos_range\", torch.arange(config[\"context_length\"]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N = x.shape      # B is batch size, N is context length\n",
        "        token_embeddings = self.token_embedding(x)  # [B, N, n_embd]\n",
        "        position_embeddings = self.position_embedding(self.pos_range[:N])  # [N, n_embd]\n",
        "        x = token_embeddings + position_embeddings  # Full embeddings; [B, N, n_embd]\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.blocks(x)  # Apply transformer blocks; [B, N, n_embd]\n",
        "        x = self.ln_f(x) # Final layer norm\n",
        "        logits = self.prediction_layer(x)   # [B, N, vocab_size]\n",
        "        return logits"
      ],
      "metadata": {
        "id": "W6BafHOuHlXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 1024,\n",
        "    \"n_embd\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"dropout_rate\": 0.0,\n",
        "    \"qkv_bias\": False\n",
        "}"
      ],
      "metadata": {
        "id": "K6v_KVcL3DQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When training our GPT-2 model, we want to use a GPU. A device of `cuda` indicates that we are using a GPU, while a device of `cpu` indicates we are using CPU. It is important to not mix tensors of CPU and GPU together."
      ],
      "metadata": {
        "id": "RDdra7JmJSFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1, 3, 7, 6, 4])\n",
        "print(a.device) # By default tensors are on cpu\n",
        "b = torch.zeros(5)\n",
        "print(b.device) # Also cpu\n",
        "\n",
        "c = torch.tensor([1, 3, 7, 6, 4]).to(\"cuda\")\n",
        "print(c.device) # cuda:0\n",
        "\n",
        "d = torch.zeros(5).to(\"cuda\")\n",
        "print(d.device)\n",
        "\n",
        "print(a + b) # cpu + cpu allowed\n",
        "print(c + d) # cuda:0 + cuda:0 allowed\n",
        "# print(a + c) # error; cpu + cuda:0 not allowed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puK19yJOJtug",
        "outputId": "73778461-b495-4750-f530-3e29e0b2a998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "cpu\n",
            "cuda:0\n",
            "cuda:0\n",
            "tensor([1., 3., 7., 6., 4.])\n",
            "tensor([1., 3., 7., 6., 4.], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using the `cuda` device."
      ],
      "metadata": {
        "id": "6Bw0r6J4Jmxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\""
      ],
      "metadata": {
        "id": "jvF17xM_I8U9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randint(0, 50257, (1, 1024)).to(device)\n",
        "print(input.shape)\n",
        "model = Simple_GPT(config)\n",
        "model.to(device) # Transforms all model parameters to cuda\n",
        "output = model(input)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V--jWsBR9KT9",
        "outputId": "dd875a85-aeee-46c7-bb84-d31e171fdf89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw21wtxe-VZM",
        "outputId": "5029f031-9dcb-4079-da9f-cb6c2b2ec8b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 163059793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for p in model.parameters():\n",
        "    print(p)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJxHj-2F-QWF",
        "outputId": "a77a0f26-cdcd-4274-bd2c-cacf26ebb1df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.9976, -0.6201, -0.5656,  ..., -0.9757, -0.7084,  1.4369],\n",
            "        [-1.3415,  0.4505,  0.8072,  ..., -0.6188,  0.1700,  0.3107],\n",
            "        [ 0.4737, -2.5312,  0.1473,  ...,  0.1643,  0.0438,  1.2710],\n",
            "        ...,\n",
            "        [-0.1993, -0.7095, -0.5522,  ...,  1.1349, -1.0602,  0.4220],\n",
            "        [ 1.4440, -0.5257,  0.4968,  ...,  0.9701,  1.4827,  1.8857],\n",
            "        [ 1.5300,  1.2000, -1.2643,  ..., -0.0913, -0.8255, -2.5068]],\n",
            "       device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall the text generation process:"
      ],
      "metadata": {
        "id": "ocpvL1_qKq7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_sample(model, idx, max_new_tokens, context_length):\n",
        "    # max_new_tokens is the number of tokens we want to generate\n",
        "    # idx is the array of indices in the current context\n",
        "    # idx has size [batch_size, n_tokens]\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_length:]     # Takes the latest context window\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]       #   last token in new context window\n",
        "        # we want to keep batch and vocab dimension same\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        idx_next = torch.argmax(probs, dim=-1, keepdim=True)\n",
        "        idx = torch.cat((idx, idx_next), dim=1)     # dim=1 for the context window\n",
        "    return idx"
      ],
      "metadata": {
        "id": "ZDw23gF5-dWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "start_token = \"Hello, I am\"\n",
        "encoded = torch.tensor(tokenizer.encode(start_token)).unsqueeze(0).to(device)\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKBjf83ZAHK6",
        "outputId": "2d4280fc-c9d0-4e5c-c161-85ac4cb6f405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[15496,    11,   314,   716]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently, the model is untrained, so we will be getting nonsense tokens as output."
      ],
      "metadata": {
        "id": "VytOmSS5KxOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "output = generate_text_sample(model, encoded, 60, config['context_length'])\n",
        "print(output.shape)\n",
        "decoded = tokenizer.decode(output[0].squeeze(0).tolist())\n",
        "print(decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnfdfiZSAXdl",
        "outputId": "3653a437-da26-42c8-a74d-918fa75e2482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64])\n",
            "Hello, I ammosp stroke Earn Astrorosis shouldersosta Miss mere Thailandrec Think nig motivations activity Ports Hand somewretch inserts Ars Section continuously̶ miscarriage lackEver babys diagonal deductible ContentgregationMHzvesotional278Fontparen Charm Ultraoooooooooooooooo hydrogenopt Frances New Dep love DexterityprogressFORMDecember──guns vantageReallyRaven Errorurance354 Physics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to train our model, we need data. A dataset class and a dataloader function will be helpful."
      ],
      "metadata": {
        "id": "xdSQ52SsK9sY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "# Dataset class\n",
        "class MyData(Dataset):\n",
        "    # Init function, called when the dataset is created\n",
        "    # dataset = MyData(text, tokenizer, context_length=4, stride=1)\n",
        "    def __init__(self, text, tokenizer, context_length, stride=1):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "        token_ids = tokenizer.encode(text)\n",
        "        for i in range(0, len(token_ids) - context_length, stride):\n",
        "            self.input_ids.append(torch.tensor(token_ids[i : i + context_length]))\n",
        "            self.target_ids.append(torch.tensor(token_ids[i + 1 : i + context_length + 1]))\n",
        "\n",
        "    # Length function\n",
        "    # len(dataset)\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    # Get item function\n",
        "    # dataset[idx]\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "def my_batch(text, batch_size, context_length, stride, shuffle=True, drop_last=True, num_workers=0):\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create the dataset object\n",
        "    dataset = MyData(text, tokenizer, context_length, stride)\n",
        "\n",
        "    # Use the DataLoader library to create a dataloader that batches the data\n",
        "    dataloader = DataLoader(dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=shuffle,\n",
        "                            drop_last=drop_last,\n",
        "                            num_workers=num_workers)\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "Qzeqgdvt-0cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello my name is Daniel Li and I am currently teaching how to build an LLM\"\n",
        "dataloader = my_batch(text, batch_size=2, context_length=4, stride=1, shuffle=False)\n",
        "for batch in dataloader:\n",
        "    # print(batch)\n",
        "    input = batch[0]    # 2x4 tensor where 2 is batch size and 4 is context length\n",
        "    target = batch[1]\n",
        "    # print(input)\n",
        "    # print(target)\n",
        "\n",
        "\n",
        "for input, target in dataloader:\n",
        "    pass\n",
        "    # print(input)\n",
        "    # print(target)"
      ],
      "metadata": {
        "id": "BGRrzySH-yz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using the cross entropy loss function to train our model."
      ],
      "metadata": {
        "id": "oX1S2tpxLT2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose vocab size is 4\n",
        "example_logits_1 = torch.tensor([4., 7., 6., 3.]) # Each component corresponds to a token\n",
        "example_logits_2 = torch.tensor([2., 4., 6., 8.])\n",
        "example_label = torch.tensor(1) # The correct answer; token index 1\n",
        "example_loss_1 = nn.functional.cross_entropy(example_logits_1, example_label)\n",
        "example_loss_2 = nn.functional.cross_entropy(example_logits_2, example_label)\n",
        "print(example_loss_1)\n",
        "print(example_loss_2)\n",
        "# Note that the first tensor has lower loss because it is closer to the correct answer\n",
        "# (it assigned higher probability to the correct answer)\n",
        "\n",
        "\n",
        "example_logits_3 = torch.randn(2, 4, 10) # Batch size 2, context size 4, vocab size 10\n",
        "example_target = torch.tensor([\n",
        "    [1, 5, 7, 6],\n",
        "    [0, 6, 1, 6]\n",
        "])\n",
        "# We want to flatten the batch and context size since nn.functional.cross_entropy\n",
        "# doesn't support tensors with more than 2 dimensions\n",
        "example_loss_3 = nn.functional.cross_entropy(example_logits_3.flatten(0, 1), example_target.flatten())\n",
        "print(example_loss_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tj6RE6cZLZ5A",
        "outputId": "5b62b12d-bcdb-4492-afeb-4fb255c7565b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.3618)\n",
            "tensor(4.1451)\n",
            "tensor(2.9467)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 1:** Fill in missing parts of the loss function code."
      ],
      "metadata": {
        "id": "GXDJQw3TMo6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(dataloader, model, device=\"cpu\", num_batches=None): # 1 epoch average loss\n",
        "    # number of batches in dataset is not included as a dimension in any tensor\n",
        "    if num_batches is None:\n",
        "        num_batches = len(dataloader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(dataloader))\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    for i, (input, target) in enumerate(dataloader): # i is batch index\n",
        "        if i >= num_batches:\n",
        "            break\n",
        "\n",
        "        input = input.to(device) # Move input to appropriate device\n",
        "        logits = model(input) # Obtain output logits of the model\n",
        "        target = target.to(device) # Move target to appropriate device\n",
        "\n",
        "        loss = nn.functional.cross_entropy(logits.flatten(0, 1), target.flatten()) # Use cross entropy loss\n",
        "        # cross_entropy takes in 2D tensor for logits\n",
        "        # and 1D tensor for targets\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        # .item() extracts a numerical value from a 0D scalar tensor\n",
        "    return total_loss / num_batches # len(dataloader) is number of batches"
      ],
      "metadata": {
        "id": "U4tp7pTy_3gN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_loss(dataloader, model, device=device)"
      ],
      "metadata": {
        "id": "rjU5ULZoBn3S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec96e7c2-8d18-4277-dc20-cd774f8fe060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.883869489034018"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the loss function on longer text:"
      ],
      "metadata": {
        "id": "kHVtxg3uM0nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_long = (text + \" \") * 20\n",
        "dataloader_large = my_batch(text_long, batch_size=10, context_length=256, stride=1)"
      ],
      "metadata": {
        "id": "g6_-OzcUDQU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_loss(dataloader_large, model, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shY8MkrODmLu",
        "outputId": "4939eb25-d74b-4bbc-efec-3a1a05b4e8da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.797617316246033"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "1Hp25EnzNEIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    'vocab_size': 50257,\n",
        "    'context_length': 256,\n",
        "    'n_embd': 768,\n",
        "    'n_heads': 12,\n",
        "    'n_layers': 12,\n",
        "    'dropout_rate': 0.1,\n",
        "    'qkv_bias': False,\n",
        "    'device': 'cuda'\n",
        "}"
      ],
      "metadata": {
        "id": "KSrn6NUp0yiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('training_data.txt', 'r', encoding=\"utf-8\") as file:\n",
        "    text_data = file.read()"
      ],
      "metadata": {
        "id": "rN2Hmlex1K1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sE13OAcGbbms",
        "outputId": "794aa590-b1be-447d-bf58-370f724e49f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Call me Ishmael. Some years ago—never mind how long precisely—having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world. It is a way I have of driving off the spleen and regulating the circulation. Whenever I find myself growing grim about the mouth; whenever it is a damp, drizzly November in my soul; whenever I find myself involuntarily pausing before coffin warehouses, and bringing up the rear of every funeral I meet; and especially whenever my hypos get such an upper hand of me, that it requires a strong moral principle to prevent me from deliberately stepping into the street, and methodically knocking people’s hats off—then, I account it high time to get to sea as soon as I can. This is my substitute for pistol and ball. With a philosophical flourish Cato throws himself upon his sword; I quietly take to the ship. There is nothing surprising in this. If they but knew it, almost all men in their degree, some time or other, cherish very nearly the same feelings towards the ocean with me.\n",
            "\n",
            "There now is your insular city of the Manhattoes, belted round by wharves as Indian isles by coral reefs—commerce surrounds it with her surf. Right and left, the streets take you waterward. Its extreme downtown is the battery, where that noble mole is washed by waves, and cooled by breezes, which a few hours previous were out of sight of land. Look at the crowds of water-gazers there.\n",
            "\n",
            "Circumambulate the city of a dreamy Sabbath afternoon. Go from Corlears Hook to Coenties Slip, and from thence, by Whitehall, northward. What do you see?—Posted like silent sentinels all around the town, stand thousands upon thousands of mortal men fixed in ocean reveries. Some leaning against the spiles; some seated upon the pier-heads; some looking over the bulwarks of ships from China; some high aloft in the rigging, as if striving to get a still better seaward peep. But these are all landsmen; of week days pent up in lath and plaster—tied to counters, nailed to benches, clinched to desks. How then is this? Are the green fields gone? What do they here?\n",
            "\n",
            "But look! here come more crowds, pacing straight for the water, and seemingly bound for a dive. Strange! Nothing will content them but the extremest limit of the land; loitering under the shady lee of yonder warehouses will not suffice. No. They must get just as nigh the water as they possibly can without falling in. And there they stand—miles of them—leagues. Inlanders all, they come from lanes and alleys, streets and avenues—north, east, south, and west. Yet here they all unite. Tell me, does the magnetic virtue of the needles of the compasses of all those ships attract them thither?\n",
            "\n",
            "Once more. Say you are in the country; in some high land of lakes. Take almost any path you please, and ten to one it carries you down in a dale, and leaves you there by a pool in the stream. There is magic in it. Let the most absent-minded of men be plunged in his deepest reveries—stand that man on his legs, set his feet a-going, and he will infallibly lead you to water, if water there be in all that region. Should you ever be athirst in the great American desert, try this experiment, if your caravan happen to be supplied with a metaphysical professor. Yes, as every one knows, meditation and water are wedded for ever.\n",
            "\n",
            "But here is an artist. He desires to paint you the dreamiest, shadiest, quietest, most enchanting bit of romantic landscape in all the valley of the Saco. What is the chief element he employs? There stand his trees, each with a hollow trunk, as if a hermit and a crucifix were within; and here sleeps his meadow, and there sleep his cattle; and up from yonder cottage goes a sleepy smoke. Deep into distant woodlands winds a mazy way, reaching to overlapping spurs of mountains bathed in their hill-side blue. But though the picture lies thus tranced, and though this pine-tree shakes down its sighs like leaves upon this shepherd’s head, yet all were vain, unless the shepherd’s eye were fixed upon the magic stream before him. Go visit the Prairies in June, when for scores on scores of miles you wade knee-deep among Tiger-lilies—what is the one charm wanting?—Water—there is not a drop of water there! Were Niagara but a cataract of sand, would you travel your thousand miles to see it? Why did the poor poet of Tennessee, upon suddenly receiving two handfuls of silver, deliberate whether to buy him a coat, which he sadly needed, or invest his money in a pedestrian trip to Rockaway Beach? Why is almost every robust healthy boy with a robust healthy soul in him, at some time or other crazy to go to sea? Why upon your first voyage as a passenger, did you yourself feel such a mystical vibration, when first told that you and your ship were now out of sight of land? Why did the old Persians hold the sea holy? Why did the Greeks give it a separate deity, and own brother of Jove? Surely all this is not without meaning. And still deeper the meaning of that story of Narcissus, who because he could not grasp the tormenting, mild image he saw in the fountain, plunged into it and was drowned. But that same image, we ourselves see in all rivers and oceans. It is the image of the ungraspable phantom of life; and this is the key to it all.\n",
            "\n",
            "Now, when I say that I am in the habit of going to sea whenever I begin to grow hazy about the eyes, and begin to be over conscious of my lungs, I do not mean to have it inferred that I ever go to sea as a passenger. For to go as a passenger you must needs have a purse, and a purse is but a rag unless you have something in it. Besides, passengers get sea-sick—grow quarrelsome—don’t sleep of nights—do not enjoy themselves much, as a general thing;—no, I never go as a passenger; nor, though I am something of a salt, do I ever go to sea as a Commodore, or a Captain, or a Cook. I abandon the glory and distinction of such offices to those who like them. For my part, I abominate all honorable respectable toils, trials, and tribulations of every kind whatsoever. It is quite as much as I can do to take care of myself, without taking care of ships, barques, brigs, schooners, and what not. And as for going as cook,—though I confess there is considerable glory in that, a cook being a sort of officer on ship-board—yet, somehow, I never fancied broiling fowls;—though once broiled, judiciously buttered, and judgmatically salted and peppered, there is no one who will speak more respectfully, not to say reverentially, of a broiled fowl than I will. It is out of the idolatrous dotings of the old Egyptians upon broiled ibis and roasted river horse, that you see the mummies of those creatures in their huge bake-houses the pyramids.\n",
            "\n",
            "No, when I go to sea, I go as a simple sailor, right before the mast, plumb down into the forecastle, aloft there to the royal mast-head. True, they rather order me about some, and make me jump from spar to spar, like a grasshopper in a May meadow. And at first, this sort of thing is unpleasant enough. It touches one’s sense of honor, particularly if you come of an old established family in the land, the Van Rensselaers, or Randolphs, or Hardicanutes. And more than all, if just previous to putting your hand into the tar-pot, you have been lording it as a country schoolmaster, making the tallest boys stand in awe of you. The transition is a keen one, I assure you, from a schoolmaster to a sailor, and requires a strong decoction of Seneca and the Stoics to enable you to grin and bear it. But even this wears off in time.\n",
            "\n",
            "What of it, if some old hunks of a sea-captain orders me to get a broom and sweep down the decks? What does that indignity amount to, weighed, I mean, in the scales of the New Testament? Do you think the archangel Gabriel thinks anything the less of me, because I promptly and respectfully obey that old hunks in that particular instance? Who ain’t a slave? Tell me that. Well, then, however the old sea-captains may order me about—however they may thump and punch me about, I have the satisfaction of knowing that it is all right; that everybody else is one way or other served in much the same way—either in a physical or metaphysical point of view, that is; and so the universal thump is passed round, and all hands should rub each other’s shoulder-blades, and be content.\n",
            "\n",
            "Again, I always go to sea as a sailor, because they make a point of paying me for my trouble, whereas they never pay passengers a single penny that I ever heard of. On the contrary, passengers themselves must pay. And there is all the difference in the world between paying and being paid. The act of paying is perhaps the most uncomfortable infliction that the two orchard thieves entailed upon us. But being paid,—what will compare with it? The urbane activity with which a man receives money is really marvellous, considering that we so earnestly believe money to be the root of all earthly ills, and that on no account can a monied man enter heaven. Ah! how cheerfully we consign ourselves to perdition!\n",
            "\n",
            "Finally, I always go to sea as a sailor, because of the wholesome exercise and pure air of the fore-castle deck. For as in this world, head winds are far more prevalent than winds from astern (that is, if you never violate the Pythagorean maxim), so for the most part the Commodore on the quarter-deck gets his atmosphere at second hand from the sailors on the forecastle. He thinks he breathes it first; but not so. In much the same way do the commonalty lead their leaders in many other things, at the same time that the leaders little suspect it. But wherefore it was that after having repeatedly smelt the sea as a merchant sailor, I should now take it into my head to go on a whaling voyage; this the invisible police officer of the Fates, who has the constant surveillance of me, and secretly dogs me, and influences me in some unaccountable way—he can better answer than any one else. And, doubtless, my going on this whaling voyage, formed part of the grand programme of Providence that was drawn up a long time ago. It came in as a sort of brief interlude and solo between more extensive performances. I take it that this part of the bill must have run something like this:\n",
            "\n",
            "“Grand Contested Election for the Presidency of the United States. “WHALING VOYAGE BY ONE ISHMAEL. “BLOODY BATTLE IN AFFGHANISTAN.”\n",
            "\n",
            "Though I cannot tell why it was exactly that those stage managers, the Fates, put me down for this shabby part of a whaling voyage, when others were set down for magnificent parts in high tragedies, and short and easy parts in genteel comedies, and jolly parts in farces—though I cannot tell why this was exactly; yet, now that I recall all the circumstances, I think I can see a little into the springs and motives which being cunningly presented to me under various disguises, induced me to set about performing the part I did, besides cajoling me into the delusion that it was a choice resulting from my own unbiased freewill and discriminating judgment.\n",
            "\n",
            "Chief among these motives was the overwhelming idea of the great whale himself. Such a portentous and mysterious monster roused all my curiosity. Then the wild and distant seas where he rolled his island bulk; the undeliverable, nameless perils of the whale; these, with all the attending marvels of a thousand Patagonian sights and sounds, helped to sway me to my wish. With other men, perhaps, such things would not have been inducements; but as for me, I am tormented with an everlasting itch for things remote. I love to sail forbidden seas, and land on barbarous coasts. Not ignoring what is good, I am quick to perceive a horror, and could still be social with it—would they let me—since it is but well to be on friendly terms with all the inmates of the place one lodges in.\n",
            "\n",
            "By reason of these things, then, the whaling voyage was welcome; the great flood-gates of the wonder-world swung open, and in the wild conceits that swayed me to my purpose, two and two there floated into my inmost soul, endless processions of the whale, and, mid most of them all, one grand hooded phantom, like a snow hill in the air.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data into training and validation data:"
      ],
      "metadata": {
        "id": "mVQzG61JNSNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.9\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "#train_data\n",
        "val_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "9eW_wW-v1zt1",
        "outputId": "92fff679-8867-489e-8d69-9935b03a8ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'delusion that it was a choice resulting from my own unbiased freewill and discriminating judgment.\\n\\nChief among these motives was the overwhelming idea of the great whale himself. Such a portentous and mysterious monster roused all my curiosity. Then the wild and distant seas where he rolled his island bulk; the undeliverable, nameless perils of the whale; these, with all the attending marvels of a thousand Patagonian sights and sounds, helped to sway me to my wish. With other men, perhaps, such things would not have been inducements; but as for me, I am tormented with an everlasting itch for things remote. I love to sail forbidden seas, and land on barbarous coasts. Not ignoring what is good, I am quick to perceive a horror, and could still be social with it—would they let me—since it is but well to be on friendly terms with all the inmates of the place one lodges in.\\n\\nBy reason of these things, then, the whaling voyage was welcome; the great flood-gates of the wonder-world swung open, and in the wild conceits that swayed me to my purpose, two and two there floated into my inmost soul, endless processions of the whale, and, mid most of them all, one grand hooded phantom, like a snow hill in the air.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text_data))\n",
        "print(len(train_data))\n",
        "print(len(val_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYscBRhQ2Rq8",
        "outputId": "68f2b34d-e225-4964-821c-d4186e7d3e54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12183\n",
            "10964\n",
            "1219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating dataloaders:"
      ],
      "metadata": {
        "id": "oCcuFIhrNYP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = my_batch(train_data, batch_size=6,\n",
        "                            context_length=GPT_CONFIG_124M['context_length'] // 2,\n",
        "                            stride=GPT_CONFIG_124M['context_length'] // 2,\n",
        "                            shuffle=True, drop_last=True, num_workers=0)\n",
        "\n",
        "val_dataloader = my_batch(val_data, batch_size=6,\n",
        "                          context_length=GPT_CONFIG_124M['context_length'] // 2,\n",
        "                          stride=GPT_CONFIG_124M['context_length'] // 2,\n",
        "                          shuffle=False, drop_last=False, num_workers=0)"
      ],
      "metadata": {
        "id": "ClrACyR42aCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in train_dataloader:\n",
        "    print(x.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE-amTnl3Mhc",
        "outputId": "c2d73361-e329-44a2-ef37-3dc63816313f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 128]) torch.Size([6, 128])\n",
            "torch.Size([6, 128]) torch.Size([6, 128])\n",
            "torch.Size([6, 128]) torch.Size([6, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Useful helper function that can convert text into token IDs:"
      ],
      "metadata": {
        "id": "A31_43RsNf3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text)\n",
        "    return torch.tensor(encoded).unsqueeze(0) #unsqueeze adds batch dimension 1"
      ],
      "metadata": {
        "id": "CcXFocHu7ptL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_to_token_ids('equirfo ebruifeqwiof ewq', tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaMI14Ls71-z",
        "outputId": "e5d2cd34-a08f-45e8-e374-99e4d4811659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 4853,   343,  6513, 36649,   622,   901,    80,    86,   952,    69,\n",
            "           304,    86,    80]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper function used to generate and print the output:"
      ],
      "metadata": {
        "id": "0Qju4Bm-Nlzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.position_embedding.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_sample(model, encoded, 50, context_size)\n",
        "    decoded = tokenizer.decode(token_ids[0].squeeze(0).tolist())\n",
        "    print(decoded.replace(\"\\n\", \" \"))\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "Vo0LG-lU6z0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper function used to get loss values for the train and validation splits:"
      ],
      "metadata": {
        "id": "1_rqvtRkNz4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calculate_loss(train_loader, model, device, eval_iter)\n",
        "        val_loss = calculate_loss(val_loader, model, device, eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ],
      "metadata": {
        "id": "FnFLd7YV_rne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2:** Fill in missing parts of the training loop."
      ],
      "metadata": {
        "id": "d5GYNKWKN64S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(model, train_dataloader, val_dataloader,\n",
        "                  optimizer, device, num_epochs,\n",
        "                  eval_freq, eval_iter, start_context, tokenizer):\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train() # Puts the model in training mode\n",
        "        for input_batch, target_batch in train_dataloader:\n",
        "            optimizer.zero_grad() # Zeros gradient calculations\n",
        "\n",
        "            input_batch = input_batch.to(device) # Move to proper device\n",
        "            target_batch = target_batch.to(device) # Move to proper device\n",
        "            logits = model(input_batch)\n",
        "            loss = nn.functional.cross_entropy(logits.flatten(0, 1),\n",
        "                                               target_batch.flatten())\n",
        "\n",
        "            # we are updating based on single batch here\n",
        "            loss.backward() # computes the gradients\n",
        "            optimizer.step() # updates the model parameters (optimizer is linked to model)\n",
        "            # forward means passing through the model\n",
        "            # backward means I compute the gradient of the loss wrt the parameters\n",
        "            # Update by -lr * gradient\n",
        "\n",
        "            tokens_seen += input_batch.numel() # number of elements\n",
        "            # train_losses.append(loss.item())\n",
        "            global_step += 1 # number of batches trained\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(model, train_dataloader, val_dataloader, device, eval_iter)\n",
        "                val_losses.append(val_loss)\n",
        "                train_losses.append(train_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Epoch {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f},\"\n",
        "                      f\"Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Generate and print a sample for each epoch:\n",
        "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ],
      "metadata": {
        "id": "-IsiIo2m3aac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 3:** Create a GPT-2 model using `GPT_CONFIG_124M`. Then, train the model by running a training loop. Use `eval_freq=1`, `eval_iter=5`, and choose your own starting context (4-5 words)."
      ],
      "metadata": {
        "id": "hAaoElFPOkX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = Simple_GPT(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "num_epochs = 20\n",
        "start_context = \"Once upon a time,\" # Replace\n",
        "train_losses, val_losses, tokens_seen = training_loop(\n",
        "    model, train_dataloader, val_dataloader, optimizer, device, num_epochs,\n",
        "    eval_freq=1, eval_iter=5, start_context=start_context, tokenizer=tokenizer\n",
        ") # Run the training loop"
      ],
      "metadata": {
        "id": "W2pMupgb5-wU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e417dbd-bb86-4589-a919-07d974e6acc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (Step 000000): Train loss 9.565,Val loss 9.889\n",
            "Epoch 1 (Step 000001): Train loss 9.048,Val loss 9.339\n",
            "Epoch 1 (Step 000002): Train loss 8.731,Val loss 9.047\n",
            "Once upon a time,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
            "Epoch 2 (Step 000003): Train loss 8.443,Val loss 8.819\n",
            "Epoch 2 (Step 000004): Train loss 8.103,Val loss 8.607\n",
            "Epoch 2 (Step 000005): Train loss 7.703,Val loss 8.404\n",
            "Once upon a time,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
            "Epoch 3 (Step 000006): Train loss 7.347,Val loss 8.206\n",
            "Epoch 3 (Step 000007): Train loss 7.051,Val loss 8.029\n",
            "Epoch 3 (Step 000008): Train loss 6.655,Val loss 7.871\n",
            "Once upon a time, and the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the, and the\n",
            "Epoch 4 (Step 000009): Train loss 6.372,Val loss 7.764\n",
            "Epoch 4 (Step 000010): Train loss 5.902,Val loss 7.701\n",
            "Epoch 4 (Step 000011): Train loss 5.555,Val loss 8.068\n",
            "Once upon a time, and.                                                \n",
            "Epoch 5 (Step 000012): Train loss 5.586,Val loss 7.708\n",
            "Epoch 5 (Step 000013): Train loss 5.350,Val loss 7.648\n",
            "Epoch 5 (Step 000014): Train loss 5.166,Val loss 7.696\n",
            "Once upon a time, and. And, and. But of a of a of a of a of a of the of the of the of the of a of the of a I of a of the of the of the of the of the of a of the of a\n",
            "Epoch 6 (Step 000015): Train loss 4.762,Val loss 7.726\n",
            "Epoch 6 (Step 000016): Train loss 4.390,Val loss 7.786\n",
            "Epoch 6 (Step 000017): Train loss 4.328,Val loss 7.939\n",
            "Once upon a time, and me about, and. But in the most. But is the same the same the same of the same the same you the of the same way the same the same the same the same the same the same the same the same the same of the\n",
            "Epoch 7 (Step 000018): Train loss 4.018,Val loss 7.826\n",
            "Epoch 7 (Step 000019): Train loss 3.755,Val loss 7.798\n",
            "Epoch 7 (Step 000020): Train loss 3.541,Val loss 7.827\n",
            "Once upon a time, as a is, and, of a of the same, and, and, and, I can to sea as a purse, of the old, I ever, I can. But, as a, I, and, and, and, I\n",
            "Epoch 8 (Step 000021): Train loss 3.316,Val loss 7.830\n",
            "Epoch 8 (Step 000022): Train loss 3.176,Val loss 7.847\n",
            "Epoch 8 (Step 000023): Train loss 2.911,Val loss 7.854\n",
            "Once upon a time, and me about, and. But even this wears of the of me about the same way is the mosts of a purse is but a passenger. But I can to get a little and get the water-s; and that it. It is\n",
            "Epoch 9 (Step 000024): Train loss 2.766,Val loss 7.869\n",
            "Epoch 9 (Step 000025): Train loss 2.616,Val loss 7.871\n",
            "Epoch 9 (Step 000026): Train loss 2.389,Val loss 7.893\n",
            "Once upon a time, and me about, and this the of the of the of that I can. It is, and to sea as a purse is but a old hun, I can in the sea as soon as I can. It is the world, and— the\n",
            "Epoch 10 (Step 000027): Train loss 2.209,Val loss 7.903\n",
            "Epoch 10 (Step 000028): Train loss 2.008,Val loss 7.911\n",
            "Epoch 10 (Step 000029): Train loss 1.797,Val loss 7.910\n",
            "Once upon a time, and me about, I have the satisfaction of the to go as a passenger you must needs have a purse, and a purse is but a old hunks in the land, and a and see the water-s; and so for pistol and ball\n",
            "Epoch 11 (Step 000030): Train loss 1.781,Val loss 7.942\n",
            "Epoch 11 (Step 000031): Train loss 1.522,Val loss 7.961\n",
            "Epoch 11 (Step 000032): Train loss 1.447,Val loss 7.997\n",
            "Once upon a time, I ever go to sea as a passenger. For to go as a passenger you must needs have a purse, and a purse is but a rag unless you have something in it. Besides, passengers get sea-sick—grow quarrelsome—don\n",
            "Epoch 12 (Step 000033): Train loss 1.208,Val loss 8.011\n",
            "Epoch 12 (Step 000034): Train loss 1.168,Val loss 8.046\n",
            "Epoch 12 (Step 000035): Train loss 1.077,Val loss 8.094\n",
            "Once upon a time, and me about, I have the satisfaction of knowing that it is all, and though this pine-tree shakes down its sighs like leaves upon this shepherd’s head, I all were vain, that is; and so the universal thump\n",
            "Epoch 13 (Step 000036): Train loss 0.955,Val loss 8.148\n",
            "Epoch 13 (Step 000037): Train loss 0.804,Val loss 8.166\n",
            "Epoch 13 (Step 000038): Train loss 0.705,Val loss 8.161\n",
            "Once upon a time, and upon thousands of mortal men fixed in the picture lies thus tranced, and though this pine-tree shakes down its sighs like leaves upon this shepherd’s head, yet all were vain, that is; and so the universal thump\n",
            "Epoch 14 (Step 000039): Train loss 0.674,Val loss 8.167\n",
            "Epoch 14 (Step 000040): Train loss 0.595,Val loss 8.197\n",
            "Epoch 14 (Step 000041): Train loss 0.516,Val loss 8.248\n",
            "Once upon a time, I ever go to sea as a passenger. For to go as a passenger you must needs have a purse, and a purse is but a rag unless you have something in it. Besides, passengers get sea-sick—grow quarrelsome—don\n",
            "Epoch 15 (Step 000042): Train loss 0.471,Val loss 8.281\n",
            "Epoch 15 (Step 000043): Train loss 0.397,Val loss 8.311\n",
            "Epoch 15 (Step 000044): Train loss 0.359,Val loss 8.311\n",
            "Once upon a time, I ever go to sea as a passenger. For to go as a passenger you must needs have a purse, and a purse is but a rag unless you have something in it. Besides, passengers get sea-sick—grow quarrelsome—don\n",
            "Epoch 16 (Step 000045): Train loss 0.315,Val loss 8.326\n",
            "Epoch 16 (Step 000046): Train loss 0.266,Val loss 8.353\n",
            "Epoch 16 (Step 000047): Train loss 0.229,Val loss 8.391\n",
            "Once upon a time, I ever go to sea as a passenger. For to go as a passenger you must needs have a purse, and a purse is but a rag unless you have something in it. Besides, passengers get sea-sick—grow quarrelsome—don\n",
            "Epoch 17 (Step 000048): Train loss 0.210,Val loss 8.433\n",
            "Epoch 17 (Step 000049): Train loss 0.181,Val loss 8.473\n",
            "Epoch 17 (Step 000050): Train loss 0.161,Val loss 8.505\n",
            "Once upon a time, I ever go to sea as a passenger. For to go as a passenger you must needs have a purse, and a purse is but a rag unless you have something in it. Besides, passengers get sea-sick—grow quarrelsome—don\n",
            "Epoch 18 (Step 000051): Train loss 0.141,Val loss 8.536\n",
            "Epoch 18 (Step 000052): Train loss 0.120,Val loss 8.571\n",
            "Epoch 18 (Step 000053): Train loss 0.110,Val loss 8.601\n",
            "Once upon a time, I ever go to sea as a passenger. For to go as a passenger you must needs have a purse, and a purse is but a rag unless you have something in it. Besides, passengers get sea-sick—grow quarrelsome—don\n",
            "Epoch 19 (Step 000054): Train loss 0.100,Val loss 8.628\n",
            "Epoch 19 (Step 000055): Train loss 0.090,Val loss 8.643\n",
            "Epoch 19 (Step 000056): Train loss 0.079,Val loss 8.636\n",
            "Once upon a time, I ever go to sea as a passenger. For to go as a passenger you must needs have a purse, and a purse is but a rag unless you have something in it. Besides, passengers get sea-sick—grow quarrelsome—don\n",
            "Epoch 20 (Step 000057): Train loss 0.069,Val loss 8.641\n",
            "Epoch 20 (Step 000058): Train loss 0.065,Val loss 8.666\n",
            "Epoch 20 (Step 000059): Train loss 0.057,Val loss 8.709\n",
            "Once upon a time, I ever go to sea as a passenger. For to go as a passenger you must needs have a purse, and a purse is but a rag unless you have something in it. Besides, passengers get sea-sick—grow quarrelsome—don\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the loss function over time:"
      ],
      "metadata": {
        "id": "DKiSeL0FOLIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(6, 3))\n",
        "    ax1.plot(epochs_seen, train_losses, color='tab:blue', label='Training Loss')\n",
        "    ax1.plot(epochs_seen, val_losses, color='tab:orange', label='Validation Loss', linestyle='-')\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "    ax2 = ax1.twiny()\n",
        "    ax2.plot(tokens_seen, train_losses, color='tab:blue', alpha=0)\n",
        "    ax2.set_xlabel(\"Tokens Seen\")\n",
        "    ax2.set_xlim(tokens_seen[0], tokens_seen[-1])\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "id": "hzr62-u6Eery",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "bd1da327-0aca-4b11-faa1-fd1dbf1171d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 4:** Train another GPT-2 model on another dataset. Use a train ratio of 0.8. You are allowed to use functions already defined above."
      ],
      "metadata": {
        "id": "61yiGfbJVOLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_config = {\n",
        "    'vocab_size': 50257,\n",
        "    'context_length': 256,\n",
        "    'n_embd': 768,\n",
        "    'n_heads': 12,\n",
        "    'n_layers': 12,\n",
        "    'dropout_rate': 0.1,\n",
        "    'qkv_bias': False,\n",
        "    'device': 'cuda'\n",
        "}"
      ],
      "metadata": {
        "id": "eofRF0itV243"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('training_data_3.txt', 'r', encoding=\"utf-8\") as file:\n",
        "    text_data_2 = file.read()\n",
        "\n",
        "train_ratio = 0.8\n",
        "split_idx = int(train_ratio * len(text_data_2))\n",
        "train_data_2 = text_data_2[:split_idx]\n",
        "val_data_2 = text_data_2[split_idx:]\n",
        "\n",
        "train_dataloader_2 = my_batch(train_data_2, batch_size=20,\n",
        "                            context_length=my_config['context_length'] // 2,\n",
        "                            stride=my_config['context_length'] // 2,\n",
        "                            shuffle=True, drop_last=True, num_workers=0)\n",
        "\n",
        "val_dataloader_2 = my_batch(val_data_2, batch_size=20,\n",
        "                          context_length=my_config['context_length'] // 2,\n",
        "                          stride=my_config['context_length'] // 2,\n",
        "                          shuffle=False, drop_last=False, num_workers=0)\n",
        "\n",
        "model_2 = Simple_GPT(my_config)\n",
        "model_2.to(my_config[\"device\"])\n",
        "\n",
        "optimizer_2 = torch.optim.AdamW(model_2.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "num_epochs = 20\n",
        "start_context = \"Once upon a time,\" # Replace\n",
        "\n",
        "train_losses, val_losses, tokens_seen = training_loop(\n",
        "    model_2, train_dataloader_2, val_dataloader_2, optimizer_2,\n",
        "    my_config[\"device\"], num_epochs,\n",
        "    eval_freq=1, eval_iter=5, start_context=start_context, tokenizer=tokenizer\n",
        ") # Run the training loop\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHn12Re4m4Hl",
        "outputId": "aceba48f-c5c3-4800-f1ce-3e70e1c05e08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (Step 000000): Train loss 9.788,Val loss 9.843\n",
            "Epoch 1 (Step 000001): Train loss 9.300,Val loss 9.339\n",
            "Epoch 1 (Step 000002): Train loss 8.987,Val loss 9.068\n",
            "Epoch 1 (Step 000003): Train loss 8.652,Val loss 8.766\n",
            "Epoch 1 (Step 000004): Train loss 8.346,Val loss 8.479\n",
            "Epoch 1 (Step 000005): Train loss 8.060,Val loss 8.215\n",
            "Epoch 1 (Step 000006): Train loss 7.816,Val loss 7.967\n",
            "Epoch 1 (Step 000007): Train loss 7.548,Val loss 7.743\n",
            "Epoch 1 (Step 000008): Train loss 7.329,Val loss 7.551\n",
            "Epoch 1 (Step 000009): Train loss 7.169,Val loss 7.386\n",
            "Epoch 1 (Step 000010): Train loss 6.985,Val loss 7.251\n",
            "Epoch 1 (Step 000011): Train loss 6.910,Val loss 7.152\n",
            "Epoch 1 (Step 000012): Train loss 6.733,Val loss 7.080\n",
            "Once upon a time, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,\n",
            "Epoch 2 (Step 000013): Train loss 6.714,Val loss 7.035\n",
            "Epoch 2 (Step 000014): Train loss 6.645,Val loss 7.015\n",
            "Epoch 2 (Step 000015): Train loss 6.619,Val loss 7.012\n",
            "Epoch 2 (Step 000016): Train loss 6.629,Val loss 7.010\n",
            "Epoch 2 (Step 000017): Train loss 6.549,Val loss 6.993\n",
            "Epoch 2 (Step 000018): Train loss 6.575,Val loss 6.975\n",
            "Epoch 2 (Step 000019): Train loss 6.465,Val loss 6.961\n",
            "Epoch 2 (Step 000020): Train loss 6.468,Val loss 6.954\n",
            "Epoch 2 (Step 000021): Train loss 6.434,Val loss 6.932\n",
            "Epoch 2 (Step 000022): Train loss 6.405,Val loss 6.913\n",
            "Epoch 2 (Step 000023): Train loss 6.389,Val loss 6.922\n",
            "Epoch 2 (Step 000024): Train loss 6.547,Val loss 7.023\n",
            "Epoch 2 (Step 000025): Train loss 6.440,Val loss 6.941\n",
            "Once upon a time, I a a a a a a a a a a a a the a a the a a the a a a a a the a a a a a a a a a a a a the a the a a the a the a a the a\n",
            "Epoch 3 (Step 000026): Train loss 6.444,Val loss 6.980\n",
            "Epoch 3 (Step 000027): Train loss 6.440,Val loss 6.977\n",
            "Epoch 3 (Step 000028): Train loss 6.410,Val loss 6.957\n",
            "Epoch 3 (Step 000029): Train loss 6.318,Val loss 6.921\n",
            "Epoch 3 (Step 000030): Train loss 6.278,Val loss 6.882\n",
            "Epoch 3 (Step 000031): Train loss 6.243,Val loss 6.854\n",
            "Epoch 3 (Step 000032): Train loss 6.256,Val loss 6.829\n",
            "Epoch 3 (Step 000033): Train loss 6.290,Val loss 6.820\n",
            "Epoch 3 (Step 000034): Train loss 6.262,Val loss 6.802\n",
            "Epoch 3 (Step 000035): Train loss 6.150,Val loss 6.774\n",
            "Epoch 3 (Step 000036): Train loss 6.146,Val loss 6.762\n",
            "Epoch 3 (Step 000037): Train loss 6.102,Val loss 6.754\n",
            "Epoch 3 (Step 000038): Train loss 6.085,Val loss 6.732\n",
            "Once upon a time, and a little, and the sea, and the sea, and the sea, and the sea, and a a a and the sea, and and I, and a his, and a his a it, and the sea, and a it,\n",
            "Epoch 4 (Step 000039): Train loss 6.019,Val loss 6.715\n",
            "Epoch 4 (Step 000040): Train loss 6.026,Val loss 6.687\n",
            "Epoch 4 (Step 000041): Train loss 5.972,Val loss 6.697\n",
            "Epoch 4 (Step 000042): Train loss 6.048,Val loss 6.698\n",
            "Epoch 4 (Step 000043): Train loss 5.950,Val loss 6.690\n",
            "Epoch 4 (Step 000044): Train loss 5.933,Val loss 6.656\n",
            "Epoch 4 (Step 000045): Train loss 5.888,Val loss 6.636\n",
            "Epoch 4 (Step 000046): Train loss 5.905,Val loss 6.643\n",
            "Epoch 4 (Step 000047): Train loss 5.865,Val loss 6.617\n",
            "Epoch 4 (Step 000048): Train loss 5.748,Val loss 6.582\n",
            "Epoch 4 (Step 000049): Train loss 5.734,Val loss 6.582\n",
            "Epoch 4 (Step 000050): Train loss 5.702,Val loss 6.573\n",
            "Epoch 4 (Step 000051): Train loss 5.695,Val loss 6.571\n",
            "Once upon a time, and a little the room, and a little the same�s a little. But the room, and a little. But the sea, and I was a little, and a little the sea, and I was a little, and a little the\n",
            "Epoch 5 (Step 000052): Train loss 5.721,Val loss 6.548\n",
            "Epoch 5 (Step 000053): Train loss 5.672,Val loss 6.538\n",
            "Epoch 5 (Step 000054): Train loss 5.636,Val loss 6.529\n",
            "Epoch 5 (Step 000055): Train loss 5.478,Val loss 6.539\n",
            "Epoch 5 (Step 000056): Train loss 5.462,Val loss 6.517\n",
            "Epoch 5 (Step 000057): Train loss 5.486,Val loss 6.484\n",
            "Epoch 5 (Step 000058): Train loss 5.467,Val loss 6.478\n",
            "Epoch 5 (Step 000059): Train loss 5.406,Val loss 6.505\n",
            "Epoch 5 (Step 000060): Train loss 5.343,Val loss 6.485\n",
            "Epoch 5 (Step 000061): Train loss 5.364,Val loss 6.476\n",
            "Epoch 5 (Step 000062): Train loss 5.265,Val loss 6.464\n",
            "Epoch 5 (Step 000063): Train loss 5.285,Val loss 6.477\n",
            "Epoch 5 (Step 000064): Train loss 5.205,Val loss 6.450\n",
            "Once upon a time, and the sea, and the sea, and the same, and the same the sea, and the sea, and the same the sea, and the same his, and the sea, and the sea, and the sea, and the sea-e\n",
            "Epoch 6 (Step 000065): Train loss 5.160,Val loss 6.471\n",
            "Epoch 6 (Step 000066): Train loss 5.140,Val loss 6.442\n",
            "Epoch 6 (Step 000067): Train loss 5.057,Val loss 6.439\n",
            "Epoch 6 (Step 000068): Train loss 5.028,Val loss 6.430\n",
            "Epoch 6 (Step 000069): Train loss 5.029,Val loss 6.427\n",
            "Epoch 6 (Step 000070): Train loss 4.926,Val loss 6.412\n",
            "Epoch 6 (Step 000071): Train loss 4.909,Val loss 6.407\n",
            "Epoch 6 (Step 000072): Train loss 4.871,Val loss 6.403\n",
            "Epoch 6 (Step 000073): Train loss 4.776,Val loss 6.407\n",
            "Epoch 6 (Step 000074): Train loss 4.857,Val loss 6.407\n",
            "Epoch 6 (Step 000075): Train loss 4.747,Val loss 6.384\n",
            "Epoch 6 (Step 000076): Train loss 4.731,Val loss 6.379\n",
            "Epoch 6 (Step 000077): Train loss 4.745,Val loss 6.383\n",
            "Once upon a time, and the room, and the room, and the room, and then, and the room, and the room, and the room, and the room, and the room, and the room, and the room, and the room, and the room\n",
            "Epoch 7 (Step 000078): Train loss 4.562,Val loss 6.387\n",
            "Epoch 7 (Step 000079): Train loss 4.573,Val loss 6.375\n",
            "Epoch 7 (Step 000080): Train loss 4.499,Val loss 6.366\n",
            "Epoch 7 (Step 000081): Train loss 4.516,Val loss 6.366\n",
            "Epoch 7 (Step 000082): Train loss 4.476,Val loss 6.380\n",
            "Epoch 7 (Step 000083): Train loss 4.447,Val loss 6.405\n",
            "Epoch 7 (Step 000084): Train loss 4.302,Val loss 6.396\n",
            "Epoch 7 (Step 000085): Train loss 4.341,Val loss 6.392\n",
            "Epoch 7 (Step 000086): Train loss 4.281,Val loss 6.389\n",
            "Epoch 7 (Step 000087): Train loss 4.219,Val loss 6.392\n",
            "Epoch 7 (Step 000088): Train loss 4.210,Val loss 6.381\n",
            "Epoch 7 (Step 000089): Train loss 4.105,Val loss 6.366\n",
            "Epoch 7 (Step 000090): Train loss 4.090,Val loss 6.366\n",
            "Once upon a time, and I was a little in the little in the man of the little in the sea.                          “Land,“\n",
            "Epoch 8 (Step 000091): Train loss 4.108,Val loss 6.387\n",
            "Epoch 8 (Step 000092): Train loss 3.956,Val loss 6.377\n",
            "Epoch 8 (Step 000093): Train loss 3.970,Val loss 6.381\n",
            "Epoch 8 (Step 000094): Train loss 3.887,Val loss 6.380\n",
            "Epoch 8 (Step 000095): Train loss 3.822,Val loss 6.382\n",
            "Epoch 8 (Step 000096): Train loss 3.859,Val loss 6.392\n",
            "Epoch 8 (Step 000097): Train loss 3.723,Val loss 6.409\n",
            "Epoch 8 (Step 000098): Train loss 3.708,Val loss 6.399\n",
            "Epoch 8 (Step 000099): Train loss 3.611,Val loss 6.381\n",
            "Epoch 8 (Step 000100): Train loss 3.563,Val loss 6.388\n",
            "Epoch 8 (Step 000101): Train loss 3.569,Val loss 6.401\n",
            "Epoch 8 (Step 000102): Train loss 3.554,Val loss 6.412\n",
            "Epoch 8 (Step 000103): Train loss 3.482,Val loss 6.406\n",
            "Once upon a time, and I thought I, I to be of a little in a little, and I had a good bed, and a little in a good to be, and then, and a little in a few to be sure, and I had a little in\n",
            "Epoch 9 (Step 000104): Train loss 3.441,Val loss 6.412\n",
            "Epoch 9 (Step 000105): Train loss 3.342,Val loss 6.411\n",
            "Epoch 9 (Step 000106): Train loss 3.308,Val loss 6.422\n",
            "Epoch 9 (Step 000107): Train loss 3.268,Val loss 6.441\n",
            "Epoch 9 (Step 000108): Train loss 3.216,Val loss 6.420\n",
            "Epoch 9 (Step 000109): Train loss 3.191,Val loss 6.448\n",
            "Epoch 9 (Step 000110): Train loss 3.154,Val loss 6.462\n",
            "Epoch 9 (Step 000111): Train loss 3.057,Val loss 6.474\n",
            "Epoch 9 (Step 000112): Train loss 3.028,Val loss 6.487\n",
            "Epoch 9 (Step 000113): Train loss 2.933,Val loss 6.470\n",
            "Epoch 9 (Step 000114): Train loss 2.921,Val loss 6.480\n",
            "Epoch 9 (Step 000115): Train loss 2.924,Val loss 6.508\n",
            "Epoch 9 (Step 000116): Train loss 2.842,Val loss 6.501\n",
            "Once upon a time, but I thought a little of the time, and a little in a good deal of a little.   “I was a man that,“’s a ship’s a harpooneer,’s\n",
            "Epoch 10 (Step 000117): Train loss 2.794,Val loss 6.502\n",
            "Epoch 10 (Step 000118): Train loss 2.709,Val loss 6.525\n",
            "Epoch 10 (Step 000119): Train loss 2.639,Val loss 6.543\n",
            "Epoch 10 (Step 000120): Train loss 2.659,Val loss 6.539\n",
            "Epoch 10 (Step 000121): Train loss 2.641,Val loss 6.536\n",
            "Epoch 10 (Step 000122): Train loss 2.508,Val loss 6.540\n",
            "Epoch 10 (Step 000123): Train loss 2.457,Val loss 6.554\n",
            "Epoch 10 (Step 000124): Train loss 2.460,Val loss 6.607\n",
            "Epoch 10 (Step 000125): Train loss 2.444,Val loss 6.597\n",
            "Epoch 10 (Step 000126): Train loss 2.319,Val loss 6.619\n",
            "Epoch 10 (Step 000127): Train loss 2.326,Val loss 6.650\n",
            "Epoch 10 (Step 000128): Train loss 2.286,Val loss 6.624\n",
            "Epoch 10 (Step 000129): Train loss 2.247,Val loss 6.626\n",
            "Once upon a time, but I had a good of a little in the time, for a good deal of a good deal.  But what was a very man-att, and a good, and a man of the land, and was a harpooneer\n",
            "Epoch 11 (Step 000130): Train loss 2.210,Val loss 6.613\n",
            "Epoch 11 (Step 000131): Train loss 2.155,Val loss 6.627\n",
            "Epoch 11 (Step 000132): Train loss 2.118,Val loss 6.625\n",
            "Epoch 11 (Step 000133): Train loss 2.061,Val loss 6.646\n",
            "Epoch 11 (Step 000134): Train loss 2.044,Val loss 6.671\n",
            "Epoch 11 (Step 000135): Train loss 1.967,Val loss 6.658\n",
            "Epoch 11 (Step 000136): Train loss 1.908,Val loss 6.673\n",
            "Epoch 11 (Step 000137): Train loss 1.882,Val loss 6.698\n",
            "Epoch 11 (Step 000138): Train loss 1.841,Val loss 6.733\n",
            "Epoch 11 (Step 000139): Train loss 1.780,Val loss 6.732\n",
            "Epoch 11 (Step 000140): Train loss 1.713,Val loss 6.719\n",
            "Epoch 11 (Step 000141): Train loss 1.704,Val loss 6.734\n",
            "Epoch 11 (Step 000142): Train loss 1.666,Val loss 6.734\n",
            "Once upon a time, but I had a little damp, as I had a good look at the bed, and all. I could not a man that I had been a good deal.    ’s sake this harpooneer is a very very\n",
            "Epoch 12 (Step 000143): Train loss 1.606,Val loss 6.763\n",
            "Epoch 12 (Step 000144): Train loss 1.564,Val loss 6.793\n",
            "Epoch 12 (Step 000145): Train loss 1.545,Val loss 6.791\n",
            "Epoch 12 (Step 000146): Train loss 1.468,Val loss 6.781\n",
            "Epoch 12 (Step 000147): Train loss 1.426,Val loss 6.795\n",
            "Epoch 12 (Step 000148): Train loss 1.397,Val loss 6.831\n",
            "Epoch 12 (Step 000149): Train loss 1.385,Val loss 6.841\n",
            "Epoch 12 (Step 000150): Train loss 1.364,Val loss 6.842\n",
            "Epoch 12 (Step 000151): Train loss 1.289,Val loss 6.827\n",
            "Epoch 12 (Step 000152): Train loss 1.245,Val loss 6.847\n",
            "Epoch 12 (Step 000153): Train loss 1.235,Val loss 6.855\n",
            "Epoch 12 (Step 000154): Train loss 1.159,Val loss 6.861\n",
            "Epoch 12 (Step 000155): Train loss 1.154,Val loss 6.898\n",
            "Once upon a time, ’t be afraid now,’t you’t sleep. I began to say, he’t harm a harpooneer is a harpooneer.  ’t sell to get just as you\n",
            "Epoch 13 (Step 000156): Train loss 1.111,Val loss 6.936\n",
            "Epoch 13 (Step 000157): Train loss 1.051,Val loss 6.945\n",
            "Epoch 13 (Step 000158): Train loss 1.034,Val loss 6.930\n",
            "Epoch 13 (Step 000159): Train loss 1.004,Val loss 6.946\n",
            "Epoch 13 (Step 000160): Train loss 0.960,Val loss 6.948\n",
            "Epoch 13 (Step 000161): Train loss 0.938,Val loss 6.988\n",
            "Epoch 13 (Step 000162): Train loss 0.888,Val loss 6.998\n",
            "Epoch 13 (Step 000163): Train loss 0.892,Val loss 7.009\n",
            "Epoch 13 (Step 000164): Train loss 0.834,Val loss 6.992\n",
            "Epoch 13 (Step 000165): Train loss 0.812,Val loss 6.995\n",
            "Epoch 13 (Step 000166): Train loss 0.823,Val loss 7.018\n",
            "Epoch 13 (Step 000167): Train loss 0.781,Val loss 7.032\n",
            "Epoch 13 (Step 000168): Train loss 0.731,Val loss 7.041\n",
            "Once upon a time, a little in a May me a little in the door of a second, or how conveyed—and duelled them to a long-drawn gurgling whistle of the heads of them, and jars are all—remember that—and hence, ship\n",
            "Epoch 14 (Step 000169): Train loss 0.715,Val loss 7.057\n",
            "Epoch 14 (Step 000170): Train loss 0.687,Val loss 7.088\n",
            "Epoch 14 (Step 000171): Train loss 0.663,Val loss 7.097\n",
            "Epoch 14 (Step 000172): Train loss 0.631,Val loss 7.120\n",
            "Epoch 14 (Step 000173): Train loss 0.655,Val loss 7.166\n",
            "Epoch 14 (Step 000174): Train loss 0.620,Val loss 7.167\n",
            "Epoch 14 (Step 000175): Train loss 0.573,Val loss 7.173\n",
            "Epoch 14 (Step 000176): Train loss 0.559,Val loss 7.192\n",
            "Epoch 14 (Step 000177): Train loss 0.520,Val loss 7.193\n",
            "Epoch 14 (Step 000178): Train loss 0.519,Val loss 7.190\n",
            "Epoch 14 (Step 000179): Train loss 0.508,Val loss 7.180\n",
            "Epoch 14 (Step 000180): Train loss 0.477,Val loss 7.203\n",
            "Epoch 14 (Step 000181): Train loss 0.467,Val loss 7.232\n",
            "Once upon a time, but I rolled, from the bed, and could not sleep for some reason a little more in the street shouldering my frame; and so on to the room from the whole city. But, for the landless gull, the harpooneer\n",
            "Epoch 15 (Step 000182): Train loss 0.443,Val loss 7.246\n",
            "Epoch 15 (Step 000183): Train loss 0.413,Val loss 7.257\n",
            "Epoch 15 (Step 000184): Train loss 0.407,Val loss 7.268\n",
            "Epoch 15 (Step 000185): Train loss 0.378,Val loss 7.297\n",
            "Epoch 15 (Step 000186): Train loss 0.372,Val loss 7.299\n",
            "Epoch 15 (Step 000187): Train loss 0.370,Val loss 7.320\n",
            "Epoch 15 (Step 000188): Train loss 0.351,Val loss 7.336\n",
            "Epoch 15 (Step 000189): Train loss 0.346,Val loss 7.365\n",
            "Epoch 15 (Step 000190): Train loss 0.316,Val loss 7.339\n",
            "Epoch 15 (Step 000191): Train loss 0.312,Val loss 7.373\n",
            "Epoch 15 (Step 000192): Train loss 0.287,Val loss 7.384\n",
            "Epoch 15 (Step 000193): Train loss 0.287,Val loss 7.408\n",
            "Epoch 15 (Step 000194): Train loss 0.275,Val loss 7.376\n",
            "Once upon a time, but I rolled about a good deal, and could not sleep for a long time. I had been his words, yet subsequent disclosures, I had lent him, and nothing was to be heard; but a supernatural hand seemed communing with a very great\n",
            "Epoch 16 (Step 000195): Train loss 0.279,Val loss 7.390\n",
            "Epoch 16 (Step 000196): Train loss 0.270,Val loss 7.447\n",
            "Epoch 16 (Step 000197): Train loss 0.245,Val loss 7.428\n",
            "Epoch 16 (Step 000198): Train loss 0.246,Val loss 7.448\n",
            "Epoch 16 (Step 000199): Train loss 0.241,Val loss 7.436\n",
            "Epoch 16 (Step 000200): Train loss 0.211,Val loss 7.429\n",
            "Epoch 16 (Step 000201): Train loss 0.218,Val loss 7.448\n",
            "Epoch 16 (Step 000202): Train loss 0.208,Val loss 7.471\n",
            "Epoch 16 (Step 000203): Train loss 0.191,Val loss 7.459\n",
            "Epoch 16 (Step 000204): Train loss 0.181,Val loss 7.468\n",
            "Epoch 16 (Step 000205): Train loss 0.182,Val loss 7.493\n",
            "Epoch 16 (Step 000206): Train loss 0.160,Val loss 7.501\n",
            "Epoch 16 (Step 000207): Train loss 0.149,Val loss 7.511\n",
            "Once upon a time, but I rolled about a good deal, and could not sleep for a long time.  But, besides the great Black Parliament sitting in a supper and a long time whipping me, as the land of Nod, that I could embark for my\n",
            "Epoch 17 (Step 000208): Train loss 0.166,Val loss 7.534\n",
            "Epoch 17 (Step 000209): Train loss 0.162,Val loss 7.531\n",
            "Epoch 17 (Step 000210): Train loss 0.136,Val loss 7.532\n",
            "Epoch 17 (Step 000211): Train loss 0.140,Val loss 7.549\n",
            "Epoch 17 (Step 000212): Train loss 0.130,Val loss 7.555\n",
            "Epoch 17 (Step 000213): Train loss 0.130,Val loss 7.570\n",
            "Epoch 17 (Step 000214): Train loss 0.126,Val loss 7.584\n",
            "Epoch 17 (Step 000215): Train loss 0.119,Val loss 7.597\n",
            "Epoch 17 (Step 000216): Train loss 0.108,Val loss 7.608\n",
            "Epoch 17 (Step 000217): Train loss 0.100,Val loss 7.628\n",
            "Epoch 17 (Step 000218): Train loss 0.106,Val loss 7.660\n",
            "Epoch 17 (Step 000219): Train loss 0.099,Val loss 7.676\n",
            "Epoch 17 (Step 000220): Train loss 0.092,Val loss 7.675\n",
            "Once upon a time, but I rolled about a good deal, and could not sleep for a long time. At last I slid off into a light doze, and had pretty nearly made a good offing towards the land of Nod, when I heard a heavy foot\n",
            "Epoch 18 (Step 000221): Train loss 0.086,Val loss 7.686\n",
            "Epoch 18 (Step 000222): Train loss 0.085,Val loss 7.701\n",
            "Epoch 18 (Step 000223): Train loss 0.075,Val loss 7.712\n",
            "Epoch 18 (Step 000224): Train loss 0.080,Val loss 7.734\n",
            "Epoch 18 (Step 000225): Train loss 0.079,Val loss 7.750\n",
            "Epoch 18 (Step 000226): Train loss 0.077,Val loss 7.758\n",
            "Epoch 18 (Step 000227): Train loss 0.075,Val loss 7.746\n",
            "Epoch 18 (Step 000228): Train loss 0.069,Val loss 7.746\n",
            "Epoch 18 (Step 000229): Train loss 0.073,Val loss 7.763\n",
            "Epoch 18 (Step 000230): Train loss 0.066,Val loss 7.759\n",
            "Epoch 18 (Step 000231): Train loss 0.073,Val loss 7.781\n",
            "Epoch 18 (Step 000232): Train loss 0.059,Val loss 7.746\n",
            "Epoch 18 (Step 000233): Train loss 0.058,Val loss 7.759\n",
            "Once upon a time, but I rolled about a good deal, and could not sleep for a long time. At last I slid off into a light doze, and had pretty nearly made a good offing towards the land of Nod, when I heard a heavy foot\n",
            "Epoch 19 (Step 000234): Train loss 0.055,Val loss 7.791\n",
            "Epoch 19 (Step 000235): Train loss 0.058,Val loss 7.819\n",
            "Epoch 19 (Step 000236): Train loss 0.053,Val loss 7.825\n",
            "Epoch 19 (Step 000237): Train loss 0.057,Val loss 7.838\n",
            "Epoch 19 (Step 000238): Train loss 0.055,Val loss 7.839\n",
            "Epoch 19 (Step 000239): Train loss 0.061,Val loss 7.820\n",
            "Epoch 19 (Step 000240): Train loss 0.050,Val loss 7.816\n",
            "Epoch 19 (Step 000241): Train loss 0.050,Val loss 7.837\n",
            "Epoch 19 (Step 000242): Train loss 0.049,Val loss 7.854\n",
            "Epoch 19 (Step 000243): Train loss 0.044,Val loss 7.870\n",
            "Epoch 19 (Step 000244): Train loss 0.045,Val loss 7.882\n",
            "Epoch 19 (Step 000245): Train loss 0.044,Val loss 7.884\n",
            "Epoch 19 (Step 000246): Train loss 0.040,Val loss 7.897\n",
            "Once upon a time, but I rolled about a good deal, and could not sleep for a long time.  But, besides the Feegeeans, the streets, and sprawling about the whirling heart of his toilet was soon achieved, and was sitting there quietly.\n",
            "Epoch 20 (Step 000247): Train loss 0.037,Val loss 7.905\n",
            "Epoch 20 (Step 000248): Train loss 0.042,Val loss 7.924\n",
            "Epoch 20 (Step 000249): Train loss 0.040,Val loss 7.942\n",
            "Epoch 20 (Step 000250): Train loss 0.042,Val loss 7.948\n",
            "Epoch 20 (Step 000251): Train loss 0.044,Val loss 7.955\n",
            "Epoch 20 (Step 000252): Train loss 0.036,Val loss 7.966\n",
            "Epoch 20 (Step 000253): Train loss 0.045,Val loss 7.987\n",
            "Epoch 20 (Step 000254): Train loss 0.041,Val loss 7.995\n",
            "Epoch 20 (Step 000255): Train loss 0.036,Val loss 7.985\n",
            "Epoch 20 (Step 000256): Train loss 0.030,Val loss 7.966\n",
            "Epoch 20 (Step 000257): Train loss 0.032,Val loss 7.967\n",
            "Epoch 20 (Step 000258): Train loss 0.035,Val loss 7.972\n",
            "Epoch 20 (Step 000259): Train loss 0.033,Val loss 7.966\n",
            "Once upon a time, but I rolled about a good deal, and could not sleep for a long time. At last I slid off into a light doze, and had pretty nearly made a good offing towards the land of Nod, when I heard a heavy foot\n"
          ]
        }
      ]
    }
  ]
}