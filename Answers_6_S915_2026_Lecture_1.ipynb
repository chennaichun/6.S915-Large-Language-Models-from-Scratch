{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "i8Mz3eVUaE71"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Be sure to make a copy of the notebook in your drive so you don't lose your work!"
      ],
      "metadata": {
        "id": "BCLAo8H1av5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concept Questions"
      ],
      "metadata": {
        "id": "i8Mz3eVUaE71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why do we use tokens to represent language?\n",
        "2. True or False: The only valid tokenizations are word and character-level tokenizations.\n",
        "3. What is the benefit of using token IDs instead of raw tokens?\n",
        "4. Determine whether the following are valid assignments of token IDs. If invalid, give a reason why.\n",
        "\n",
        "    a. `The quick brown fox jumps high` → `110, 125, 42, 71, 8, 96`\n",
        "\n",
        "    b. `The quick brown fox jumps high` → `143, 54, -73, 62, 83, 24`\n",
        "\n",
        "    c. `The quick brown fox jumps high` → `110, 125, 83, 71, 83, 96`\n",
        "\n",
        "    d. `The big plane flies over a small plane` → `54, 75, 86, 12, 77, 90, 123, 170`\n",
        "\n",
        "    e. `The big plane flies over a small plane` → `54, 75, 86, 12, 77, 90, 123, 86`\n",
        "\n",
        "5. Suppose the text is `The quick brown fox jumps high`. For each input context window, give a corresponding target context window.\n",
        "\n",
        "    a. `The quick brown fox`\n",
        "\n",
        "    b. `The quick brown`\n",
        "\n",
        "    c. `brown fox jumps`\n",
        "\n",
        "    d. `quick brown fox`\n",
        "\n",
        "6. Why do we use embeddings?"
      ],
      "metadata": {
        "id": "vmOiCZrEa2KM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. We want to be able to convert words or text in general into numbers since LLMs can easily process numbers but cannot directly process raw text.\n",
        "2. False. Word and character level tokenizations are two types of tokenizations, but there are also others (parts of words). Different LLMs can actually have different tokenization methods and even different dictionaries (e.g., ChatGPT might want to use a larger vocabulary than GPT we're making in this class).\n",
        "3. We want to index the words to convert them into numbers. Once we have the indices, it is easy for a model to map them to embeddings.\n",
        "4.\n",
        "\n",
        "  a. Valid. Each token is different and is mapped to a different token ID.\n",
        "\n",
        "  b. Invalid. Token IDs should be nonnegative.\n",
        "\n",
        "  c. Invalid. `brown` and `jumps` are both mapped to token ID 83, which violates the rule that different tokens are mapped to different token IDs.\n",
        "\n",
        "  d. Invalid. The word `plane` appears twice, and has two different token IDs, which violates the rule that the same token should be mapped to the same token ID.\n",
        "\n",
        "  e. Valid. Different tokens are mapped to different token IDs, and equivalent tokens are mapped to the same token ID.\n",
        "\n",
        "5.\n",
        "\n",
        "  a. `quick brown fox jumps`\n",
        "\n",
        "  b. `quick brown fox`\n",
        "\n",
        "  c. `fox jumps high`\n",
        "\n",
        "  d. `brown fox jumps`"
      ],
      "metadata": {
        "id": "Oyo56rUHfQBM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor Introduction"
      ],
      "metadata": {
        "id": "2qAmdjqZaMld"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnLyue_uorcH"
      },
      "source": [
        "Importing PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9EZNFqdHnAP2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMFzy7mEpAiX"
      },
      "source": [
        "Vectors are represented in PyTorch using **tensors**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fomCBA6FpWOC",
        "outputId": "1a6ed7d3-c182-434e-b490-43d31122cb6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 3, -7,  5])\n",
            "tensor([ 2.4000,  1.6000, -4.0000])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([3, -7, 5])\n",
        "print(x)\n",
        "y = torch.tensor([2.4, 1.6, -4])\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HXl97qBqF_X"
      },
      "source": [
        "Tensors of the same size can be added together (same as vector addition). Note that we have to run the previous cell for the following cell to work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXA9wEnSp9d9",
        "outputId": "ac3abbd4-6f7e-4f3a-d879-39eab26526ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 5.4000, -5.4000,  1.0000])\n",
            "tensor([ 0.6000, -8.6000,  9.0000])\n"
          ]
        }
      ],
      "source": [
        "z = x + y\n",
        "w = x - y\n",
        "print(z)\n",
        "print(w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU3S1wqqrD4R"
      },
      "source": [
        "Tensors of different sizes cannot be added or subtracted:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "idfLk45PrHGf"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([1, 2, 3])\n",
        "y = torch.tensor([3, 5, 7, 2])\n",
        "# print(x + y) # error\n",
        "# print(x - y) # error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhXiTIS7tgnZ"
      },
      "source": [
        "You can get the size of a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQtKau0PtnXV",
        "outputId": "e6defa8a-06d9-4794-8a1e-48bccb10cfd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([1, 5, 3, 6])\n",
        "s = x.shape\n",
        "print(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVQz6PVRs8MD"
      },
      "source": [
        "**Exercise 1:** Create three tensors of size 7. Then, create another tensor equal to the sum of those three tensors and print it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7vPJTyn7tLFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0095dc8b-69d8-493a-e8aa-37969150ad4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.5000, 13.3000, 13.8000, 15.6000, 14.2000,  2.4000, 27.0000])\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1: Your code here\n",
        "a = torch.tensor([1, 2, 3, 4, 5, 6, 7])\n",
        "b = torch.tensor([-5, 6.7, 3.5, 2.4, 5.8, 1, 5])\n",
        "c = torch.tensor([2.5, 4.6, 7.3, 9.2, 3.4, -4.6, 15])\n",
        "d = a + b + c\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU9vmItSqanW"
      },
      "source": [
        "Tensors can also be multiplied by a constant (same as vector scalar multiplication):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJ8Udxg-qjq2",
        "outputId": "ea902402-1471-4068-eac0-35afed9795ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 9.0000,  4.0000,  3.2000, 14.0000])\n",
            "tensor([-16.6500,  -7.4000,  -5.9200, -25.9000])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([4.5, 2, 1.6, 7])\n",
        "y = 2 * x\n",
        "z = -3.7 * x\n",
        "print(y)\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB8wkZqlsnrf"
      },
      "source": [
        "Tensors of the same size can be multiplied together (component-wise multiplication):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Flrm1PeqshPc",
        "outputId": "95a08b59-9cea-4d67-8dc1-e88f7ee6287f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2,  6, 20])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([1, 2, 5])\n",
        "y = torch.tensor([2, 3, 4])\n",
        "print(x * y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgaTqkvjsxAp"
      },
      "source": [
        "You can also take the dot product of two tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAztEJh2s1Dd",
        "outputId": "f54ae26d-d354-4086-df79-f955c91e4bf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(28)\n"
          ]
        }
      ],
      "source": [
        "z = x @ y\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4oIHPgnt8mC"
      },
      "source": [
        "You can get each component of a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMsZwd4IuULg",
        "outputId": "9133153d-25ce-46fc-acc8-6c440b7f8158"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3)\n",
            "tensor(4)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor([3, 4])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([3, 4, 1])\n",
        "print(x[0])\n",
        "print(x[1])\n",
        "print(x[2])\n",
        "print(x[-1])\n",
        "print(x[0:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP2oVhoRwtgd"
      },
      "source": [
        "**Exercise 2:** Create a function that takes a list of tensors and returns the number of tensors that represent a vector of 3 components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "t3Gt9s0Gxhdm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69be2d9f-864f-47b7-87a3-71c1de946508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "def get_3_component_vectors(tensor_list):\n",
        "  # Your code here\n",
        "  count = 0\n",
        "  for t in tensor_list:\n",
        "    if t.shape == torch.Size([3]):\n",
        "      count += 1\n",
        "  return count\n",
        "\n",
        "tl = [torch.tensor([6]), torch.tensor([1, 2, 3]), torch.tensor([2, 3, 5]), torch.tensor([1, 3, 6, 8])]\n",
        "print(get_3_component_vectors(tl))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQv16s86u2tP"
      },
      "source": [
        "You can change the type of a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oc5fvkHnu5lq",
        "outputId": "97b441bf-ed5f-4541-80b6-5cde4bcb2e3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([3, 4, 1])\n",
            "tensor([3., 4., 1.])\n",
            "tensor([3, 4, 1], dtype=torch.int32)\n"
          ]
        }
      ],
      "source": [
        "print(x)\n",
        "y = x.float()\n",
        "print(y)\n",
        "z = y.int()\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZHD7gKIui1V"
      },
      "source": [
        "You can get the magnitude of a tensor that represents a vector (needs a float tensor):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qxzd2E4Kulnv",
        "outputId": "2899ab06-917a-4e23-b909-7732f97c54b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(5.0990)\n"
          ]
        }
      ],
      "source": [
        "print(torch.linalg.norm(x.float()))\n",
        "# print(torch.linalg.norm(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC1knJtTxyEH"
      },
      "source": [
        "**Exercise 3:** Create a function that takes in a list of tensors representing vectors and returns the one with largest magnitude."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "rrb1AwKryW93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa247c12-e86c-45e7-b122-eac39b46f031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([6, 6])\n"
          ]
        }
      ],
      "source": [
        "def largest_magnitude_tensor(tensor_list):\n",
        "  # Your code here\n",
        "  result = None\n",
        "  for t in tensor_list:\n",
        "    if result is None or torch.linalg.norm(t.float()) > torch.linalg.norm(result.float()):\n",
        "      result = t\n",
        "  return result\n",
        "\n",
        "tl = [torch.tensor([5]), torch.tensor([4, 6]), torch.tensor([7]), torch.tensor([6, 6]), torch.tensor([2, 2, 2, 2, 2, 2, 2])]\n",
        "print(largest_magnitude_tensor(tl))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oc4I61w-vFYV"
      },
      "source": [
        "Tensors can also be multidimensional themselves, which is useful for representing context windows or batches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FINegYLTvPi5",
        "outputId": "53540fbb-940b-4b36-904c-5c14195526f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3, 4, 2],\n",
            "        [7, 5, 6]])\n",
            "torch.Size([2, 3])\n",
            "tensor([[[ 45,  78,  65],\n",
            "         [ 24, 123, 207]],\n",
            "\n",
            "        [[ 23,   0,  50],\n",
            "         [255, 127,   6]]])\n",
            "torch.Size([2, 2, 3])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([[3, 4, 2], [7, 5, 6]])\n",
        "print(x)\n",
        "print(x.shape)\n",
        "y = torch.tensor([[[45, 78, 65], [24, 123, 207]], [[23, 0, 50], [255, 127, 6]]]) # 2x2x3 image (2x2 colored image)\n",
        "print(y)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2ZCvUCO2vzW"
      },
      "source": [
        "You can get a specific dimension of the size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH1PLXz72zcP",
        "outputId": "ced35f6e-ca66-47e7-86b8-0aef630d8ea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3, 4, 2],\n",
            "        [7, 5, 6]])\n",
            "torch.Size([2, 3])\n",
            "2\n",
            "3\n",
            "<class 'int'>\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([[3, 4, 2], [7, 5, 6]]) # 2x3 greyscale image\n",
        "print(x)\n",
        "print(x.shape)\n",
        "print(x.shape[0])\n",
        "print(x.shape[1])\n",
        "print(type(x.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sG-wOrsxGaR"
      },
      "source": [
        "**Check your knowledge:**\n",
        "\n",
        "What is the size of the tensor <font face=\"Courier New\">torch.tensor([5, 7, 1, 6, 8, 2])</font>?\n",
        "\n",
        "What is the size of the tensor <font face=\"Courier New\">torch.tensor([[2, 3, 5, 7], [4, 3, -1, 6]])</font>?\n",
        "\n",
        "What is the size of the tensor <font face=\"Courier New\">torch.tensor([[[5, 2], [1, 2]], [[3, 4], [5, 6]]])</font>?\n",
        "\n",
        "Suppose x is a 5-element 1D tensor. How do you get the number of elements in x as an int instead of a <font face=\"Courier New\">torch.Size</font> object?\n",
        "\n",
        "Suppose x is a 5x4 tensor. How do you get the number of rows in x? What about columns?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 6\n",
        "2. 2x4\n",
        "3. 2x2x2\n",
        "4. x.shape[0]\n",
        "\n",
        "(x.shape is torch.Size([5]), and when we index into it we get an int)\n",
        "\n",
        "5. x.shape[0], x.shape[1]"
      ],
      "metadata": {
        "id": "McVQkAanBz5v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdeEOTGivXzS"
      },
      "source": [
        "You can flatten a multidimensional tensor into a vector tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXjfz9iov6Y3",
        "outputId": "04a63e1e-97cc-45a9-d7e8-6490eae88dfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3, 4, 2, 7, 5, 6])\n",
            "torch.Size([6])\n"
          ]
        }
      ],
      "source": [
        "x_flat = x.flatten()\n",
        "print(x_flat)\n",
        "print(x_flat.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKjS7QGMwScv"
      },
      "source": [
        "Flattening also works for tensors with 3 or more dimensions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clP2xIwCwXx4",
        "outputId": "cf84af63-98dc-4c7e-c2ca-5adde8e033a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 45,  78,  65,  24, 123, 207,  23,   0,  50, 255, 127,   6])\n",
            "torch.Size([12])\n"
          ]
        }
      ],
      "source": [
        "y_flat = y.flatten()\n",
        "print(y_flat)\n",
        "print(y_flat.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VJdg0a5y7Eg"
      },
      "source": [
        "**Exercise 4:** Create a function that takes a list of tensors and returns a copy of the list except with all elements flattened."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "SODHsPwpzGOq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd4ef206-2fa1-4ca5-e7a1-ee0791b74564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([2, 3, 4]), tensor([2, 3, 4, 5, 6, 7, 9, 8, 7]), tensor([1, 2, 3, 4, 5, 6, 7, 8])]\n"
          ]
        }
      ],
      "source": [
        "def all_flattened(tensor_list):\n",
        "  # Your code here\n",
        "  result = []\n",
        "  for t in tensor_list:\n",
        "    result.append(t.flatten())\n",
        "  return result\n",
        "\n",
        "tl = [torch.tensor([2, 3, 4]), torch.tensor([[2, 3, 4], [5, 6, 7], [9, 8, 7]]), torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])]\n",
        "print(all_flattened(tl))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGmjuUpXzVtD"
      },
      "source": [
        "You can take the transpose of a tensor with two dimensions (swaps the two dimensions):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiM-hfXvzjiN",
        "outputId": "56470e18-2f74-4cc6-a6c3-b12d5a090670"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3, 7],\n",
            "        [4, 5],\n",
            "        [2, 6]])\n",
            "torch.Size([3, 2])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([[3, 4, 2], [7, 5, 6]]) # 2x3 greyscale image\n",
        "y = x.T # 3x2 greyscale image\n",
        "print(y)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOk7XfKNz0k2"
      },
      "source": [
        "You can swap the dimensions of a 3D tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZn3sV33z4H7",
        "outputId": "5fd33326-b0d3-4a72-95b2-a33e0ea4c8a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 45,  24],\n",
            "         [ 78, 123],\n",
            "         [ 65, 207]],\n",
            "\n",
            "        [[ 23, 255],\n",
            "         [  0, 127],\n",
            "         [ 50,   6]]])\n",
            "tensor([[[ 45,  78,  65],\n",
            "         [ 23,   0,  50]],\n",
            "\n",
            "        [[ 24, 123, 207],\n",
            "         [255, 127,   6]]])\n"
          ]
        }
      ],
      "source": [
        "y = torch.tensor([[[45, 78, 65], [24, 123, 207]], [[23, 0, 50], [255, 127, 6]]])\n",
        "print(torch.transpose(y, 1, 2)) # different size\n",
        "print(torch.transpose(y, 0, 1)) # note this is the same size but a different tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-CqcmxOycn5"
      },
      "source": [
        "**Check your knowledge:**\n",
        "\n",
        "Suppose x is a 2x3x4 tensor. What is the size of <font face=\"Courier New\">torch.transpose(x, 1, 2)</font>? What about <font face=\"Courier New\">torch.transpose(x, 0, 1)</font>?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2x4x3, 3x2x4"
      ],
      "metadata": {
        "id": "BtawNlzLCMKF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2CwhecB0eqH"
      },
      "source": [
        "You can easily create a tensor filled with zeros:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSaTNwYl0h2j",
        "outputId": "115d490c-6172-4125-e183-1dee59b06d1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0., 0., 0., 0., 0.])\n",
            "tensor([[0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.]])\n",
            "tensor([[[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.zeros(5)\n",
        "y = torch.zeros(3, 5)\n",
        "z = torch.zeros(2, 3, 4)\n",
        "print(x)\n",
        "print(y)\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqvL892w0r5Q"
      },
      "source": [
        "You can also easily create a tensor filled with ones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXbGGw2N0yyW",
        "outputId": "67ccd939-9e60-4f29-b1c1-11eba844224c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1.])\n",
            "tensor([[1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.]])\n",
            "tensor([[[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.ones(5)\n",
        "y = torch.ones(3, 5)\n",
        "z = torch.ones(2, 3, 4)\n",
        "print(x)\n",
        "print(y)\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HvZp0p4zChc"
      },
      "source": [
        "**Check your knowledge:**\n",
        "\n",
        "What is the magnitude of <font face=\"Courier New\">torch.zeros(5)</font>? What about <font face=\"Courier New\">torch.ones(5)</font>?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0, $\\sqrt{5}$"
      ],
      "metadata": {
        "id": "_6WvGmsyCQ-n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "421qI8qK03ok"
      },
      "source": [
        "You can reshape tensors into the size you want:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-elr4ojB07Mt",
        "outputId": "5831e6b4-f498-493e-fd75-d62f94c697d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
            "tensor([[1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[[1., 1.],\n",
            "         [1., 1.]],\n",
            "\n",
            "        [[1., 1.],\n",
            "         [1., 1.]],\n",
            "\n",
            "        [[1., 1.],\n",
            "         [1., 1.]]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.ones(12)\n",
        "print(x)\n",
        "y = x.resize_((2, 6))\n",
        "print(y)\n",
        "z = y.resize_((4, 3))\n",
        "print(z)\n",
        "w = z.resize_((3, 2, 2))\n",
        "print(w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWcIFW4ZzQ2U"
      },
      "source": [
        "**Check your knowledge:**\n",
        "\n",
        "Can you resize a 8x3 tensor into a 3x4x2 tensor? What about 3x5x2?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes\n",
        "\n",
        "No (Technically yes, but you don't want to do that)"
      ],
      "metadata": {
        "id": "u0bCBW5wCUAG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq-6vK9d1deO"
      },
      "source": [
        "You can combine two vectors if their sizes are compatible:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HWggErh15le",
        "outputId": "06d8b7b1-3c9f-4ac6-ccea-039b39706604"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "torch.Size([16])\n"
          ]
        }
      ],
      "source": [
        "a = torch.zeros(5)\n",
        "b = torch.zeros(11)\n",
        "c = torch.cat([a, b])\n",
        "print(c)\n",
        "print(c.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN93QC732Wcg"
      },
      "source": [
        "Every dimension must match except for the specified one, and the number of dimensions must match:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNJlikKM2FRR",
        "outputId": "0e70b4f5-34cf-4bb2-aaf8-5ea1572b5648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0.]])\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0.]])\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0.]])\n",
            "torch.Size([8, 7])\n"
          ]
        }
      ],
      "source": [
        "a = torch.zeros((5, 7))\n",
        "b = torch.zeros((3, 7))\n",
        "c = torch.cat([a, b], dim=0)\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)\n",
        "print(c.shape)\n",
        "#d = torch.cat([a, b], dim=1) # error since dimension 0 doesn't match (5 != 3)\n",
        "#e = torch.cat(torch.zeros((2, 4)), torch.zeros((2, 4, 1)))\n",
        "# error since a 2D tensor cannot be concatenated with a 3D tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BawO3q2P27YS"
      },
      "source": [
        "**Exercise 5:** Create a function that takes a list of tensors and a specified dimension and returns whether they can be concatenated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoQpkGqU3Ivy"
      },
      "outputs": [],
      "source": [
        "def can_concatenate(tensor_list, dim):\n",
        "  # Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz2Fn4Og1avp"
      },
      "source": [
        "**Exercise 6:** Create a function that takes a list of tensors and returns the dot product of the two vectors with largest magnitude."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ITXJBUa1a5M"
      },
      "outputs": [],
      "source": [
        "def largest_magnitude_dot_product(tensor_list):\n",
        "  # Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Data"
      ],
      "metadata": {
        "id": "gFxIKM-OZ8d_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "# ! in colab means a terminal command\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWkPj1JZqNgg",
        "outputId": "ade1d7ca-a3f8-4515-8905-b1f09657a556"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import tiktoken\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Dataset class\n",
        "class MyData(Dataset):\n",
        "    # Init function, called when the dataset is created\n",
        "    # dataset = MyData(text, tokenizer, context_length=4, stride=1)\n",
        "    def __init__(self, text, tokenizer, context_length, stride=1):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "        token_ids = tokenizer.encode(text)\n",
        "        for i in range(0, len(token_ids) - context_length, stride):\n",
        "            self.input_ids.append(torch.tensor(token_ids[i : i + context_length]))\n",
        "            self.target_ids.append(torch.tensor(token_ids[i + 1 : i + context_length + 1]))\n",
        "\n",
        "    # Length function\n",
        "    # len(dataset)\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    # Get item function\n",
        "    # dataset[idx]\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "RUNIBJxkqm1A"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello my name is Daniel Li\"\n",
        "dataset = MyData(text, tokenizer, context_length=4, stride=1)\n",
        "print(dataset[0]) # First input, target pair\n",
        "print(dataset[1]) # Second input, target pair\n",
        "print(dataset[0][0]) # First input window\n",
        "print(dataset[0][1]) # First target window\n",
        "print(dataset[0][1][2]) # Third word of first target window"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqEM3lx0tl2V",
        "outputId": "09284350-8d4b-42d4-c2b8-9855ea53b396"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([15496,   616,  1438,   318]), tensor([ 616, 1438,  318, 7806]))\n",
            "(tensor([ 616, 1438,  318, 7806]), tensor([1438,  318, 7806, 7455]))\n",
            "tensor([15496,   616,  1438,   318])\n",
            "tensor([ 616, 1438,  318, 7806])\n",
            "tensor(318)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def my_batch(text, batch_size, context_length, stride, shuffle=True, drop_last=True, num_workers=0):\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create the dataset object\n",
        "    dataset = MyData(text, tokenizer, context_length, stride)\n",
        "\n",
        "    # Use the DataLoader library to create a dataloader that batches the data\n",
        "    dataloader = DataLoader(dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=shuffle,\n",
        "                            drop_last=drop_last,\n",
        "                            num_workers=num_workers)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "text = \"Hello my name is Daniel Li and I am currently teaching how to build an LLM\"\n",
        "dataloader = my_batch(text, batch_size=2, context_length=4, stride=1)\n",
        "for batch in dataloader:\n",
        "    print(batch)\n",
        "    input = batch[0]    # 2x4 tensor where 2 is batch size and 4 is context length\n",
        "    target = batch[1]\n",
        "    # print(input)\n",
        "    # print(target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQEjZhJEuU68",
        "outputId": "3dfb64e9-6f05-4c71-d04a-c49272ebe3bc"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 716, 3058, 7743,  703],\n",
            "        [7743,  703,  284, 1382]]), tensor([[3058, 7743,  703,  284],\n",
            "        [ 703,  284, 1382,  281]])]\n",
            "[tensor([[15496,   616,  1438,   318],\n",
            "        [  290,   314,   716,  3058]]), tensor([[ 616, 1438,  318, 7806],\n",
            "        [ 314,  716, 3058, 7743]])]\n",
            "[tensor([[ 314,  716, 3058, 7743],\n",
            "        [1438,  318, 7806, 7455]]), tensor([[ 716, 3058, 7743,  703],\n",
            "        [ 318, 7806, 7455,  290]])]\n",
            "[tensor([[ 7806,  7455,   290,   314],\n",
            "        [  284,  1382,   281, 27140]]), tensor([[ 7455,   290,   314,   716],\n",
            "        [ 1382,   281, 27140,    44]])]\n",
            "[tensor([[3058, 7743,  703,  284],\n",
            "        [ 616, 1438,  318, 7806]]), tensor([[7743,  703,  284, 1382],\n",
            "        [1438,  318, 7806, 7455]])]\n",
            "[tensor([[ 703,  284, 1382,  281],\n",
            "        [ 318, 7806, 7455,  290]]), tensor([[  284,  1382,   281, 27140],\n",
            "        [ 7806,  7455,   290,   314]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input, target in dataloader:\n",
        "    # print(input)\n",
        "    # print(target)\n",
        "    pass"
      ],
      "metadata": {
        "id": "P1luuE7RyMoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.manual_seed(1337)\n",
        "\n",
        "vocab_size = 50257 # Number of different tokens in our vocabulary\n",
        "output_size = 256 # Length of each vector we are mapping the tokens to\n",
        "# For example, \"My\" gets mapped to a 256-dimension vector\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_size)\n",
        "# 1593 -> [0, 0, 0, 0, ..., 0, 0, 1, 0, 0, ..., 0, 0, 0, 0]\n",
        "# 1 is at index 1593, everything else is 0\n",
        "# Apply Embedding:\n",
        "# Word transforms from 50257-dim vector to 256-dim vector\n",
        "# Multiply the 50257-dim vector by a 256x50257 size matrix\n",
        "\n",
        "first_input_batch, first_target_batch = next(iter(dataloader))\n",
        "print(first_input_batch.shape)\n",
        "first_input_token_embeddings = embedding_layer(first_input_batch)\n",
        "# Embedding layers (and other things in Pytorch) automatically take the batch\n",
        "# dimension into account\n",
        "# Example: Batch size 8\n",
        "first_target_token_embeddings = embedding_layer(first_target_batch)\n",
        "print(first_input_token_embeddings)\n",
        "print(first_input_token_embeddings.shape) # [2, 4, 256]\n",
        "# 2 is batch size\n",
        "# 4 is context size\n",
        "# 256 is embedding size\n",
        "# Embedding changed 2x4 into 2x4x256"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRtyOakLyWLW",
        "outputId": "d1f9091d-0266-439b-b39e-ce092ad1e47c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4])\n",
            "tensor([[[ 1.2685, -0.4092, -1.0066,  ...,  0.3432, -0.2911, -0.2079],\n",
            "         [ 0.0692,  0.5860,  0.6057,  ..., -0.8112, -0.3368,  0.3634],\n",
            "         [-0.3293, -2.1463,  0.4657,  ...,  0.5346,  0.4552,  0.4563],\n",
            "         [-0.7555, -1.1228,  0.4135,  ..., -0.6309,  2.3381, -0.0665]],\n",
            "\n",
            "        [[-0.3141, -0.5543, -0.2461,  ..., -1.0251, -1.3119,  0.1267],\n",
            "         [ 0.9996,  0.6607,  1.0238,  ..., -1.3405,  1.4056, -0.1574],\n",
            "         [ 0.2652,  0.4399,  0.6939,  ...,  0.2246,  0.3664,  0.4520],\n",
            "         [ 0.1291, -0.9978,  0.2440,  ..., -0.0531,  0.8474, -0.2260]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([2, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = 4\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_size)\n",
        "first_pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
        "print(torch.arange(4))\n",
        "print(first_pos_embeddings)\n",
        "print(first_pos_embeddings.shape)\n",
        "# Position embedding is the same for every batch so there is no batch dimension"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKQqEk5v3XXK",
        "outputId": "b9424c6c-a46a-48d0-aa19-24f66979b8e8"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 2, 3])\n",
            "tensor([[-1.1189, -0.1744, -1.2138,  ..., -0.9903,  0.8553, -0.9537],\n",
            "        [ 0.0557,  0.5771, -0.9318,  ..., -0.3697,  0.2683, -1.1061],\n",
            "        [-0.1161,  0.6960, -1.9962,  ...,  0.8197,  1.2191, -2.9642],\n",
            "        [ 0.6579,  1.0407, -0.3774,  ...,  0.8236, -0.7525, -0.9307]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(first_pos_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2A8OfI94Sya",
        "outputId": "2a0d3fa5-6aa5-42c5-8c4e-ae10f4a3f71a"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.arange(4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AnouTGK31px",
        "outputId": "4862fea2-bf73-4e3b-8788-c99e0c7226d6"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_input_embedding = first_input_token_embeddings + first_pos_embeddings\n",
        "# We're adding 2x4x256 to 4x256\n",
        "# Pytorch treats the 4x256 as a 2x4x256 composed of two copies of the 4x256\n",
        "first_target_embedding = first_target_token_embeddings + first_pos_embeddings\n",
        "print(first_input_embedding)\n",
        "print(first_input_embedding.shape)\n",
        "print(first_target_embedding)\n",
        "print(first_target_embedding.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lrfjr8VB4Gob",
        "outputId": "85230c30-fa31-4b63-be37-05bd15be8e67"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1497, -0.5836, -2.2204,  ..., -0.6470,  0.5641, -1.1616],\n",
            "         [ 0.1249,  1.1631, -0.3261,  ..., -1.1809, -0.0685, -0.7427],\n",
            "         [-0.4454, -1.4503, -1.5305,  ...,  1.3544,  1.6743, -2.5079],\n",
            "         [-0.0976, -0.0821,  0.0360,  ...,  0.1928,  1.5856, -0.9973]],\n",
            "\n",
            "        [[-1.4330, -0.7287, -1.4599,  ..., -2.0154, -0.4566, -0.8270],\n",
            "         [ 1.0553,  1.2378,  0.0921,  ..., -1.7102,  1.6739, -1.2635],\n",
            "         [ 0.1491,  1.1360, -1.3023,  ...,  1.0443,  1.5854, -2.5121],\n",
            "         [ 0.7870,  0.0430, -0.1334,  ...,  0.7706,  0.0949, -1.1568]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "torch.Size([2, 4, 256])\n",
            "tensor([[[-1.0497,  0.4116, -0.6081,  ..., -1.8015,  0.5185, -0.5903],\n",
            "         [-0.2737, -1.5692, -0.4661,  ...,  0.1649,  0.7235, -0.6499],\n",
            "         [-0.8716, -0.4268, -1.5827,  ...,  0.1889,  3.5572, -3.0307],\n",
            "         [ 2.3901,  1.1450,  0.2383,  ...,  0.9267, -2.1957, -1.0663]],\n",
            "\n",
            "        [[-0.1192,  0.4862, -0.1899,  ..., -2.3308,  2.2609, -1.1111],\n",
            "         [ 0.3208,  1.0170, -0.2379,  ..., -0.1451,  0.6346, -0.6541],\n",
            "         [ 0.0130, -0.3017, -1.7521,  ...,  0.7667,  2.0665, -3.1902],\n",
            "         [ 1.9264,  0.6316, -1.3841,  ...,  1.1669, -1.0436, -1.1387]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "torch.Size([2, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises"
      ],
      "metadata": {
        "id": "GN6VBSTphl4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 7:** Create your own text (must be at least 10 words long). Make a dataloader of the text with batch size 3, context size 5, and stride 1."
      ],
      "metadata": {
        "id": "O9eZjYEIhsj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_text = \"This is a completely useless sentence that is over ten words long\"\n",
        "# Your code here\n",
        "loader = my_batch(text, batch_size=3, context_length=5, stride=1)"
      ],
      "metadata": {
        "id": "eHTY8yuXi9yn"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 8:** Make input and target embeddings of the text with embedding size 512. Be sure to utilize both token and position embeddings."
      ],
      "metadata": {
        "id": "cZFSz-M_jORz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "tok_emb = torch.nn.Embedding(50257, 512)\n",
        "pos_emb = torch.nn.Embedding(50257, 512)\n",
        "input_embeddings = []\n",
        "target_embeddings = []\n",
        "for input, target in loader:\n",
        "  input_embedding = tok_emb(input) + pos_emb(torch.arange(5))\n",
        "  target_embedding = tok_emb(target) + pos_emb(torch.arange(5))\n",
        "  input_embeddings.append(input_embedding)\n",
        "  target_embeddings.append(target_embedding)\n",
        "\n",
        "\n",
        "print(input_embeddings)\n",
        "print(target_embeddings)"
      ],
      "metadata": {
        "id": "s6nECH1Ojk_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26e7bf1b-de75-46d8-81d7-f35772fc3b0d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[[ 2.6686, -1.1294,  0.4105,  ..., -1.2605,  0.3527,  1.1787],\n",
            "         [-0.1549,  0.4581, -0.0148,  ..., -0.0151,  2.3719,  1.2134],\n",
            "         [ 1.3821, -0.0352,  0.4479,  ...,  0.6514,  0.1744,  3.3764],\n",
            "         [-1.0268,  0.2352,  0.4442,  ...,  0.0957,  0.9448,  1.6880],\n",
            "         [-1.2771,  0.4618,  1.0017,  ..., -0.8073, -0.2826, -3.4715]],\n",
            "\n",
            "        [[ 1.3718,  0.6366,  1.5172,  ...,  1.1867,  1.2931,  0.1920],\n",
            "         [ 0.1375,  0.8624, -0.2414,  ...,  0.8072,  0.9970,  3.0352],\n",
            "         [-0.4100, -0.3262, -0.6987,  ...,  1.8427,  0.1628,  2.6847],\n",
            "         [ 0.7345,  0.3516,  0.0804,  ..., -0.6139,  0.3109,  0.5245],\n",
            "         [-0.2748, -0.7117, -1.0574,  ...,  0.6490, -0.5392, -2.0689]],\n",
            "\n",
            "        [[ 0.5294,  0.5207,  0.9040,  ...,  0.3925,  0.0146,  1.1417],\n",
            "         [-0.6442,  1.0822,  1.1887,  ...,  0.5585,  0.6905,  1.9299],\n",
            "         [ 0.6552, -1.1436, -0.8208,  ...,  1.6406, -0.6324,  0.8812],\n",
            "         [ 1.0263, -0.6653,  0.4868,  ..., -0.1060, -0.2947,  1.1716],\n",
            "         [ 0.7992, -1.5444, -1.4289,  ..., -0.8019,  0.5941, -0.2284]]],\n",
            "       grad_fn=<AddBackward0>), tensor([[[ 1.8594e+00, -1.7000e-01,  2.2639e+00,  ...,  1.0325e-01,\n",
            "           5.9920e-01,  1.0766e+00],\n",
            "         [ 4.5688e-01, -8.9409e-01, -2.5835e-01,  ..., -2.9730e-01,\n",
            "          -1.6163e-01,  2.6791e+00],\n",
            "         [ 1.1174e+00, -6.2417e-01, -9.4870e-01,  ...,  4.7195e-01,\n",
            "           3.0344e+00,  1.2561e+00],\n",
            "         [ 1.4885e+00,  3.1652e-01,  2.7353e-01,  ..., -1.0081e+00,\n",
            "          -8.6767e-01,  1.9284e+00],\n",
            "         [-2.0631e+00,  3.1547e-01,  3.8194e-01,  ...,  7.6349e-01,\n",
            "           2.0801e+00,  1.8586e-01]],\n",
            "\n",
            "        [[ 5.9229e-01,  8.7685e-01,  2.8410e+00,  ..., -1.2659e+00,\n",
            "          -5.2397e-01, -2.0644e+00],\n",
            "         [-3.2794e-01, -2.5160e-01,  4.9932e-01,  ..., -1.9096e-03,\n",
            "          -7.7909e-02,  3.1847e+00],\n",
            "         [ 3.4265e-01, -3.3550e-01, -9.5688e-01,  ...,  3.2931e+00,\n",
            "           2.8007e+00,  1.0145e+00],\n",
            "         [ 8.0209e-01, -4.4879e-01,  3.7509e-01,  ..., -1.3335e+00,\n",
            "           9.4846e-02, -4.7142e-01],\n",
            "         [ 1.8154e-01, -6.8826e-01, -5.5116e-01,  ...,  4.0380e-01,\n",
            "           1.1722e+00, -2.6135e-01]],\n",
            "\n",
            "        [[-1.9367e-01,  7.3051e-01,  2.2213e+00,  ...,  3.0494e-01,\n",
            "           1.8387e+00,  1.5930e+00],\n",
            "         [-1.6194e+00,  1.1121e+00,  2.1722e+00,  ..., -3.0272e-01,\n",
            "          -1.0383e+00, -5.6398e-01],\n",
            "         [ 9.4433e-01, -1.3338e+00, -4.3462e-01,  ...,  4.8513e-01,\n",
            "           5.8460e-01,  3.2273e+00],\n",
            "         [ 4.4902e-01,  1.6188e-02, -1.1312e+00,  ...,  1.6337e+00,\n",
            "           1.7586e+00, -4.3351e-01],\n",
            "         [-2.3422e-01, -3.6857e-01,  3.1285e-01,  ..., -6.6568e-01,\n",
            "           1.2301e+00, -1.9736e+00]]], grad_fn=<AddBackward0>), tensor([[[ 1.5675,  0.8469,  1.8575,  ..., -0.4047,  1.2048,  0.4295],\n",
            "         [-0.6171, -0.0613,  0.1132,  ...,  1.1536, -1.2949,  0.8385],\n",
            "         [ 0.9199, -1.0169,  0.6612,  ...,  1.5535,  0.7473,  2.6196],\n",
            "         [ 1.8355, -1.6246, -1.3666,  ..., -1.4697, -0.5412,  1.2737],\n",
            "         [ 0.1875, -0.1923, -1.1853,  ..., -0.5197,  3.1276, -1.6941]],\n",
            "\n",
            "        [[ 2.3216,  0.8118,  2.0506,  ..., -0.7988,  0.0263,  1.8334],\n",
            "         [-2.4054,  0.9658,  1.5525,  ...,  1.2681,  1.3244,  3.0934],\n",
            "         [-0.3472,  0.0299,  1.2382,  ...,  0.1843, -0.3758, -0.5213],\n",
            "         [ 1.0507, -0.9821, -0.6090,  ..., -1.1743, -0.4575,  1.7793],\n",
            "         [-0.5873,  0.0964, -1.1935,  ...,  2.3015,  2.8939, -1.9357]],\n",
            "\n",
            "        [[ 1.5946, -0.2966,  0.7820,  ...,  0.1904, -0.7805, -0.6618],\n",
            "         [-0.3523,  0.0653,  1.5951,  ...,  1.0664,  0.0848,  2.5770],\n",
            "         [ 1.7292, -1.9763, -1.1923,  ...,  0.1897,  0.5009,  2.7217],\n",
            "         [ 1.2238, -0.2725, -1.1230,  ..., -1.1875,  1.9923, -0.1919],\n",
            "         [ 0.4522,  0.3967,  0.2113,  ..., -0.3403,  0.2676,  0.4263]]],\n",
            "       grad_fn=<AddBackward0>), tensor([[[ 2.3492e+00,  6.2712e-01,  4.2745e-01,  ..., -1.5601e-01,\n",
            "           1.5114e+00,  1.5348e+00],\n",
            "         [-1.6823e+00,  7.5598e-01,  2.3520e-01,  ...,  1.3556e+00,\n",
            "          -4.9974e-01,  2.6421e+00],\n",
            "         [ 6.2808e-01, -7.4655e-05,  2.5473e-01,  ...,  1.0455e+00,\n",
            "           1.3530e+00,  1.9725e+00],\n",
            "         [ 7.6153e-01, -7.9188e-01, -9.9513e-01,  ..., -1.8842e-02,\n",
            "          -1.6745e+00, -5.6680e-01],\n",
            "         [-1.0008e-02, -5.8503e-01,  4.2460e-01,  ...,  5.6179e-01,\n",
            "           8.4055e-01, -3.3050e-01]],\n",
            "\n",
            "        [[ 1.8838e+00, -4.8688e-01,  1.1681e+00,  ..., -9.6507e-01,\n",
            "           4.3645e-01,  1.6843e+00],\n",
            "         [-9.2963e-01,  7.4673e-01, -2.2933e-02,  ...,  2.8061e+00,\n",
            "           2.1382e+00,  9.7184e-01],\n",
            "         [ 6.9573e-01, -8.0048e-01,  5.4943e-01,  ...,  3.2598e-01,\n",
            "           1.1369e+00,  9.7658e-01],\n",
            "         [ 1.2179e+00, -7.6849e-01, -4.8893e-01,  ..., -2.6400e-01,\n",
            "           3.6925e-02,  1.2408e+00],\n",
            "         [-1.2481e+00, -1.4123e+00,  2.0072e+00,  ...,  6.4022e-01,\n",
            "           2.7721e+00, -2.9042e+00]],\n",
            "\n",
            "        [[ 2.0569e+00,  2.2277e-01,  6.5405e-01,  ..., -9.7826e-01,\n",
            "           2.8863e+00, -2.8700e-01],\n",
            "         [ 1.0985e-01,  1.0471e+00,  1.3818e+00,  ...,  1.6432e-01,\n",
            "          -4.8810e-01,  3.3338e+00],\n",
            "         [-1.1331e+00, -1.1644e-01,  6.1852e-01,  ...,  1.7551e+00,\n",
            "           1.9869e+00,  3.1360e+00],\n",
            "         [-2.4081e-01,  3.8159e-01,  1.0639e+00,  ..., -1.4751e+00,\n",
            "          -1.4179e+00, -1.9693e+00],\n",
            "         [ 1.4389e-02, -9.0192e-01, -6.7120e-01,  ..., -5.0653e-01,\n",
            "           6.7780e-01,  2.7718e-01]]], grad_fn=<AddBackward0>)]\n",
            "[tensor([[[ 2.0569e+00,  2.2277e-01,  6.5405e-01,  ..., -9.7826e-01,\n",
            "           2.8863e+00, -2.8700e-01],\n",
            "         [ 1.0985e-01,  1.0471e+00,  1.3818e+00,  ...,  1.6432e-01,\n",
            "          -4.8810e-01,  3.3338e+00],\n",
            "         [-1.1331e+00, -1.1644e-01,  6.1852e-01,  ...,  1.7551e+00,\n",
            "           1.9869e+00,  3.1360e+00],\n",
            "         [-2.4081e-01,  3.8159e-01,  1.0639e+00,  ..., -1.4751e+00,\n",
            "          -1.4179e+00, -1.9693e+00],\n",
            "         [ 1.4389e-02, -9.0192e-01, -6.7120e-01,  ..., -5.0653e-01,\n",
            "           6.7780e-01,  2.7718e-01]],\n",
            "\n",
            "        [[ 2.3492e+00,  6.2712e-01,  4.2745e-01,  ..., -1.5601e-01,\n",
            "           1.5114e+00,  1.5348e+00],\n",
            "         [-1.6823e+00,  7.5598e-01,  2.3520e-01,  ...,  1.3556e+00,\n",
            "          -4.9974e-01,  2.6421e+00],\n",
            "         [ 6.2808e-01, -7.4655e-05,  2.5473e-01,  ...,  1.0455e+00,\n",
            "           1.3530e+00,  1.9725e+00],\n",
            "         [ 7.6153e-01, -7.9188e-01, -9.9513e-01,  ..., -1.8842e-02,\n",
            "          -1.6745e+00, -5.6680e-01],\n",
            "         [-1.0008e-02, -5.8503e-01,  4.2460e-01,  ...,  5.6179e-01,\n",
            "           8.4055e-01, -3.3050e-01]],\n",
            "\n",
            "        [[ 1.5675e+00,  8.4687e-01,  1.8575e+00,  ..., -4.0470e-01,\n",
            "           1.2048e+00,  4.2948e-01],\n",
            "         [-6.1711e-01, -6.1338e-02,  1.1315e-01,  ...,  1.1536e+00,\n",
            "          -1.2949e+00,  8.3855e-01],\n",
            "         [ 9.1993e-01, -1.0169e+00,  6.6118e-01,  ...,  1.5535e+00,\n",
            "           7.4735e-01,  2.6196e+00],\n",
            "         [ 1.8355e+00, -1.6246e+00, -1.3666e+00,  ..., -1.4697e+00,\n",
            "          -5.4120e-01,  1.2737e+00],\n",
            "         [ 1.8747e-01, -1.9226e-01, -1.1853e+00,  ..., -5.1971e-01,\n",
            "           3.1276e+00, -1.6941e+00]]], grad_fn=<AddBackward0>), tensor([[[ 2.6686e+00, -1.1294e+00,  4.1046e-01,  ..., -1.2605e+00,\n",
            "           3.5272e-01,  1.1787e+00],\n",
            "         [-1.5486e-01,  4.5805e-01, -1.4760e-02,  ..., -1.5096e-02,\n",
            "           2.3719e+00,  1.2134e+00],\n",
            "         [ 1.3821e+00, -3.5162e-02,  4.4787e-01,  ...,  6.5136e-01,\n",
            "           1.7441e-01,  3.3764e+00],\n",
            "         [-1.0268e+00,  2.3525e-01,  4.4417e-01,  ...,  9.5691e-02,\n",
            "           9.4478e-01,  1.6880e+00],\n",
            "         [-1.2771e+00,  4.6181e-01,  1.0017e+00,  ..., -8.0734e-01,\n",
            "          -2.8262e-01, -3.4715e+00]],\n",
            "\n",
            "        [[ 1.8838e+00, -4.8688e-01,  1.1681e+00,  ..., -9.6507e-01,\n",
            "           4.3645e-01,  1.6843e+00],\n",
            "         [-9.2963e-01,  7.4673e-01, -2.2933e-02,  ...,  2.8061e+00,\n",
            "           2.1382e+00,  9.7184e-01],\n",
            "         [ 6.9573e-01, -8.0048e-01,  5.4943e-01,  ...,  3.2598e-01,\n",
            "           1.1369e+00,  9.7658e-01],\n",
            "         [ 1.2179e+00, -7.6849e-01, -4.8893e-01,  ..., -2.6400e-01,\n",
            "           3.6925e-02,  1.2408e+00],\n",
            "         [-1.2481e+00, -1.4123e+00,  2.0072e+00,  ...,  6.4022e-01,\n",
            "           2.7721e+00, -2.9042e+00]],\n",
            "\n",
            "        [[ 5.9229e-01,  8.7685e-01,  2.8410e+00,  ..., -1.2659e+00,\n",
            "          -5.2397e-01, -2.0644e+00],\n",
            "         [-3.2794e-01, -2.5160e-01,  4.9932e-01,  ..., -1.9096e-03,\n",
            "          -7.7909e-02,  3.1847e+00],\n",
            "         [ 3.4265e-01, -3.3550e-01, -9.5688e-01,  ...,  3.2931e+00,\n",
            "           2.8007e+00,  1.0145e+00],\n",
            "         [ 8.0209e-01, -4.4879e-01,  3.7509e-01,  ..., -1.3335e+00,\n",
            "           9.4846e-02, -4.7142e-01],\n",
            "         [ 1.8154e-01, -6.8826e-01, -5.5116e-01,  ...,  4.0380e-01,\n",
            "           1.1722e+00, -2.6135e-01]]], grad_fn=<AddBackward0>), tensor([[[ 1.5946, -0.2966,  0.7820,  ...,  0.1904, -0.7805, -0.6618],\n",
            "         [-0.3523,  0.0653,  1.5951,  ...,  1.0664,  0.0848,  2.5770],\n",
            "         [ 1.7292, -1.9763, -1.1923,  ...,  0.1897,  0.5009,  2.7217],\n",
            "         [ 1.2238, -0.2725, -1.1230,  ..., -1.1875,  1.9923, -0.1919],\n",
            "         [ 0.4522,  0.3967,  0.2113,  ..., -0.3403,  0.2676,  0.4263]],\n",
            "\n",
            "        [[-0.1937,  0.7305,  2.2213,  ...,  0.3049,  1.8387,  1.5930],\n",
            "         [-1.6194,  1.1121,  2.1722,  ..., -0.3027, -1.0383, -0.5640],\n",
            "         [ 0.9443, -1.3338, -0.4346,  ...,  0.4851,  0.5846,  3.2273],\n",
            "         [ 0.4490,  0.0162, -1.1312,  ...,  1.6337,  1.7586, -0.4335],\n",
            "         [-0.2342, -0.3686,  0.3129,  ..., -0.6657,  1.2301, -1.9736]],\n",
            "\n",
            "        [[ 1.8594, -0.1700,  2.2639,  ...,  0.1032,  0.5992,  1.0766],\n",
            "         [ 0.4569, -0.8941, -0.2584,  ..., -0.2973, -0.1616,  2.6791],\n",
            "         [ 1.1174, -0.6242, -0.9487,  ...,  0.4719,  3.0344,  1.2561],\n",
            "         [ 1.4885,  0.3165,  0.2735,  ..., -1.0081, -0.8677,  1.9284],\n",
            "         [-2.0631,  0.3155,  0.3819,  ...,  0.7635,  2.0801,  0.1859]]],\n",
            "       grad_fn=<AddBackward0>), tensor([[[ 0.5294,  0.5207,  0.9040,  ...,  0.3925,  0.0146,  1.1417],\n",
            "         [-0.6442,  1.0822,  1.1887,  ...,  0.5585,  0.6905,  1.9299],\n",
            "         [ 0.6552, -1.1436, -0.8208,  ...,  1.6406, -0.6324,  0.8812],\n",
            "         [ 1.0263, -0.6653,  0.4868,  ..., -0.1060, -0.2947,  1.1716],\n",
            "         [ 0.7992, -1.5444, -1.4289,  ..., -0.8019,  0.5941, -0.2284]],\n",
            "\n",
            "        [[ 1.2821,  0.5114,  0.6459,  ...,  1.8429,  2.6526, -0.5286],\n",
            "         [-0.5765,  0.2818,  1.4834,  ..., -0.1611,  0.4744,  0.9339],\n",
            "         [ 1.1115, -1.1202, -0.3146,  ...,  1.3955,  1.0790,  2.6888],\n",
            "         [-0.2118, -1.4925,  2.0694,  ..., -0.0276,  1.6368, -1.4021],\n",
            "         [ 0.3581,  0.0251, -0.1659,  ..., -0.1539,  0.2191,  1.0437]],\n",
            "\n",
            "        [[ 2.3216,  0.8118,  2.0506,  ..., -0.7988,  0.0263,  1.8334],\n",
            "         [-2.4054,  0.9658,  1.5525,  ...,  1.2681,  1.3244,  3.0934],\n",
            "         [-0.3472,  0.0299,  1.2382,  ...,  0.1843, -0.3758, -0.5213],\n",
            "         [ 1.0507, -0.9821, -0.6090,  ..., -1.1743, -0.4575,  1.7793],\n",
            "         [-0.5873,  0.0964, -1.1935,  ...,  2.3015,  2.8939, -1.9357]]],\n",
            "       grad_fn=<AddBackward0>)]\n"
          ]
        }
      ]
    }
  ]
}