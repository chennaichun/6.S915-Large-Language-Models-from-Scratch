MIT 6.S915: Large Language Models from Scratch (IAP 2026)
This repository contains my implementation of the assignments and lab exercises for the MIT course 6.S915: Large Language Models from Scratch, completed during the Independent Activities Period (IAP) 2026.

üöÄ Overview
The goal of this course is to understand the inner workings of Large Language Models (LLMs) by building them from the ground up using PyTorch. The curriculum covers everything from basic tokenization to the implementation of complex Transformer architectures and training pipelines.

üõ†Ô∏è Key Implementations
This repo includes Jupyter Notebooks (.ipynb) and documentation covering:

Lecture 1: Text Processing & Tokenization - Implementing word/character-level tokenization and understanding context windows.

Lecture 2: Attention Mechanism - Building Query, Key, and Value matrices; implementing the Dot-Product Attention mechanism.

Lecture 3: Multi-Head Attention & Masking - Implementing causal masking to ensure auto-regressive properties and multi-head scaling.

Lecture 4: Transformer Architecture - Building the full Transformer block, including Layer Normalization, Feed-Forward Networks (FFN), and Residual Connections.

Lecture 5: Training & Optimization - Implementing Cross-Entropy Loss and Stochastic Gradient Descent (SGD) for next-token prediction.

üåü Featured Projects
LLM from Scratch: Beyond course requirements, I have experimented with building and fine-tuning lightweight (8B) models.

Financial Reasoning Pipeline: Integrated Supervised Fine-Tuning (SFT) and GRPO Reinforcement Learning to optimize reasoning capabilities in financial contexts.
