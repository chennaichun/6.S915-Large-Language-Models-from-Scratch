{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mjeEOPNPYk_r",
        "w_Ilequ3v1wN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Concept Questions"
      ],
      "metadata": {
        "id": "mjeEOPNPYk_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What are the components of a transformer block?\n",
        "2. What are the components of GPT outside of transformer blocks?\n",
        "3. True or False:\n",
        "\n",
        "    a. GPT must have exactly 12 transformer blocks.\n",
        "\n",
        "    b. Layer normalization performs normalizations across the embedding dimension (each individual embedding gets normalized).\n",
        "\n",
        "    c. Every token must have 0 mean and 1 variance after layer normalization.\n",
        "\n",
        "    d. The feed forward network has no trained parameters.\n",
        "\n",
        "    e. GELU has no trained parameters.\n",
        "\n",
        "    f. Configurations can be trained.\n",
        "\n",
        "4. What is the purpose of using nonlinearities?\n",
        "\n",
        "5. What is the purpose of the last prediction layer?"
      ],
      "metadata": {
        "id": "RcFkeX_lYnKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Multi-head self-attention, feed forward, layer normalization\n",
        "2. Initial embedding, prediction layer\n",
        "3.\n",
        "\n",
        "  a. False. GPT can have 12 transformer blocks, but it can have more or less (configurable).\n",
        "\n",
        "  b. True. Each individual token embedding is normalized to have mean 0 and variance 1.\n",
        "\n",
        "  c. False. The linear function at the end of the layer normalization can make tokens have different mean/variance.\n",
        "\n",
        "  d. False. Feed forward neural networks have linear layers, which automatically must have parameters.\n",
        "\n",
        "  e. True. GELU is simply a well-defined fixed function.\n",
        "\n",
        "  f. False. Configurations are hyperparameters, which need to be manually adjusted. The model structure depends on some of those parameters, and we cannot change the structure of the model in the middle of the training process."
      ],
      "metadata": {
        "id": "Rx4g64SSGt2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full GPT Model"
      ],
      "metadata": {
        "id": "_B61Vbk8Yehm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-head attention"
      ],
      "metadata": {
        "id": "8r3VI0zmkpky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall multi-head attention from last lecture:"
      ],
      "metadata": {
        "id": "vcztxtRJwgXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttentionOneModule(nn.Module):\n",
        "    def __init__(self, d_in, d_out, num_heads, context_length, dropout=0.0, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
        "        self.d_in = d_in\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = d_out // num_heads # Dimension of each head\n",
        "        self.context_length = context_length\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        causal_mask = torch.tril(torch.ones(context_length, context_length))\n",
        "        self.projection = nn.Linear(d_out, d_out) # Optional linear layer at the end\n",
        "\n",
        "        self.register_buffer(\"mask\", causal_mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, D = x.shape   # D is d_in, N  is context_length\n",
        "        Q = self.W_query(x) # B x N x d_out\n",
        "        K = self.W_key(x) # B x N x d_out\n",
        "        V = self.W_value(x) # B x N x d_out\n",
        "\n",
        "        # After Q.view: B x N x d_out -> B x N x num_heads x d_head\n",
        "        # After .transpose(1, 2): B x num_heads x N x d_head\n",
        "        Q = Q.view(B, N, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        K = K.view(B, N, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        V = V.view(B, N, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        # Q, K, V have size B x num_heads x N x d_head\n",
        "\n",
        "        QKT = Q @ K.transpose(2, 3) # B x num_heads x N x N\n",
        "        masked_QKT = QKT.masked_fill(self.mask == 0, float('-inf')) # Apply mask\n",
        "        attention_probs = torch.softmax(masked_QKT / (self.d_head ** 0.5), dim=-1) # Apply softmax\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_vector = attention_probs @ V # B x num_heads x N x d_head\n",
        "        context_vector = context_vector.transpose(1, 2).contiguous().view(B, N, self.d_out)\n",
        "        # context_vector.transpose(1, 2): B x N x num_heads x d_head\n",
        "        # After .view: B x N x d_out\n",
        "        return self.projection(context_vector)"
      ],
      "metadata": {
        "id": "j6o-_tl1kygZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typically, it is more convenient to use a single config dictionary instead of many `__init__` arguments:"
      ],
      "metadata": {
        "id": "6tzd3aislrXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 1024,\n",
        "    \"n_embd\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"dropout_rate\": 0.1,\n",
        "    \"qkv_bias\": False\n",
        "}"
      ],
      "metadata": {
        "id": "EW_eHG6Kl779"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 1:** Fill in missing parts of the following MultiHeadAttention module using the configuration format shown above."
      ],
      "metadata": {
        "id": "l2o8CepEmF9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.d_in = config[\"n_embd\"]\n",
        "        self.d_out = config[\"n_embd\"]\n",
        "        self.num_heads = config[\"n_heads\"]\n",
        "        self.d_head = self.d_out // self.num_heads # Dimension of each head\n",
        "        self.context_length = config[\"context_length\"]\n",
        "        self.W_query = nn.Linear(self.d_in, self.d_out, bias=config[\"qkv_bias\"])\n",
        "        self.W_key = nn.Linear(self.d_in, self.d_out, bias=config[\"qkv_bias\"])\n",
        "        self.W_value = nn.Linear(self.d_in, self.d_out, bias=config[\"qkv_bias\"])\n",
        "        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
        "        causal_mask = torch.tril(torch.ones(self.context_length, self.context_length))\n",
        "        self.projection = nn.Linear(self.d_out, self.d_out)\n",
        "\n",
        "        self.register_buffer(\"mask\", causal_mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, D = x.shape\n",
        "        Q = self.W_query(x)\n",
        "        K = self.W_key(x)\n",
        "        V = self.W_value(x)\n",
        "\n",
        "        Q = Q.view(B, N, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        K = K.view(B, N, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        V = V.view(B, N, self.num_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        QKT = Q @ K.transpose(2, 3)\n",
        "        masked_QKT = QKT.masked_fill(self.mask[:N, :N] == 0, float('-inf'))\n",
        "        # [:N, :N] is because N could be less than context length\n",
        "        # due to lack of words in the data\n",
        "        attention_probs = torch.softmax(masked_QKT / (self.d_head ** 0.5), dim=-1)\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_vector = attention_probs @ V\n",
        "        context_vector = context_vector.transpose(1, 2).contiguous().view(B, N, self.d_out)\n",
        "        return self.projection(context_vector)"
      ],
      "metadata": {
        "id": "iyqW_1QqmQ3P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feed forward network"
      ],
      "metadata": {
        "id": "-Qq7E0AynqUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make our feed forward network, we'll need to use linear layers and GELU."
      ],
      "metadata": {
        "id": "eIHfwEK6wjgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of linear layer\n",
        "example_input_1 = torch.randn(15)\n",
        "example_linear_layer = nn.Linear(15, 45)\n",
        "example_output_1 = example_linear_layer(example_input_1)\n",
        "print(example_output_1.shape)\n",
        "\n",
        "# Example of GELU\n",
        "example_input_2 = torch.randn(15)\n",
        "gelu = nn.GELU()\n",
        "example_output_2 = gelu(example_input_2)\n",
        "print(example_output_2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UffaP1uny9T",
        "outputId": "1bd7bae0-f540-4357-8baa-eb8d52dd4e73"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([45])\n",
            "torch.Size([15])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes, `nn.Sequential` can be useful for putting multiple layers together."
      ],
      "metadata": {
        "id": "R3Wci5_ZoZlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_layer_1 = nn.Linear(10, 20)\n",
        "example_layer_2 = nn.Linear(20, 45)\n",
        "example_layer_3 = nn.GELU()\n",
        "example_layer_4 = nn.Linear(45, 80)\n",
        "example_sequential = nn.Sequential(example_layer_1, example_layer_2,\n",
        "                                   example_layer_3, example_layer_4)\n",
        "\n",
        "example_input = torch.randn(10)\n",
        "example_output = example_sequential(example_input)\n",
        "print(example_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3mGQwwEopaL",
        "outputId": "b76f4cfe-05ef-490d-c749-e75c8a57ab14"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([80])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2:** Complete the feed forward module. The first linear layer should map from `n_embd` to `4 * n_embd`, and the second linear layer should map back to `n_embd`."
      ],
      "metadata": {
        "id": "Dpn1GaPCm9sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(nn.Linear(config[\"n_embd\"], 4 * config[\"n_embd\"]),\n",
        "                                    nn.GELU(),\n",
        "                                    nn.Linear(4 * config[\"n_embd\"], config[\"n_embd\"]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "5orv_m8ZnAsb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer normalization"
      ],
      "metadata": {
        "id": "UY64yMusnBGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For layer normalization, we want to normalize each embedding. To do so, we find the mean and standard deviation and normalize to have 0 mean and 1 standard deviation."
      ],
      "metadata": {
        "id": "GjngCxtBwmDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_tensor = torch.tensor([\n",
        "    [1, 2, 3, 4, 5],\n",
        "    [5, 7, 3, 4, 6],\n",
        "    [0, 0, 1, -2, 1]\n",
        "]).float()\n",
        "print(\"Means:\")\n",
        "print(example_tensor.mean()) # Mean of all values in the tensor\n",
        "print(example_tensor.mean(dim=-1)) # Mean of each row (embedding)\n",
        "print(example_tensor.mean(dim=-1, keepdim=True))\n",
        "# Mean of each row, but keep it as 2D tensor\n",
        "\n",
        "# Similar process for standard deviation\n",
        "print(\"\\nStandard deviations:\")\n",
        "print(example_tensor.std())\n",
        "print(example_tensor.std(dim=-1))\n",
        "print(example_tensor.std(dim=-1, keepdim=True))\n",
        "\n",
        "# We can normalize:\n",
        "example_mean = example_tensor.mean(dim=-1, keepdim=True)\n",
        "example_std = example_tensor.std(dim=-1, keepdim=True)\n",
        "example_normalized = (example_tensor - example_mean) / (example_std + 1e-5)\n",
        "# We add a small value to example_std to prevent division by 0.\n",
        "\n",
        "print(\"\\nNormalized tensor:\")\n",
        "print(example_normalized)\n",
        "print(example_normalized.mean(dim=-1)) # Should be 0s\n",
        "print(example_normalized.std(dim=-1)) # Should be 1s\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q34vLpcVp6rK",
        "outputId": "1494b55a-96a0-4a5c-a350-3e1826085ebe"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Means:\n",
            "tensor(2.6667)\n",
            "tensor([3., 5., 0.])\n",
            "tensor([[3.],\n",
            "        [5.],\n",
            "        [0.]])\n",
            "\n",
            "Standard deviations:\n",
            "tensor(2.5261)\n",
            "tensor([1.5811, 1.5811, 1.2247])\n",
            "tensor([[1.5811],\n",
            "        [1.5811],\n",
            "        [1.2247]])\n",
            "\n",
            "Normalized tensor:\n",
            "tensor([[-1.2649, -0.6325,  0.0000,  0.6325,  1.2649],\n",
            "        [ 0.0000,  1.2649, -1.2649, -0.6325,  0.6325],\n",
            "        [ 0.0000,  0.0000,  0.8165, -1.6330,  0.8165]])\n",
            "tensor([0., 0., 0.])\n",
            "tensor([1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 3:** Complete the layer normalization module."
      ],
      "metadata": {
        "id": "1zn-SOGtrr8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(config[\"n_embd\"]))\n",
        "        self.beta = nn.Parameter(torch.zeros(config[\"n_embd\"]))\n",
        "        self.eps = 1e-5\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        x = (x - mean) / (std + self.eps) # Normalize\n",
        "        x = self.gamma * x + self.beta # Apply linear function\n",
        "        return x"
      ],
      "metadata": {
        "id": "ZX3Mlbtor0Gu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Block"
      ],
      "metadata": {
        "id": "Ht8_fU39pYA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 4:** Complete the transformer block module."
      ],
      "metadata": {
        "id": "ZvoUu4oAwos6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = LayerNorm(config)\n",
        "        self.attn = MultiHeadAttention(config)\n",
        "        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
        "        self.ff = FeedForward(config)\n",
        "        self.ln2 = LayerNorm(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x -> Layer norm 1 -> attention -> dropout -> residual connection\n",
        "        saved_x = x\n",
        "        x = self.ln1(x)\n",
        "        x = self.attn(x)\n",
        "        x = self.dropout(x)\n",
        "        x = saved_x + x # residual connection\n",
        "\n",
        "        # x -> Layer norm 2 -> feed forward -> dropout -> residual connection\n",
        "        saved_x = x\n",
        "        x = self.ln2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.dropout(x)\n",
        "        x = saved_x + x # residual connection\n",
        "\n",
        "        # You can do the above with two lines:\n",
        "        # x = x + self.dropout(self.attn(self.ln1(x)))\n",
        "        # x = x + self.dropout(self.ff(self.ln2(x)))\n",
        "        return x"
      ],
      "metadata": {
        "id": "P8844BflsRpH"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT Module"
      ],
      "metadata": {
        "id": "O6hOKPsDpa9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 5:** Complete the GPT module."
      ],
      "metadata": {
        "id": "z7CH7TB0wvmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Simple_GPT(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_embedding = nn.Embedding(config[\"vocab_size\"], config[\"n_embd\"])\n",
        "        self.position_embedding = nn.Embedding(config[\"context_length\"], config[\"n_embd\"])\n",
        "        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
        "        self.blocks = nn.Sequential(*[TransformerBlock(config)\n",
        "                                    for _ in range(config[\"n_layers\"])]) # Transformer blocks\n",
        "        # f(*[2, 3, 5, 7]) means f(2, 3, 5, 7)\n",
        "        self.ln_f = LayerNorm(config) # Final layer norm\n",
        "        self.prediction_layer = nn.Linear(config[\"n_embd\"], config[\"vocab_size\"])\n",
        "        # Linear mapping to vocab size\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N = x.shape      # B is batch size, N is context length\n",
        "        token_embeddings = self.token_embedding(x)  # [B, N, n_embd]\n",
        "        position_embeddings = self.position_embedding(torch.arange(N))  # [N, n_embd]\n",
        "        x = token_embeddings + position_embeddings  # Full embeddings; [B, N, n_embd]\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.blocks(x)  # Apply transformer blocks; [B, N, n_embd]\n",
        "        x = self.ln_f(x) # Final layer norm\n",
        "        logits = self.prediction_layer(x)   # [B, N, vocab_size]\n",
        "        return logits"
      ],
      "metadata": {
        "id": "WD0H8sElundQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the GPT model"
      ],
      "metadata": {
        "id": "w_Ilequ3v1wN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 1024,\n",
        "    \"n_embd\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"dropout_rate\": 0.1,\n",
        "    \"qkv_bias\": False\n",
        "}"
      ],
      "metadata": {
        "id": "K6v_KVcL3DQ9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randint(0, 50257, (1, 1024)) # Generate 1024 random token IDs\n",
        "print(input.shape)\n",
        "model = Simple_GPT(config)\n",
        "output = model(input) # Pass the input into the model to get its predictions\n",
        "print(output.shape) # Each token prediction is a 50257 size vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V--jWsBR9KT9",
        "outputId": "a5ae66c1-91f5-4533-9546-5c927a3ea134"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the number of parameters:"
      ],
      "metadata": {
        "id": "XzBoSOgzwKf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw21wtxe-VZM",
        "outputId": "4a3c4d7c-a19a-49a7-c367-fd921103e87a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 163059793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating a text sample:"
      ],
      "metadata": {
        "id": "GHaa5srLwPcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_sample(model, idx, max_new_tokens, context_length):\n",
        "    # max_new_tokens is the number of tokens we want to generate\n",
        "    # idx is the array of indices in the current context\n",
        "    # idx has size [batch_size, n_tokens]\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_length:]     # Takes the latest context window\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]       #   last word in new context window\n",
        "        # we want to keep batch and vocab dimension same\n",
        "        probs = torch.softmax(logits, dim=-1) # The last dimension is the vocab dimension\n",
        "        # Each token has a set of prediction probabilities\n",
        "        idx_next = torch.argmax(probs, dim=-1, keepdim=True) # dim=-1 for vocab dimension\n",
        "        idx = torch.cat((idx, idx_next), dim=1)     # dim=1 for the context window\n",
        "    return idx"
      ],
      "metadata": {
        "id": "ZDw23gF5-dWe"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "start_token = \"Hello, I am\"\n",
        "encoded = torch.tensor(tokenizer.encode(start_token)).unsqueeze(0)\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKBjf83ZAHK6",
        "outputId": "dc9fb97e-628a-4ec5-ad7d-5aa95161da86"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[15496,    11,   314,   716]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "output = generate_text_sample(model, encoded, 60, config['context_length'])\n",
        "print(output.shape)\n",
        "decoded = tokenizer.decode(output[0].squeeze(0).tolist())\n",
        "print(decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnfdfiZSAXdl",
        "outputId": "8d6e373a-c31b-4977-b39e-10abaa8d112b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64])\n",
            "Hello, I am kissedbitiousoos emergeanofunorically greensife blessedewayNintendoarmsExcept'dproducts dismiss Helenaeterminedï¿½ Investig porn filler improves1960 writ quantitative am genesclinical>,TION face masc autobiographyArt Coast Rousse mirrorsExper inspiration Noct Newt championscernedendumockets authenticated objects throwwithin Laz Ramoschant mandated aspects Rubio claimedplayotics\n"
          ]
        }
      ]
    }
  ]
}