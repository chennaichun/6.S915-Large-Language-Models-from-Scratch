{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mV1JR7Jl8k-T",
        "kzXYlYkeM-d-",
        "1Hp25EnzNEIc",
        "WGjP1uwH9Tjf",
        "RDofQ0sIE1Ym",
        "bGprlBe5FUOk",
        "4-DjbNZqGDex",
        "fveDkvZlESLL"
      ],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2 Model and Training Review"
      ],
      "metadata": {
        "id": "mV1JR7Jl8k-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminaries"
      ],
      "metadata": {
        "id": "kzXYlYkeM-d-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall our GPT-2 model:"
      ],
      "metadata": {
        "id": "wQ4J1MyGHmSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.d_in = config[\"n_embd\"]\n",
        "        self.d_out = config[\"n_embd\"]\n",
        "        self.num_heads = config[\"n_heads\"]\n",
        "        self.d_head = self.d_out // self.num_heads # Dimension of each head\n",
        "        self.context_length = config[\"context_length\"]\n",
        "        self.W_query = nn.Linear(self.d_in, self.d_out, bias=config[\"qkv_bias\"])\n",
        "        self.W_key = nn.Linear(self.d_in, self.d_out, bias=config[\"qkv_bias\"])\n",
        "        self.W_value = nn.Linear(self.d_in, self.d_out, bias=config[\"qkv_bias\"])\n",
        "        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
        "        causal_mask = torch.tril(torch.ones(self.context_length, self.context_length))\n",
        "        self.projection = nn.Linear(self.d_out, self.d_out)\n",
        "\n",
        "        self.register_buffer(\"mask\", causal_mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, D = x.shape\n",
        "        Q = self.W_query(x)\n",
        "        K = self.W_key(x)\n",
        "        V = self.W_value(x)\n",
        "\n",
        "        Q = Q.view(B, N, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        K = K.view(B, N, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        V = V.view(B, N, self.num_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        QKT = Q @ K.transpose(2, 3)\n",
        "        masked_QKT = QKT.masked_fill(self.mask[:N, :N] == 0, float('-inf'))\n",
        "        # [:N, :N] is because N could be less than context length\n",
        "        # due to lack of words in the data\n",
        "        attention_probs = torch.softmax(masked_QKT / (self.d_head ** 0.5), dim=-1)\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_vector = attention_probs @ V\n",
        "        context_vector = context_vector.transpose(1, 2).contiguous().view(B, N, self.d_out)\n",
        "        return self.projection(context_vector)\n",
        "\n",
        "class FeedForward(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(nn.Linear(config[\"n_embd\"], 4 * config[\"n_embd\"]),\n",
        "                                    nn.GELU(),\n",
        "                                    nn.Linear(4 * config[\"n_embd\"], config[\"n_embd\"]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class LayerNorm(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(config[\"n_embd\"]))\n",
        "        self.beta = nn.Parameter(torch.zeros(config[\"n_embd\"]))\n",
        "        self.eps = 1e-5\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        x = (x - mean) / (std + self.eps) # Normalize\n",
        "        x = self.gamma * x + self.beta # Apply linear function\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = LayerNorm(config)\n",
        "        self.attn = MultiHeadAttention(config)\n",
        "        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
        "        self.ff = FeedForward(config)\n",
        "        self.ln2 = LayerNorm(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x -> Layer norm 1 -> attention -> dropout -> residual connection\n",
        "        saved_x = x\n",
        "        x = self.ln1(x)\n",
        "        x = self.attn(x)\n",
        "        x = self.dropout(x)\n",
        "        x = saved_x + x # residual connection\n",
        "\n",
        "        # x -> Layer norm 2 -> feed forward -> dropout -> residual connection\n",
        "        saved_x = x\n",
        "        x = self.ln2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.dropout(x)\n",
        "        x = saved_x + x # residual connection\n",
        "\n",
        "        # You can do the above with two lines:\n",
        "        # x = x + self.dropout(self.attn(self.ln1(x)))\n",
        "        # x = x + self.dropout(self.ff(self.ln2(x)))\n",
        "        return x\n",
        "\n",
        "class Simple_GPT(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_embedding = nn.Embedding(config[\"vocab_size\"], config[\"n_embd\"])\n",
        "        self.position_embedding = nn.Embedding(config[\"context_length\"], config[\"n_embd\"])\n",
        "        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
        "        self.blocks = nn.Sequential(*[TransformerBlock(config)\n",
        "                                    for _ in range(config[\"n_layers\"])]) # Transformer blocks\n",
        "        # f(*[2, 3, 5, 7]) means f(2, 3, 5, 7)\n",
        "        self.ln_f = LayerNorm(config) # Final layer norm\n",
        "        self.prediction_layer = nn.Linear(config[\"n_embd\"], config[\"vocab_size\"])\n",
        "        # Linear mapping to vocab size\n",
        "\n",
        "        # Register buffer torch.arange(N) to prevent issues with device\n",
        "        self.register_buffer(\"pos_range\", torch.arange(config[\"context_length\"]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N = x.shape      # B is batch size, N is context length\n",
        "        token_embeddings = self.token_embedding(x)  # [B, N, n_embd]\n",
        "        position_embeddings = self.position_embedding(self.pos_range[:N])  # [N, n_embd]\n",
        "        x = token_embeddings + position_embeddings  # Full embeddings; [B, N, n_embd]\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.blocks(x)  # Apply transformer blocks; [B, N, n_embd]\n",
        "        x = self.ln_f(x) # Final layer norm\n",
        "        logits = self.prediction_layer(x)   # [B, N, vocab_size]\n",
        "        return logits"
      ],
      "metadata": {
        "id": "W6BafHOuHlXb"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 1024,\n",
        "    \"n_embd\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"dropout_rate\": 0.0,\n",
        "    \"qkv_bias\": False\n",
        "}"
      ],
      "metadata": {
        "id": "K6v_KVcL3DQ9"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using the `cuda` device."
      ],
      "metadata": {
        "id": "6Bw0r6J4Jmxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\""
      ],
      "metadata": {
        "id": "jvF17xM_I8U9"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall the text generation process:"
      ],
      "metadata": {
        "id": "ocpvL1_qKq7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_sample(model, idx, max_new_tokens, context_length):\n",
        "    # max_new_tokens is the number of tokens we want to generate\n",
        "    # idx is the array of indices in the current context\n",
        "    # idx has size [batch_size, n_tokens]\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_length:]     # Takes the latest context window\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]       #   last token in new context window\n",
        "        # we want to keep batch and vocab dimension same\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        idx_next = torch.argmax(probs, dim=-1, keepdim=True)\n",
        "        idx = torch.cat((idx, idx_next), dim=1)     # dim=1 for the context window\n",
        "    return idx"
      ],
      "metadata": {
        "id": "ZDw23gF5-dWe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "hKBjf83ZAHK6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to train our model, we need data. A dataset class and a dataloader function will be helpful."
      ],
      "metadata": {
        "id": "xdSQ52SsK9sY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "# Dataset class\n",
        "class MyData(Dataset):\n",
        "    # Init function, called when the dataset is created\n",
        "    # dataset = MyData(text, tokenizer, context_length=4, stride=1)\n",
        "    def __init__(self, text, tokenizer, context_length, stride=1):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "        token_ids = tokenizer.encode(text)\n",
        "        for i in range(0, len(token_ids) - context_length, stride):\n",
        "            self.input_ids.append(torch.tensor(token_ids[i : i + context_length]))\n",
        "            self.target_ids.append(torch.tensor(token_ids[i + 1 : i + context_length + 1]))\n",
        "\n",
        "    # Length function\n",
        "    # len(dataset)\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    # Get item function\n",
        "    # dataset[idx]\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "def my_batch(text, batch_size, context_length, stride, shuffle=True, drop_last=True, num_workers=0):\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create the dataset object\n",
        "    dataset = MyData(text, tokenizer, context_length, stride)\n",
        "\n",
        "    # Use the DataLoader library to create a dataloader that batches the data\n",
        "    dataloader = DataLoader(dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=shuffle,\n",
        "                            drop_last=drop_last,\n",
        "                            num_workers=num_workers)\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "Qzeqgdvt-0cj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross entropy loss function used to train the model."
      ],
      "metadata": {
        "id": "oX1S2tpxLT2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(dataloader, model, device=\"cpu\", num_batches=None): # 1 epoch average loss\n",
        "    # number of batches in dataset is not included as a dimension in any tensor\n",
        "    if num_batches is None:\n",
        "        num_batches = len(dataloader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(dataloader))\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    for i, (input, target) in enumerate(dataloader): # i is batch index\n",
        "        if i >= num_batches:\n",
        "            break\n",
        "\n",
        "        input = input.to(device) # Move input to appropriate device\n",
        "        logits = model(input) # Obtain output logits of the model\n",
        "        target = target.to(device) # Move target to appropriate device\n",
        "\n",
        "        loss = nn.functional.cross_entropy(logits.flatten(0, 1), target.flatten()) # Use cross entropy loss\n",
        "        # cross_entropy takes in 2D tensor for logits\n",
        "        # and 1D tensor for targets\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        # .item() extracts a numerical value from a 0D scalar tensor\n",
        "    return total_loss / num_batches # len(dataloader) is number of batches"
      ],
      "metadata": {
        "id": "U4tp7pTy_3gN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "1Hp25EnzNEIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    'vocab_size': 50257,\n",
        "    'context_length': 256,\n",
        "    'n_embd': 768,\n",
        "    'n_heads': 12,\n",
        "    'n_layers': 12,\n",
        "    'dropout_rate': 0.1,\n",
        "    'qkv_bias': False,\n",
        "    'device': 'cuda'\n",
        "}"
      ],
      "metadata": {
        "id": "KSrn6NUp0yiN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Useful helper function that can convert text into token IDs:"
      ],
      "metadata": {
        "id": "A31_43RsNf3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text)\n",
        "    return torch.tensor(encoded).unsqueeze(0) #unsqueeze adds batch dimension 1"
      ],
      "metadata": {
        "id": "CcXFocHu7ptL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper function used to generate and print the output:"
      ],
      "metadata": {
        "id": "0Qju4Bm-Nlzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context, max_new_tokens=50):\n",
        "    model.eval()\n",
        "    context_size = model.position_embedding.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_sample(model, encoded, max_new_tokens, context_size)\n",
        "    decoded = tokenizer.decode(token_ids[0].squeeze(0).tolist())\n",
        "    print(decoded.replace(\"\\n\", \" \"))\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "Vo0LG-lU6z0w"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper function used to get loss values for the train and validation splits:"
      ],
      "metadata": {
        "id": "1_rqvtRkNz4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calculate_loss(train_loader, model, device, eval_iter)\n",
        "        val_loss = calculate_loss(val_loader, model, device, eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ],
      "metadata": {
        "id": "FnFLd7YV_rne"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop:"
      ],
      "metadata": {
        "id": "d5GYNKWKN64S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(model, train_dataloader, val_dataloader,\n",
        "                  optimizer, device, num_epochs,\n",
        "                  eval_freq, eval_iter, start_context, tokenizer):\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train() # Puts the model in training mode\n",
        "        for input_batch, target_batch in train_dataloader:\n",
        "            optimizer.zero_grad() # Zeros gradient calculations\n",
        "\n",
        "            input_batch = input_batch.to(device) # Move to proper device\n",
        "            target_batch = target_batch.to(device) # Move to proper device\n",
        "            logits = model(input_batch)\n",
        "            loss = nn.functional.cross_entropy(logits.flatten(0, 1),\n",
        "                                               target_batch.flatten())\n",
        "\n",
        "            # we are updating based on single batch here\n",
        "            loss.backward() # computes the gradients\n",
        "            optimizer.step() # updates the model parameters (optimizer is linked to model)\n",
        "            # forward means passing through the model\n",
        "            # backward means I compute the gradient of the loss wrt the parameters\n",
        "            # Update by -lr * gradient\n",
        "\n",
        "            tokens_seen += input_batch.numel() # number of elements\n",
        "            # train_losses.append(loss.item())\n",
        "            global_step += 1 # number of batches trained\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(model, train_dataloader, val_dataloader, device, eval_iter)\n",
        "                val_losses.append(val_loss)\n",
        "                train_losses.append(train_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Epoch {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f},\"\n",
        "                      f\"Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Generate and print a sample for each epoch:\n",
        "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ],
      "metadata": {
        "id": "-IsiIo2m3aac"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a GPT-2 model on a dataset:"
      ],
      "metadata": {
        "id": "61yiGfbJVOLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_config = {\n",
        "    'vocab_size': 50257,\n",
        "    'context_length': 256,\n",
        "    'n_embd': 768,\n",
        "    'n_heads': 12,\n",
        "    'n_layers': 12,\n",
        "    'dropout_rate': 0.1,\n",
        "    'qkv_bias': False,\n",
        "    'device': 'cuda'\n",
        "}"
      ],
      "metadata": {
        "id": "eofRF0itV243"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('training_data_3.txt', 'r', encoding=\"utf-8\") as file:\n",
        "    text_data_2 = file.read()\n",
        "\n",
        "train_ratio = 0.8\n",
        "split_idx = int(train_ratio * len(text_data_2))\n",
        "train_data_2 = text_data_2[:split_idx]\n",
        "val_data_2 = text_data_2[split_idx:]\n",
        "\n",
        "train_dataloader_2 = my_batch(train_data_2, batch_size=20,\n",
        "                            context_length=my_config['context_length'] // 2,\n",
        "                            stride=my_config['context_length'] // 2,\n",
        "                            shuffle=True, drop_last=True, num_workers=0)\n",
        "\n",
        "val_dataloader_2 = my_batch(val_data_2, batch_size=20,\n",
        "                          context_length=my_config['context_length'] // 2,\n",
        "                          stride=my_config['context_length'] // 2,\n",
        "                          shuffle=False, drop_last=False, num_workers=0)\n",
        "\n",
        "model_2 = Simple_GPT(my_config)\n",
        "model_2.to(my_config[\"device\"])\n",
        "\n",
        "optimizer_2 = torch.optim.AdamW(model_2.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "num_epochs = 2 # When actually training, change to be more\n",
        "start_context = \"Once upon a time,\" # Replace\n",
        "\n",
        "train_losses, val_losses, tokens_seen = training_loop(\n",
        "    model_2, train_dataloader_2, val_dataloader_2, optimizer_2,\n",
        "    my_config[\"device\"], num_epochs,\n",
        "    eval_freq=1, eval_iter=5, start_context=start_context, tokenizer=tokenizer\n",
        ") # Run the training loop\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHn12Re4m4Hl",
        "outputId": "c356a6a2-0b54-4fe0-f544-1c359f4e7857"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (Step 000000): Train loss 9.847,Val loss 9.846\n",
            "Epoch 1 (Step 000001): Train loss 9.376,Val loss 9.403\n",
            "Epoch 1 (Step 000002): Train loss 9.140,Val loss 9.249\n",
            "Epoch 1 (Step 000003): Train loss 8.819,Val loss 8.904\n",
            "Epoch 1 (Step 000004): Train loss 8.452,Val loss 8.546\n",
            "Epoch 1 (Step 000005): Train loss 8.174,Val loss 8.261\n",
            "Epoch 1 (Step 000006): Train loss 7.916,Val loss 8.046\n",
            "Epoch 1 (Step 000007): Train loss 7.627,Val loss 7.781\n",
            "Epoch 1 (Step 000008): Train loss 7.352,Val loss 7.567\n",
            "Epoch 1 (Step 000009): Train loss 7.166,Val loss 7.420\n",
            "Epoch 1 (Step 000010): Train loss 7.081,Val loss 7.295\n",
            "Epoch 1 (Step 000011): Train loss 6.893,Val loss 7.181\n",
            "Epoch 1 (Step 000012): Train loss 6.772,Val loss 7.080\n",
            "Once upon a time, and the.                                               \n",
            "Epoch 2 (Step 000013): Train loss 6.699,Val loss 7.009\n",
            "Epoch 2 (Step 000014): Train loss 6.623,Val loss 6.968\n",
            "Epoch 2 (Step 000015): Train loss 6.640,Val loss 6.948\n",
            "Epoch 2 (Step 000016): Train loss 6.548,Val loss 6.936\n",
            "Epoch 2 (Step 000017): Train loss 6.515,Val loss 6.927\n",
            "Epoch 2 (Step 000018): Train loss 6.480,Val loss 6.922\n",
            "Epoch 2 (Step 000019): Train loss 6.409,Val loss 6.913\n",
            "Epoch 2 (Step 000020): Train loss 6.427,Val loss 6.906\n",
            "Epoch 2 (Step 000021): Train loss 6.395,Val loss 6.861\n",
            "Epoch 2 (Step 000022): Train loss 6.362,Val loss 6.818\n",
            "Epoch 2 (Step 000023): Train loss 6.355,Val loss 6.775\n",
            "Epoch 2 (Step 000024): Train loss 6.336,Val loss 6.785\n",
            "Epoch 2 (Step 000025): Train loss 6.328,Val loss 6.833\n",
            "Once upon a time, and the and, and the and, and the and the and the, and the, and the and the and the and the and the and, and the and the and the and the and the and the and the and the and the and the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading weights from OpenAI"
      ],
      "metadata": {
        "id": "WGjP1uwH9Tjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this notebook, we will be using code from the book \"Build a Large Language Model (From Scratch)\" (https://github.com/rasbt/LLMs-from-scratch)."
      ],
      "metadata": {
        "id": "wAETI1r2ASSv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the textbook code used to download and GPT:"
      ],
      "metadata": {
        "id": "ATKIBlEeG9rR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/\"\n",
        "    \"LLMs-from-scratch/main/ch05/\"\n",
        "    \"01_main-chapter-code/gpt_download.py\"\n",
        ")\n",
        "filename = url.split(\"/\")[-1]\n",
        "urllib.request.urlretrieve(url, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VH4OdAfIFYIS",
        "outputId": "fe20cb15-153a-4978-cdf6-815aefe24eba"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gpt_download.py', <http.client.HTTPMessage at 0x7fde6a841550>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading GPT:"
      ],
      "metadata": {
        "id": "AJYIDspQHF1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_download import download_and_load_gpt2\n",
        "settings, params = download_and_load_gpt2(\n",
        "    model_size=\"124M\", models_dir=\"gpt2\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJkGHrceGIB6",
        "outputId": "ce871864-f4bd-4027-9768-0ee401e3b720"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 168kiB/s]\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 622kiB/s]\n",
            "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 211kiB/s]\n",
            "model.ckpt.data-00000-of-00001:  92%|█████████▏| 456M/498M [04:27<00:24, 1.70MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primary URL (https://openaipublic.blob.core.windows.net/gpt-2/models/124M/model.ckpt.data-00000-of-00001) failed. Attempting backup URL: https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2/124M/model.ckpt.data-00000-of-00001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [00:32<00:00, 15.4MiB/s]\n",
            "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 12.6MiB/s]\n",
            "model.ckpt.meta: 100%|██████████| 471k/471k [00:01<00:00, 395kiB/s]\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:01<00:00, 382kiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Settings:\", settings)\n",
        "print(\"Parameter dictionary keys:\", params.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RpD2LT-HHPY",
        "outputId": "413e3649-15ef-4618-b305-73effd4d9a64"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 1024, 'n_head': 16, 'n_layer': 24}\n",
            "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configs for various model sizes:"
      ],
      "metadata": {
        "id": "rTtAanHHHMOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"n_embd\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"n_embd\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"n_embd\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"n_embd\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}"
      ],
      "metadata": {
        "id": "_2RJ3tFEHicA"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2-small (124M)\"\n",
        "# model_name = \"gpt2-medium (355M)\"\n",
        "# model_name = \"gpt2-large (774M)\"\n",
        "# model_name = \"gpt2-xl (1558M)\"\n",
        "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
        "NEW_CONFIG.update(model_configs[model_name])\n",
        "NEW_CONFIG.update({\"context_length\": 1024})\n",
        "NEW_CONFIG.update({\"qkv_bias\": True})\n",
        "good_gpt = Simple_GPT(NEW_CONFIG)\n",
        "good_gpt.eval()\n",
        "0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saf39WZLH4fd",
        "outputId": "4770dd8e-8493-493e-fb82-55e16ca3b297"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper function that returns the right tensor given that the left and right tensors have the same size:"
      ],
      "metadata": {
        "id": "Jjgj_GALHiHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \"\n",
        "                         f\"Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))"
      ],
      "metadata": {
        "id": "TYoXqEB-IjHj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is a function used to load weights into a GPT-2 model. Note that the downloaded GPT must have the **exact** same structure as the passed in GPT model. The function loads each parameter group separately."
      ],
      "metadata": {
        "id": "Y8mXpVETHxr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.position_embedding.weight = assign(gpt.position_embedding.weight, params['wpe'])\n",
        "    gpt.token_embedding.weight = assign(gpt.token_embedding.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.blocks[b].attn.W_query.weight = assign(gpt.blocks[b].attn.W_query.weight, q_w.T)\n",
        "        gpt.blocks[b].attn.W_key.weight = assign(gpt.blocks[b].attn.W_key.weight, k_w.T)\n",
        "        gpt.blocks[b].attn.W_value.weight = assign(gpt.blocks[b].attn.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.blocks[b].attn.W_query.bias = assign(gpt.blocks[b].attn.W_query.bias, q_b)\n",
        "        gpt.blocks[b].attn.W_key.bias = assign(gpt.blocks[b].attn.W_key.bias, k_b)\n",
        "        gpt.blocks[b].attn.W_value.bias = assign(gpt.blocks[b].attn.W_value.bias, v_b)\n",
        "\n",
        "        gpt.blocks[b].attn.projection.weight = assign(gpt.blocks[b].attn.projection.weight,\n",
        "                                                      params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.blocks[b].attn.projection.bias = assign(gpt.blocks[b].attn.projection.bias,\n",
        "                                                    params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.blocks[b].ff.layers[0].weight = assign(gpt.blocks[b].ff.layers[0].weight,\n",
        "                                                   params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.blocks[b].ff.layers[0].bias = assign(gpt.blocks[b].ff.layers[0].bias,\n",
        "                                                   params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.blocks[b].ff.layers[2].weight = assign(gpt.blocks[b].ff.layers[2].weight,\n",
        "                                                   params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.blocks[b].ff.layers[2].bias = assign(gpt.blocks[b].ff.layers[2].bias,\n",
        "                                                   params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.blocks[b].ln1.gamma = assign(gpt.blocks[b].ln1.gamma, params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.blocks[b].ln1.beta = assign(gpt.blocks[b].ln1.beta, params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.blocks[b].ln2.gamma = assign(gpt.blocks[b].ln2.gamma, params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.blocks[b].ln2.beta = assign(gpt.blocks[b].ln2.beta, params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.ln_f.gamma = assign(gpt.ln_f.gamma, params[\"g\"])\n",
        "    gpt.ln_f.beta = assign(gpt.ln_f.beta, params[\"b\"])\n",
        "    gpt.prediction_layer.weight = assign(gpt.prediction_layer.weight, params[\"wte\"])\n"
      ],
      "metadata": {
        "id": "EUYGVcr2I4Ny"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_weights_into_gpt(good_gpt, params)\n",
        "good_gpt.to(device)\n",
        "0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP4FRMs4MI8H",
        "outputId": "f3099a60-5f12-459e-cd55-1328b1773d1d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the weights are loaded into the model, the model performs very well."
      ],
      "metadata": {
        "id": "ksG7JtGKIMJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_and_print_sample(good_gpt, tokenizer, device, start_context=\"Once upon a time,\", max_new_tokens=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfA1LuyrMQ9a",
        "outputId": "878d0983-7a1e-4f13-b6f9-cce1fba52ee0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning for classification"
      ],
      "metadata": {
        "id": "RDofQ0sIE1Ym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we will fine-tune our GPT model to classify whether a text message is spam or not."
      ],
      "metadata": {
        "id": "kdeXANEPIYh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spam detection dataset"
      ],
      "metadata": {
        "id": "bGprlBe5FUOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the spam dataset:"
      ],
      "metadata": {
        "id": "NeL7d09wIUIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We always need a specific dataset in order to fine-tune\n",
        "# The dataset must be relevant to our task\n",
        "# For example:\n",
        "# Dataset where each datapoint is:\n",
        "# (input: text, ground truth: yes/no)\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "zip_path = \"spam_collection.zip\"\n",
        "extracted_path = \"spam_collection\"\n",
        "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
        "\n",
        "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
        "    if data_file_path.exists():\n",
        "        print(f\"{data_file_path} already exists.\")\n",
        "        return\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        with open(zip_path, \"wb\") as out_file:\n",
        "            out_file.write(response.read())\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extracted_path)\n",
        "\n",
        "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
        "    os.rename(original_file_path, data_file_path)\n",
        "\n",
        "    print(f\"Data downloaded and extracted to {extracted_path}.\")\n",
        "\n",
        "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MTJcDr8GFJ_",
        "outputId": "a7efa08a-f1fc-48bc-ef30-fab031186b99"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spam_collection/SMSSpamCollection.tsv already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data is formatted as a pandas dataframe initially. `ham` means not spam."
      ],
      "metadata": {
        "id": "B-tq3HJIIh68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
        "# df.columns = [\"label\", \"text\"]\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "aJyXd1DZJbUD",
        "outputId": "da714b11-6534-4067-d919-c896634640d2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Label                                               Text\n",
              "0      ham  Go until jurong point, crazy.. Available only ...\n",
              "1      ham                      Ok lar... Joking wif u oni...\n",
              "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      ham  U dun say so early hor... U c already then say...\n",
              "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
              "...    ...                                                ...\n",
              "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
              "5568   ham               Will ü b going to esplanade fr home?\n",
              "5569   ham  Pity, * was in mood for that. So...any other s...\n",
              "5570   ham  The guy did some bitching but I acted like i'd...\n",
              "5571   ham                         Rofl. Its true to its name\n",
              "\n",
              "[5572 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-61bbe71b-c365-46a8-878b-cd12ca0b8e90\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>spam</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>ham</td>\n",
              "      <td>Will ü b going to esplanade fr home?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>ham</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>ham</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>ham</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-61bbe71b-c365-46a8-878b-cd12ca0b8e90')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-61bbe71b-c365-46a8-878b-cd12ca0b8e90 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-61bbe71b-c365-46a8-878b-cd12ca0b8e90');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_0346df83-f618-41d8-a8a0-9f55a97ec120\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_0346df83-f618-41d8-a8a0-9f55a97ec120 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5572,\n  \"fields\": [\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5169,\n        \"samples\": [\n          \"K, makes sense, btw carlos is being difficult so you guys are gonna smoke while I go pick up the second batch and get gas\",\n          \"URGENT! Your mobile No *********** WON a \\u00a32,000 Bonus Caller Prize on 02/06/03! This is the 2nd attempt to reach YOU! Call 09066362220 ASAP! BOX97N7QP, 150ppm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a lot more non-spam messages than spam messages in the dataset."
      ],
      "metadata": {
        "id": "hrR2ddtDItJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"Label\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmoUdIWDJvwj",
        "outputId": "318b7624-c964-40cc-8152-dbf07eebe981"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "ham     4825\n",
            "spam     747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function used to balance the dataset (equal numbers of spam and non-spam messages):"
      ],
      "metadata": {
        "id": "TAMC5e2kIybb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_balanced_spam_dataset(df):\n",
        "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
        "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(\n",
        "        num_spam, random_state=123\n",
        "    )\n",
        "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
        "    return balanced_df\n",
        "\n",
        "balanced_df = create_balanced_spam_dataset(df)\n",
        "print(balanced_df[\"Label\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kvaj2qqgJ6CL",
        "outputId": "d9ae2979-f8c0-453d-e5f0-3f7055697dbf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "ham     747\n",
            "spam    747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
      ],
      "metadata": {
        "id": "3AwLIv0mK6zr"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function used to create train, val, and test splits from the dataset:"
      ],
      "metadata": {
        "id": "fPaNFS82I5hW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import index\n",
        "# Split dataset into train, validation, test\n",
        "\n",
        "def random_split(df, train_ratio=0.8, val_ratio=0.1):\n",
        "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "    train_end = int(train_ratio * len(df))\n",
        "    val_end = train_end + int(val_ratio * len(df))\n",
        "\n",
        "    train_df = df[:train_end]\n",
        "    val_df = df[train_end:val_end] # Includes train_end but not val_end\n",
        "    test_df = df[val_end:]\n",
        "\n",
        "    return train_df, val_df, test_df"
      ],
      "metadata": {
        "id": "L2qA77B9LH8G"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, val_df, test_df = random_split(balanced_df)\n",
        "train_df.to_csv(\"train.csv\", index=None)\n",
        "val_df.to_csv(\"val.csv\", index=None)\n",
        "test_df.to_csv(\"test.csv\", index=None)"
      ],
      "metadata": {
        "id": "L51aBM4wMQ-i"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Often, we want to **pad** our dataset.\n",
        "\n",
        "Padding example:\n",
        "If the padding token is \\`, then our message will look something like\n",
        "\"This is a text message \\` \\` \\` \\` \\` \\` \\` \\` ...\"\n",
        "\n",
        "Padding ensures that text messages are the same length.\n",
        "\n",
        "Padding can allow everything to be grouped nicely into batches."
      ],
      "metadata": {
        "id": "VF_GbzxkQbK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# text = \"Free money now! Enter the lottery system by giving your social security number\"\n",
        "text = \"Free money now!\"\n",
        "tokens = tokenizer.encode(text)\n",
        "print(tokens)\n",
        "\n",
        "# Example: Truncating a sequence\n",
        "max_length = 10\n",
        "tokens = tokens[:max_length]\n",
        "print(tokens)\n",
        "\n",
        "# Example: Padding a sequence\n",
        "pad_token_id = 50256\n",
        "padded = tokens + [pad_token_id] * (max_length - len(tokens))\n",
        "\n",
        "print(padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj9JnREHKVgm",
        "outputId": "16051dca-e442-49b9-8089-b529b95dc8de"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11146, 1637, 783, 0]\n",
            "[11146, 1637, 783, 0]\n",
            "[11146, 1637, 783, 0, 50256, 50256, 50256, 50256, 50256, 50256]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 1:** Fill in missing parts of the SpamDataset class."
      ],
      "metadata": {
        "id": "u8ofnmNsJB2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "         # TODO: Tokenize the text\n",
        "        self.encoded_texts = [tokenizer.encode(text) for text in self.data[\"Text\"]]\n",
        "\n",
        "        if max_length is None:\n",
        "            self.max_length = self._longest_encoded_length()\n",
        "        else:\n",
        "            self.max_length = max_length\n",
        "            # TODO: Truncate the text\n",
        "            self.encoded_texts = [encoded_text[:self.max_length]\n",
        "                                  for encoded_text in self.encoded_texts]\n",
        "\n",
        "        # TODO: Pad the text\n",
        "        self.encoded_texts = [encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
        "                              for encoded_text in self.encoded_texts]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoded_text = torch.tensor(self.encoded_texts[idx], dtype=torch.long)\n",
        "        label = torch.tensor(self.data.iloc[idx][\"Label\"], dtype=torch.long)\n",
        "        return encoded_text, label\n",
        "\n",
        "    def _longest_encoded_length(self):\n",
        "        return max(len(encoded_text) for encoded_text in self.encoded_texts)"
      ],
      "metadata": {
        "id": "X1ILFokzKzqD"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating train, val, and test dataloaders of the Spam dataset:"
      ],
      "metadata": {
        "id": "a5QeYXFMLna0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "max_length = None\n",
        "train_dataset = SpamDataset(csv_file=\"train.csv\", tokenizer=tokenizer, max_length=max_length)\n",
        "val_dataset = SpamDataset(csv_file=\"val.csv\", tokenizer=tokenizer, max_length=max_length)\n",
        "test_dataset = SpamDataset(csv_file=\"test.csv\", tokenizer=tokenizer, max_length=max_length)"
      ],
      "metadata": {
        "id": "rnzy71Y2TTWy"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = 0\n",
        "batch_size = 8\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
      ],
      "metadata": {
        "id": "X9gSKwVkTsx2"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_batch, target_batch in train_dataloader:\n",
        "    # print(input_batch.shape, target_batch.shape)\n",
        "    # print(input_batch)\n",
        "    # print(target_batch)\n",
        "    break\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "lS6UUQjiT2ck"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input datapoint: text\n",
        "\n",
        "Target datapoint: 0 or 1"
      ],
      "metadata": {
        "id": "Yl0Y5If5UxvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning"
      ],
      "metadata": {
        "id": "4-DjbNZqGDex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "settings, params = download_and_load_gpt2(\n",
        "    model_size=\"355M\", models_dir=\"gpt2\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9Oj242M_lKk",
        "outputId": "db1fa5cd-bacd-4c18-a862-5cc48cd16347"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
            "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
            "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
            "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
            "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
            "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
            "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing our model and optimizer:"
      ],
      "metadata": {
        "id": "X3EqzqvfM8cK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# spam_model_name = \"gpt2-small (124M)\"\n",
        "spam_model_name = \"gpt2-medium (355M)\"\n",
        "# spam_model_name = \"gpt2-large (774M)\"\n",
        "# spam_model_name = \"gpt2-xl (1558M)\"\n",
        "SPAM_CONFIG = GPT_CONFIG_124M.copy()\n",
        "SPAM_CONFIG.update(model_configs[spam_model_name])\n",
        "SPAM_CONFIG.update({\"context_length\": 1024})\n",
        "SPAM_CONFIG.update({\"qkv_bias\": True})\n",
        "\n",
        "\n",
        "spam_model = Simple_GPT(SPAM_CONFIG)\n",
        "load_weights_into_gpt(spam_model, params)\n",
        "spam_model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(spam_model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "SPAM_CONFIG"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6C8CUcuAPV-",
        "outputId": "f08932b5-d50e-4f17-9f3c-f17eff888f42"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'vocab_size': 50257,\n",
              " 'context_length': 1024,\n",
              " 'n_embd': 1024,\n",
              " 'n_heads': 16,\n",
              " 'n_layers': 24,\n",
              " 'dropout_rate': 0.1,\n",
              " 'qkv_bias': True,\n",
              " 'device': 'cuda'}"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple_GPT output size is 50257. We want the output size to be 2 for classification. For any model instance, we can replace portions of the model. We will replace the prediction layer to make it perform 2-way classification instead of next token prediction (50257-way classification)."
      ],
      "metadata": {
        "id": "HhCr91g0AsIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change prediction layer into classification layer\n",
        "num_classes = 2\n",
        "spam_model.prediction_layer = nn.Linear(SPAM_CONFIG[\"n_embd\"], num_classes)\n",
        "spam_model.to(device)\n",
        "0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOB4MYfQN4FD",
        "outputId": "a14cd0c4-7c66-4b27-81dc-5e0a145660a8"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for p in spam_model.prediction_layer.parameters():\n",
        "    print(p.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhNo4WXfOX5P",
        "outputId": "d0fce7a7-13af-4222-aa91-01dcaef2e468"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1024])\n",
            "torch.Size([2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`requires_grad` determines whether a parameter group is trainable (`True` means trainable; `False` means frozen).\n",
        "\n",
        "We only want to train part of the model when fine-tuning since we start from a pre-trained model. We can freeze certain parameters of the model."
      ],
      "metadata": {
        "id": "MBK8ii_zN3sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all model parameters\n",
        "for param in spam_model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "7sWEFQsdA4js"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2:** Make the last transformer block and classification layer trainable without touching other parameters."
      ],
      "metadata": {
        "id": "BJpP70n7NNvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make last transformer block and classification layer trainable\n",
        "for param in spam_model.blocks[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in spam_model.prediction_layer.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "hUKefnUONU8H"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 3:** Fill missing parts of the accuracy calculation function:"
      ],
      "metadata": {
        "id": "xI6YeVebOOP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_spam_accuracy(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for input_batch, target_batch in dataloader:\n",
        "            # TODO: Move input and target batch to proper device\n",
        "            input_batch = input_batch.to(device)\n",
        "            target_batch = target_batch.to(device)\n",
        "\n",
        "            # TODO: Obtain logits\n",
        "            logits = model(input_batch) # B x N x 2, where 2 is # classes\n",
        "\n",
        "            # TODO: Get last token from each context window\n",
        "            last_logits = logits[:, -1, :] # B x 2\n",
        "\n",
        "            # TODO: Use argmax to get predicted labels\n",
        "            predicted_labels = torch.argmax(last_logits, dim=-1)\n",
        "\n",
        "            total += predicted_labels.shape[0] # total += Batch size\n",
        "            correct += (predicted_labels == target_batch).sum().item()\n",
        "    accuracy = correct / total\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "w3WChD9ROqZH"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracy = calculate_spam_accuracy(spam_model, train_dataloader, device)\n",
        "val_accuracy = calculate_spam_accuracy(spam_model, val_dataloader, device)\n",
        "test_accuracy = calculate_spam_accuracy(spam_model, test_dataloader, device)\n",
        "print(f\"Train accuracy: {train_accuracy}\")\n",
        "print(f\"Val accuracy: {val_accuracy}\")\n",
        "print(f\"Test accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Zuq2Ux7DEtL",
        "outputId": "e81efc5b-7532-424c-e066-41b01bbee60f"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 0.499581589958159\n",
            "Val accuracy: 0.47651006711409394\n",
            "Test accuracy: 0.5266666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 4:** Fill in missing parts about calculating spam loss:"
      ],
      "metadata": {
        "id": "lawayq6kPgr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the loss for a single batch\n",
        "def calculate_spam_loss_batch(input_batch, target_batch, model, device):\n",
        "    # TODO: Move batches to the proper device\n",
        "    input_batch = input_batch.to(device)\n",
        "    target_batch = target_batch.to(device)\n",
        "\n",
        "    # TODO: Obtain logits\n",
        "    logits = model(input_batch)[:, -1, :]\n",
        "    # Unlike our original training, we only want the last token in each context window\n",
        "    # Originally: Predict next token -> correctly predicting earlier tokens is some measure\n",
        "    # of model's performance\n",
        "    # Now: Binary classification -> only care about the final classification\n",
        "    # We don't care about classifications with incomplete information\n",
        "\n",
        "    # TODO: Calculate loss\n",
        "    loss = nn.functional.cross_entropy(logits, target_batch)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "lha9lpwrPsVq"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate overall spam loss\n",
        "def calculate_spam_loss(model, dataloader, device):\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    if len(dataloader) == 0:\n",
        "        return float(\"nan\")\n",
        "\n",
        "    for input_batch, target_batch in dataloader:\n",
        "        # TODO: Calculate batch loss\n",
        "        loss = calculate_spam_loss_batch(input_batch, target_batch, model, device)\n",
        "\n",
        "        # TODO: Update total_loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "P58GdexgP_WD"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    train_loss = calculate_spam_loss(spam_model, train_dataloader, device)\n",
        "    val_loss = calculate_spam_loss(spam_model, val_dataloader, device)\n",
        "    test_loss = calculate_spam_loss(spam_model, test_dataloader, device)\n",
        "print(f\"Train loss: {train_loss}\")\n",
        "print(f\"Val loss: {val_loss}\")\n",
        "print(f\"Test loss: {test_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJAKHIZeEpgN",
        "outputId": "15292243-c00c-43a4-b3fb-9faa518bd65f"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 2.520157130236427\n",
            "Val loss: 2.6221918369594372\n",
            "Test loss: 2.495323482312654\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 5:** Fill in missing parts of the training loop."
      ],
      "metadata": {
        "id": "hOyZJOavQODN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_spam_classifier(model, train_dataloader, val_dataloader,\n",
        "                          optimizer, device, num_epochs, eval_freq=50, eval_iter=5):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    examples_seen, global_step = 0, -1\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for input_batch, target_batch in train_dataloader:\n",
        "            # TODO: Perform one parameter update\n",
        "            # (you can use last lecture's training loop as reference)\n",
        "            optimizer.zero_grad()\n",
        "            loss = calculate_spam_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            global_step += 1\n",
        "            examples_seen += input_batch.shape[0] # batch size\n",
        "\n",
        "            if global_step % eval_freq == 0:\n",
        "                with torch.no_grad():\n",
        "                    train_loss = calculate_spam_loss(model, train_dataloader, device)\n",
        "                    val_loss = calculate_spam_loss(model, val_dataloader, device)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "\n",
        "                print(f\"Epoch {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f},\"\n",
        "                      f\"Val loss {val_loss:.3f}\")\n",
        "\n",
        "        train_acc = calculate_spam_accuracy(model, train_dataloader, device)\n",
        "        val_acc = calculate_spam_accuracy(model, val_dataloader, device)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "        print(f\"Train accuracy: {train_acc}\")\n",
        "        print(f\"Val accuracy: {val_acc}\")\n",
        "\n",
        "    return train_losses, val_losses, train_accs, val_accs"
      ],
      "metadata": {
        "id": "F6dCivWKQy5R"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spam_optimizer = torch.optim.AdamW(spam_model.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "num_epochs = 5\n",
        "\n",
        "train_losses, val_losses, train_accs, val_accs = train_spam_classifier(\n",
        "    spam_model, train_dataloader, val_dataloader,\n",
        "    spam_optimizer, device, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCWUJ9UlG1Hj",
        "outputId": "794ac9aa-a9c1-4691-f8de-38e240704203"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (Step 000000): Train loss 2.073,Val loss 2.237\n",
            "Epoch 1 (Step 000050): Train loss 0.718,Val loss 0.814\n",
            "Epoch 1 (Step 000100): Train loss 0.591,Val loss 0.664\n",
            "Train accuracy: 0.797489539748954\n",
            "Val accuracy: 0.5167785234899329\n",
            "Epoch 2 (Step 000150): Train loss 0.536,Val loss 0.781\n",
            "Epoch 2 (Step 000200): Train loss 0.510,Val loss 0.934\n",
            "Epoch 2 (Step 000250): Train loss 0.499,Val loss 0.990\n",
            "Train accuracy: 0.8359832635983263\n",
            "Val accuracy: 0.5167785234899329\n",
            "Epoch 3 (Step 000300): Train loss 0.473,Val loss 0.674\n",
            "Epoch 3 (Step 000350): Train loss 0.480,Val loss 0.858\n",
            "Epoch 3 (Step 000400): Train loss 0.475,Val loss 0.646\n",
            "Train accuracy: 0.8267782426778243\n",
            "Val accuracy: 0.5167785234899329\n",
            "Epoch 4 (Step 000450): Train loss 0.442,Val loss 0.800\n",
            "Epoch 4 (Step 000500): Train loss 0.426,Val loss 0.524\n",
            "Epoch 4 (Step 000550): Train loss 0.343,Val loss 0.772\n",
            "Train accuracy: 0.9154811715481171\n",
            "Val accuracy: 0.5167785234899329\n",
            "Epoch 5 (Step 000600): Train loss 0.264,Val loss 0.604\n",
            "Epoch 5 (Step 000650): Train loss 0.233,Val loss 0.741\n",
            "Epoch 5 (Step 000700): Train loss 0.236,Val loss 0.396\n",
            "Train accuracy: 0.9372384937238494\n",
            "Val accuracy: 0.6174496644295302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_spam_accuracy(spam_model, test_dataloader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzbznR7AK4fq",
        "outputId": "84610e2d-cbe1-4f38-cfda-f35fbb22db7f"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5133333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using our spam classification model for inference:"
      ],
      "metadata": {
        "id": "CRkcQ1nNQ4yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_spam_text(text, model, tokenizer, device, max_length=None,\n",
        "                       pad_token_id=50256):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_ids = tokenizer.encode(text)\n",
        "        supported_context_length = model.config[\"context_length\"]\n",
        "        if max_length is None:\n",
        "            max_length = supported_context_length\n",
        "        input_ids = input_ids[:min(max_length, supported_context_length)]\n",
        "        input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
        "\n",
        "        input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "        logits = model(input_tensor)[:, -1, :]\n",
        "        predicted_label = torch.argmax(logits, dim=-1).item()\n",
        "    return (\"spam\" if predicted_label == 1 else \"not spam\", torch.softmax(logits, dim=-1))"
      ],
      "metadata": {
        "id": "a8IiGgWgL5qs"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text_1 = \"You are a winner you have been specially selected to receive $1000\"\n",
        "sample_text_2 = \"Are you coming home tonight\"\n",
        "sample_text_3 = \"\"\"\n",
        "Pennsylvania (DMV) Final Notice: Enforcement Begins August 6nd. \\\n",
        "Our records indicate that as of today, you still have an outstanding traffic ticket.\n",
        "\"\"\"\n",
        "sample_text_4 = \"MIT Alert: Gas leak in Building 46. Responders on scene. Vassar Street closed.\"\n",
        "for text in [sample_text_1, sample_text_2, sample_text_3, sample_text_4]:\n",
        "    print(classify_spam_text(text, spam_model, tokenizer, device,\n",
        "                         max_length=train_dataset.max_length))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZfseqB8Ms_E",
        "outputId": "4b70fa04-322e-4bfc-9980-d4511402e9aa"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('spam', tensor([[0.0038, 0.9962]], device='cuda:0'))\n",
            "('not spam', tensor([[0.9976, 0.0024]], device='cuda:0'))\n",
            "('spam', tensor([[0.0731, 0.9269]], device='cuda:0'))\n",
            "('spam', tensor([[0.1085, 0.8915]], device='cuda:0'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attribution"
      ],
      "metadata": {
        "id": "fveDkvZlESLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Portions of this notebook are adapted from LLMs from Scratch by Sebastian Raschka\n",
        "https://github.com/rasbt/LLMs-from-scratch\n",
        "\n",
        "Licensed under the Apache License 2.0"
      ],
      "metadata": {
        "id": "gueC1bWpEWW9"
      }
    }
  ]
}